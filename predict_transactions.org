* Project Description + Kaggle Link 
The competition main page is [[https://www.kaggle.com/c/santander-customer-transaction-prediction/overview][Kaggle: Santander Customer Transaction Predictions]]

We are told:

"In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem."

We are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column. The task is to predict the value of target column in the test set.

I liked this as a toy model competition because the task and data are relatively simple, but the description of the data is quite lacking, so I have to think.
* Preprocessing
** Import data and modules, split into train/test/validation  
Import the data. At this stage, we don't even import the unlabelled test data used for the competition, because we need labels to evaluate our model. So instead we'll split our training data as train/validation/test.

#+BEGIN_SRC python :session :results silent 
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
# for correlations heatmap in data exploration
import seaborn as sns

# import tensorflow as tf
# from tensorflow import keras
#+END_SRC


#+BEGIN_SRC python :session :results output 
raw_dataframe = pd.read_csv('train.csv')

# The ID_code column contains no information, so we remove it
raw_dataframe.pop('ID_code')

# Shuffle and split the data into train/validation/test dataframes (we could also consider using something like sklearn StratifiedKFold as we discover that we have class imbalance)
train_dataframe, test_dataframe = train_test_split(raw_dataframe, test_size=0.2)
train_dataframe, validation_dataframe = train_test_split(train_dataframe, test_size=0.2)

# Form np arrays of labels and features.
train_labels = np.array(train_dataframe.pop('target'))
validation_labels = np.array(validation_dataframe.pop('target'))
test_labels = np.array(test_dataframe.pop('target'))

train_features = np.array(train_dataframe)
validation_features = np.array(validation_dataframe)
test_features = np.array(test_dataframe)

#+END_SRC

#+RESULTS:

** Explore the data 
*** Look at the data and summary statistics 

#+BEGIN_SRC python :session
# Make a copy of our dataframe in case we want to make modifications
exploratory_dataframe = raw_dataframe.copy()

# Print the first 10 rows of our dataframe
exploratory_dataframe.head(10)
#+END_SRC

#+RESULTS:
#+begin_example
   target    var_0   var_1    var_2   var_3    var_4    var_5   var_6    var_7  ...  var_191  var_192  var_193  var_194  var_195  var_196  var_197  var_198  var_199
0       0   8.9255 -6.7863  11.9081  5.0930  11.4607  -9.2834  5.1187  18.6266  ...   3.9642   3.1364   1.6910  18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914
1       0  11.5006 -4.1473  13.8588  5.3890  12.3622   7.0433  5.6208  16.5338  ...   7.7214   2.5837  10.9516  15.4305   2.0339   8.1267   8.7889  18.3560   1.9518
2       0   8.6093 -2.7457  12.0805  7.8928  10.5825  -9.0837  6.9427  14.6155  ...   9.7905   1.6704   1.6858  21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965
3       0  11.0604 -2.1518   8.9522  7.1957  12.5846  -1.8361  5.8428  14.9250  ...   4.7433   0.7178   1.4214  23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996
4       0   9.8369 -1.4834  12.8746  6.6375  12.2772   2.4486  5.9405  19.2514  ...   9.5214  -0.1508   9.1942  13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104
5       0  11.4763 -2.3182  12.6080  8.6264  10.9621   3.5609  4.5322  15.2255  ...   6.6025   5.2912   0.4403  14.9452   1.0314  -3.6241   9.7670  12.5809  -4.7602
6       0  11.8091 -0.0832   9.3494  4.2916  11.1355  -8.0198  6.1961  12.0771  ...   6.4521   3.5325   0.1777  18.3314   0.5845   9.1104   9.1143  10.8869  -3.2097
7       0  13.5580 -7.9881  13.8776  7.5985   8.6543   0.8310  5.6890  22.3262  ...   6.5491   3.9906   5.8061  23.1407  -0.3776   4.2178   9.4237   8.6624   3.4806
8       0  16.1071  2.4426  13.9307  5.6327   8.8014   6.1630  4.4514  10.1854  ...  14.7510   1.6395   1.4181  14.8370  -1.9940  -1.0733   8.1975  19.5114   4.8453
9       0  12.5088  1.9743   8.8960  5.4508  13.6043 -16.2859  6.0637  16.8410  ...   6.3160   1.0371   3.6885  14.8344   0.4467  14.1287   7.9133  16.2375  14.2514

[10 rows x 201 columns]
#+end_example
 
And some statistics about the data

#+BEGIN_SRC python :session
exploratory_dataframe.describe()
#+END_SRC

#+RESULTS:
#+begin_example
              target          var_0          var_1          var_2          var_3  ...        var_195        var_196        var_197        var_198        var_199
count  200000.000000  200000.000000  200000.000000  200000.000000  200000.000000  ...  200000.000000  200000.000000  200000.000000  200000.000000  200000.000000
mean        0.100490      10.679914      -1.627622      10.715192       6.796529  ...      -0.142088       2.303335       8.908158      15.870720      -3.326537
std         0.300653       3.040051       4.050044       2.640894       2.043319  ...       1.429372       5.454369       0.921625       3.010945      10.438015
min         0.000000       0.408400     -15.043400       2.117100      -0.040200  ...      -5.261000     -14.209600       5.960600       6.299300     -38.852800
25%         0.000000       8.453850      -4.740025       8.722475       5.254075  ...      -1.170700      -1.946925       8.252800      13.829700     -11.208475
50%         0.000000      10.524750      -1.608050      10.580000       6.825000  ...      -0.172700       2.408900       8.888200      15.934050      -2.819550
75%         0.000000      12.758200       1.358625      12.516700       8.324100  ...       0.829600       6.556725       9.593300      18.064725       4.836800
max         1.000000      20.315000      10.376800      19.353000      13.188300  ...       4.272900      18.321500      12.000400      26.079100      28.500700

[8 rows x 201 columns]
#+end_example

Missing values check:

#+BEGIN_SRC python :session :results value 
# counts the number of entries in each column and checks if this number is equal across all columns
exploratory_dataframe.count().nunique()
#+END_SRC

#+RESULTS:
: 1

Every feature has has a datapoint for all 200,000 rows. Let's check for repeated entries:

#+BEGIN_SRC python :session :results value 
# count how many unique entries each row has
unique_row_values = exploratory_dataframe.nunique(axis='columns')

# output the number of unique entries
unique_row_values.unique()
#+END_SRC

#+RESULTS:
| 201 | 200 | 199 | 198 |

These are unusual numbers. Maybe the targets are interfering or maybe a value is repeated here or there by coincidence. Let's check:

#+BEGIN_SRC python :session :results silent 
# rows that have entries that are repeated in the row (ie. < 201 unique values)
rows_with_duplicates = exploratory_dataframe[unique_row_values != 201]

# note which entries are duplicated within a row 
duplicate_entries_boolean = rows_with_duplicates.apply(lambda x: x.duplicated(keep=False), axis = 1)
#+END_SRC

#+RESULTS:
#+begin_example
target    var_0   var_1    var_2  ...  var_196  var_197  var_198  var_199
14           0  13.8080  5.0514  17.2611  ...  -3.5323   9.3439  24.4479  -5.1110
22           0  10.2031  0.1925  14.0238  ...  -7.5486   9.5064   8.7281 -25.6523
26           0  15.6567 -4.4950  10.4867  ...   3.4319   7.8821  19.3055  -7.5090
68           0   8.5576  1.4385  10.6548  ...   1.0236   8.1925  18.2969 -16.2097
85           0   5.5511 -6.0495   6.8957  ...  13.8302   9.7335  11.1988  -0.7338
...      ...     ...      ...  ...      ...      ...      ...      ...
199877       1  12.3381 -3.0178  10.9429  ...   6.8868   9.2086  16.3833   9.6348
199888       0  12.6929 -4.9290  10.7029  ...  14.1003  10.6589  19.0044  11.3123
199908       0  12.4229  1.8738  10.5611  ...   0.0101   9.2432  19.8261  -3.6446
199910       0  10.7423  0.2901   9.0327  ...   3.8474   7.9792  20.8257  -0.6774
199935       0  15.6192 -2.2020  11.0134  ...   1.1220  10.1649  17.7713  -9.2515

[12975 rows x 201 columns]
        target  var_0  var_1  var_2  ...  var_196  var_197  var_198  var_199
14       False  False  False  False  ...    False    False    False    False
22       False  False  False  False  ...    False    False    False    False
26       False  False  False  False  ...    False    False    False    False
68       False  False  False  False  ...    False    False    False    False
85       False  False  False  False  ...    False    False    False    False
...    ...    ...    ...  ...      ...      ...      ...      ...
199877   False  False  False  False  ...    False    False    False    False
199888   False  False  False  False  ...    False    False    False    False
199908   False  False  False  False  ...    False    False    False    False
199910   False  False  False  False  ...    False    False    False    False
199935   False  False  False   True  ...    False    False    False    False

[12975 rows x 201 columns]
#+end_example

Let's look at repeated values across columns and their corresponding features:

#+BEGIN_SRC python :session :results output  
# there are thousands of rows with dulicates, so let's only output the first 10
for row in rows_with_duplicates.index[0:10]:
    
    # For each row, find which entries are repeated entries (duplicate_entries_boolean == True) and then output these values with their corresponding columns headings 
    print(rows_with_duplicates.loc[row,
                                    duplicate_entries_boolean.loc[row] == True])
#+END_SRC

#+RESULTS:
#+begin_example
var_109    16.4421
var_153    16.4421
Name: 14, dtype: float64
var_31     13.8222
var_104    13.8222
Name: 22, dtype: float64
var_12    13.972
var_81    13.972
Name: 26, dtype: float64
var_33    18.6714
var_92    18.6714
Name: 68, dtype: float64
var_23    3.1413
var_64    3.1413
Name: 85, dtype: float64
var_63    -3.6868
var_180   -3.6868
Name: 113, dtype: float64
var_98     2.5277
var_124    2.5277
Name: 114, dtype: float64
var_63     3.2496
var_105    3.2496
Name: 141, dtype: float64
var_46     9.3496
var_139    9.3496
Name: 193, dtype: float64
var_1    -1.1508
var_65   -1.1508
Name: 196, dtype: float64
#+end_example

Let's check for repeated values in each feature (within each dataframe column).  

#+BEGIN_SRC python :session :results values 
# find the n features with largest correlation with the target
indices = exploratory_dataframe.corr()[['target']].nlargest(10,'target').index

# check how many unique entries appear for these n features
exploratory_dataframe[indices].nunique()
#+END_SRC

#+RESULTS:
#+begin_example
target          2
var_6       38599
var_110    106121
var_53      33460
var_26     127089
var_22      90660
var_99      69300
var_190    114959
var_2       86555
var_133     19236
dtype: int64
#+end_example

Let's check for columns that have the most repeated features

#+BEGIN_SRC python :session :results values
# print column that has the most repeated values
print(exploratory_dataframe['var_68'])

# print n features with the most repeated values
exploratory_dataframe.nunique().nsmallest(20)
#+END_SRC

#+RESULTS:
#+begin_example
target         2
var_68       451
var_91      7962
var_108     8525
var_103     9376
var_12      9561
var_148    10608
var_161    11071
var_71     13527
var_25     14853
var_43     15188
var_125    16059
var_166    17902
var_169    18242
var_133    19236
var_15     19810
var_131    21464
var_23     24913
var_34     25164
var_93     26708
dtype: int64
#+end_example




We can note some things so far (~10% of binary targets are 1, rest 0, so we have an imbalanced classification problem, we can note approximate upper and lower bounds on the data, rough idea of the width of the distributions, all numeric data so no need to process categorical variables). 

*** Which naive features correlate with the target and with each other?

#+BEGIN_SRC python :session :results output 
# pick out some features to draw correlations, and prepend 'target'
num_random_features = 10 
some_features = [f'var_{i}' for i in range (num_random_features)]
some_features.insert(0,'target')

# correlations
feature_correlations = exploratory_dataframe[some_features].corr()


# calculate correlations of all features with the target, find n largest entries
print(exploratory_dataframe.corr()[['target']].nlargest(20,'target'))
#+END_SRC

#+RESULTS:
#+begin_example
target
target   1.000000
var_6    0.066731
var_110  0.064275
var_53   0.063399
var_26   0.062422
var_22   0.060558
var_99   0.058367
var_190  0.055973
var_2    0.055870
var_133  0.054548
var_0    0.052390
var_1    0.050343
var_179  0.050002
var_40   0.049530
var_184  0.048315
var_78   0.048245
var_170  0.047973
var_191  0.047114
var_94   0.046296
var_67   0.044673
#+end_example

#+BEGIN_SRC python :session :results file

# draw correlation heatmap
plt.figure(figsize=(10,10))
sns.heatmap(feature_correlations, annot=True)
plt.savefig('feature_correlations.png')
plt.close()
'feature_correlations.png'
#+END_SRC

#+RESULTS:
[[file:raw_correlations.png]]

We don't notice any strong linear correlations, so we probably need to do some feature engineering and/or use nonlinear models.

*** Plot some dataframe rows

Let's plot all of the features for a given target on the same set of axes (perhaps this represents a sequence of transactions in time or something like that).

#+BEGIN_SRC python :session :results file 
j=15
plt.plot(train_features[j])
print([i for i,x in enumerate(train_labels[:100]) if x==1]) 
plt.savefig('sample_row_plot.png')
plt.close()
'sample_row_plot.png'
#+END_SRC

#+RESULTS:
[[file:sample_features_plot.png]]

Let's plot a column of interest that has a lot of repeated values

#+BEGIN_SRC python :session :results file 
# print(np.array(kexploratory_dataframe['var_68'])
exploratory_dataframe.plot(y='var_68', style='o', markersize=1) 
plt.savefig('sample_feature_plot.png')
plt.close()
'sample_feature_plot.png'
#+END_SRC

#+RESULTS:
[[file:sample_feature_plot.png]]

*** Plot some histograms 

Plot a histogram of a column:

#+BEGIN_SRC python :session :results file
exploratory_dataframe['var_68'].hist()
plt.savefig('feature_68_hist.png')
plt.close()
'feature_68_hist.png'
#+END_SRC

#+RESULTS:
[[file:feature_68_hist.png]]
 
Let's look at what all rows with target value 1 look like

#+BEGIN_SRC python :session
exploratory_dataframe.loc[exploratory_dataframe['target']==1]
#+END_SRC

#+RESULTS:
#+begin_example
             ID_code  target    var_0   var_1    var_2   var_3    var_4    var_5   var_6    var_7   var_8   var_9  ...  var_188  var_189  var_190  var_191  var_192  var_193  var_194  var_195  var_196  var_197  var_198  var_199
13          train_13       1  16.3699  1.5934  16.7395  7.3330  12.1450   5.9004  4.8222  20.9729  1.1064  8.6978  ...  11.9586  -0.5899   7.4002   7.4031   4.3989   4.0978  17.3638  -1.3022   9.6846   9.0419  15.6064 -10.8529
29          train_29       1   5.3301 -2.6064  13.1913  3.1193   6.6483  -6.5659  5.9064  15.2341  1.2915  9.1168  ...  18.6375   0.1734   5.9215   7.9676   2.3405   1.1482  23.2168  -2.0105   3.7600   9.4513  17.4105 -14.6897
63          train_63       1   7.7072  0.0183   9.9974  8.3524   9.2886 -13.3627  6.0425  10.1108  1.3999  6.6710  ...  10.0679   1.9046   1.5832   5.0039   3.8814   7.4241  21.4844  -0.8297  -3.0468   7.5790  15.7685   5.4769
65          train_65       1  10.5358 -2.5439   8.7394  6.7548  14.4099  -3.8724  5.1584  15.8381  5.8204  9.0358  ...  10.2542   1.5517   4.6648   6.4227   3.4025  -4.0882  14.1174  -0.2472   5.3847   8.6949  15.1340   3.8449
71          train_71       1   6.7547  2.5973  14.2141  8.3514   7.4942  -1.3055  4.2336  15.0243 -1.8922  9.1282  ...  13.8773  -0.0899   1.4677   3.5935   2.0013   1.5777  18.2820  -4.3408   6.8869   9.3567  18.9013  13.3447
...              ...     ...      ...     ...      ...     ...      ...      ...     ...      ...     ...     ...  ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...
199966  train_199966       1  13.5797  2.5526   6.0512  5.2730  12.2182  -3.4048  7.3623  17.8372 -3.5604  8.8837  ...  20.7649  -0.4363   3.9023   7.9986   0.5213   2.3442  14.5510  -1.1530   8.9883   8.3389   9.5440   4.2493
199976  train_199976       1   7.9663 -2.8485   9.0919  7.3298   9.6690 -16.7872  4.5094  12.4351 -0.0113  8.5394  ...  20.1372   0.3380  10.7930   4.3876   3.7257   7.7038  14.7384   0.1561   1.5794   8.4627  14.3604  -1.6688
199981  train_199981       1  12.8140  0.6386  14.1657  7.1044   8.9365  -0.3274  6.5949  14.6078 -1.0373  8.8974  ...   7.0611   1.5463   4.8208   4.9010   2.2513   0.7308  14.7155   1.1464   5.5158   8.6519  16.0341   7.3809
199986  train_199986       1  12.0298 -8.7800   7.7071  7.4015   9.2305 -16.2174  5.9064  17.9268  3.6489  7.3970  ...   9.3059  -1.0691  16.7461   3.1249  -0.3943   8.4059  14.3367   3.0991   4.3853   8.8019  15.0031  -0.3659
199990  train_199990       1  14.1475  1.8568  11.0066  3.6779  12.1944 -16.5936  5.3217  14.8508  3.3377  6.1650  ...  16.0983   0.8156  -6.4708   4.7287   1.9034   7.2324  20.6047   1.7170  -4.0032   9.1627  13.8077  -1.9646

[20098 rows x 202 columns]
#+end_example

Plot a histogram across features for a given row that has target value 1.

#+BEGIN_SRC python :session :results file 
exploratory_dataframe.iloc[13,2:].hist()

# We need to save the figure to display inline in org mode. We also should use plt.close() so that we can respawn new different images without issues.
plt.savefig('row_hist1.png')
plt.close()
'row_hist1.png'
#+END_SRC

#+RESULTS:
[[file:hist1.png]]

* DONE Basic Neural Net model 
CLOSED: [2020-01-29 Wed 20:01]
** No class weights 

Let's follow https://www.tensorflow.org/tutorials/structured_data/imbalanced_data to implement a basic Neural Net in Tensorflow. We'll use a single layer for benchmarking and optimize later. Most of the code is copy-pasted from the tutorial. 

Define the model and metrics

#+BEGIN_SRC python :session :results output
  METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'),
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
  ]

  # Note the option to use bias initialization, see http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines
  # We modify the tutorial to allow for different numbers of hidden units
  def make_model(metrics = METRICS, output_bias=None, hidden_units = 16):
      if output_bias is not None:
          output_bias = tf.keras.initializers.Constant(output_bias)
      model = keras.Sequential([
          keras.layers.Dense(hidden_units, activation='relu',
                            input_shape=(train_features.shape[-1],)),
          keras.layers.Dropout(0.2),
          keras.layers.Dense(1, activation='sigmoid',
                            bias_initializer=output_bias)
        ])


      model.compile(
          optimizer=keras.optimizers.Adam(lr=1e-3),
          loss=keras.losses.BinaryCrossentropy(),
          metrics=metrics)
      return model
#+END_SRC

#+RESULTS:

Build the model

#+BEGIN_SRC python :session :results output
EPOCHS = 100
BATCH_SIZE = 2048

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc', 
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)

model = make_model()
model.summary()

#+END_SRC


#+RESULTS:
#+begin_example
Model: "sequential_8"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_24 (Dense)             (None, 16)                3216      
_________________________________________________________________
dropout_16 (Dropout)         (None, 16)                0         
_________________________________________________________________
dense_25 (Dense)             (None, 1)                 17        
=================================================================
Total params: 3,233
Trainable params: 3,233
Non-trainable params: 0
_________________________________________________________________
#+end_example

Test run with a small amount of data

#+BEGIN_SRC python :session :results output
# Input numpy as a numpy array
model.predict(train_features[:10])
#+END_SRC

#+RESULTS:
#+begin_example
2020-01-26 21:39:16.948460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
array([[0.9997596 ],
       [0.5061394 ],
       [0.9303511 ],
       [0.7892672 ],
       [0.9999958 ],
       [0.9980216 ],
       [0.33681375],
       [0.35988793],
       [0.9976675 ],
       [0.9999008 ]], dtype=float32)
#+end_example

So far so good, let's follow the tutorial to set the initial bias as Log(pos/neg)

#+BEGIN_SRC python :session 
initial_bias = np.log(1/9)

model = make_model(output_bias = initial_bias)
model.predict(train_features[:10])

#+END_SRC

#+RESULTS:
|    0.94163775 |
|    0.92257184 |
|     0.8388229 |
|  0.0018517158 |
| 4.5338511e-05 |
|    0.27973586 |
|  0.0094071003 |
|   0.029227791 |
|    0.47860023 |
|  0.0087348791 |


Train the model

#+BEGIN_SRC python :session :results silent 
initial_bias = np.log(1/9)

model = make_model(hidden_units = 16, output_bias = initial_bias)

# Features and labels input as numpy arrays
baseline_history = model.fit(
    train_features,
    train_labels,
    batch_size=2048,
    epochs=500,
    # callbacks = [early_stopping],
    validation_data=(validation_features, validation_labels))
#+END_SRC

#+RESULTS:
: 2020-01-27 23:12:17.650343: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 204800000 exceeds 10% of system memory.
: 2020-01-27 23:12:17.912503: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 51200000 exceeds 10% of system memory.
: Train on 128000 samples, validate on 32000 samples
:   2048/128000 [..............................] - ETA: 7:54 - loss: 22.8784 - tp: 205.0000 - fp: 1806.0000 - tn: 30.0000 - fn: 7.0000 - accuracy: 0.1147 - precision: 0.1019 - recall: 0.9670 - auc: 0.4867  8192/128000 [>.............................] - ETA: 1:53 - loss: 16.3919 - tp: 788.0000 - fp: 6764.0000 - tn: 579.0000 - fn: 61.0000 - accuracy: 0.1669 - precision: 0.1043 - recall: 0.9282 - auc: 0.4966 22528/128000 [====>.........................] - ETA: 36s - loss: 7.9990 - tp: 1208.0000 - fp: 10592.0000 - tn: 9610.0000 - fn: 1118.0000 - accuracy: 0.4802 - precision: 0.1024 - recall: 0.5193 - auc: 0.4953 36864/128000 [=======>......................] - ETA: 19s - loss: 5.8956 - tp: 1216.0000 - fp: 10686.0000 - tn: 22399.0000 - fn: 2563.0000 - accuracy: 0.6406 - precision: 0.1022 - recall: 0.3218 - auc: 0.4991 51200/128000 [===========>..................] - ETA: 11s - loss: 5.0086 - tp: 1219.0000 - fp: 10718.0000 - tn: 35259.0000 - fn: 4004.0000 - accuracy: 0.7125 - precision: 0.1021 - recall: 0.2334 - auc: 0.5007 65536/128000 [==============>...............] - ETA: 7s - loss: 4.4345 - tp: 1229.0000 - fp: 10819.0000 - tn: 47999.0000 - fn: 5489.0000 - accuracy: 0.7512 - precision: 0.1020 - recall: 0.1829 - auc: 0.5004  79872/128000 [=================>............] - ETA: 4s - loss: 3.9526 - tp: 1285.0000 - fp: 11237.0000 - tn: 60502.0000 - fn: 6848.0000 - accuracy: 0.7736 - precision: 0.1026 - recall: 0.1580 - auc: 0.5015 94208/128000 [=====================>........] - ETA: 2s - loss: 3.5805 - tp: 1436.0000 - fp: 12531.0000 - tn: 72058.0000 - fn: 8183.0000 - accuracy: 0.7801 - precision: 0.1028 - recall: 0.1493 - auc: 0.5016108544/128000 [========================>.....] - ETA: 1s - loss: 3.2964 - tp: 1675.0000 - fp: 14593.0000 - tn: 82898.0000 - fn: 9378.0000 - accuracy: 0.7792 - precision: 0.1030 - recall: 0.1515 - auc: 0.5016122880/128000 [===========================>..] - ETA: 0s - loss: 3.0659 - tp: 1852.0000 - fp: 16176.0000 - tn: 94222.0000 - fn: 10630.0000 - accuracy: 0.7819 - precision: 0.1027 - recall: 0.1484 - auc: 0.5008128000/128000 [==============================] - 9s 70us/sample - loss: 2.9920 - tp: 1909.0000 - fp: 16609.0000 - tn: 98408.0000 - fn: 11074.0000 - accuracy: 0.7837 - precision: 0.1031 - recall: 0.1470 - auc: 0.5016 - val_loss: 0.9531 - val_tp: 65.0000 - val_fp: 342.0000 - val_tn: 28514.0000 - val_fn: 3079.0000 - val_accuracy: 0.8931 - val_precision: 0.1597 - val_recall: 0.0207 - val_auc: 0.5117
** With class weights

#+BEGIN_SRC python :session :results output
# total/negative examples, total/positive examples, factor of 1/2 according to https://www.tensorflow.org/tutorials/structured_data/imbalanced_data
weight_for_0 = (10.0/9.0)*1/2.0 
weight_for_1 = 10.0/2.0

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))

#+END_SRC

#+RESULTS:
: Weight for class 0: 0.56
: Weight for class 1: 5.00

#+BEGIN_SRC python :session :results silent
initial_bias = np.log(1/9)

weighted_model = make_model(hidden_units = 16, output_bias = initial_bias)

# features and labels input as numpy arrays
weighted_history = weighted_model.fit(
    train_features,
    train_labels,
    batch_size=2048*4,
    epochs=500,
    # callbacks = [early_stopping],
    validation_data=(validation_features, validation_labels),
    # The class weights go here
    class_weight=class_weight) 
#+END_SRC

** Plot some metrics 

Define function to plot metrics

#+BEGIN_SRC python :session :results output 

import matplotlib as mpl
def plot_metrics(history):
    metrics =  ['loss', 'auc', 'precision', 'recall']
    mpl.rcParams['figure.figsize'] = (12, 10)
    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
    plt.figure(figsize=(6,4))
 
    for n, metric in enumerate(metrics):
        name = metric.replace("_"," ").capitalize()
        plt.subplot(2,2,n+1)
        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')
        plt.plot(history.epoch, history.history['val_'+metric], color=colors[0], linestyle="--", label='Val')
        plt.xlabel('Epoch')
        plt.ylabel(name)
        if metric == 'loss':
            plt.ylim([0, plt.ylim()[1]])
        elif metric == 'auc':
            plt.ylim([0.8,1])
        else:
            plt.ylim([0,1])
    
    plt.legend()
    plt.savefig('metrics.png')
    plt.close()
#+END_SRC

#+RESULTS:

Display metrics plot

#+BEGIN_SRC python :session :results file
plot_metrics(weighted_history)
'metrics.png':pr
#+END_SRC

#+RESULTS:
[[file:metrics.png]]

* DONE Basic Random Forest 
CLOSED: [2020-02-02 Sun 16:19]

Let's first run a basic Random Forest from sklearn. We'll use a blend of tutorials, with the FastAI lecture http://course18.fast.ai/lessonsml1/lesson2.html as backbone.

#+BEGIN_SRC python :session :results output
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.tree import export_graphviz
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session :results output 
# set up model parameters - for a start we can train a single small tree, with no probabilistic sample (no bootstrap), and tell it to use all of our cores.
rf = RandomForestClassifier(n_estimators=100, max_depth=15, max_features="sqrt", n_jobs=7)

# Train the model on training data
rf.fit(train_features, train_labels)

# makes predictions of probabilities on validation data 
predictions = rf.predict_proba(validation_features)

# calculate auc (note we only need second column of prediction probabilites - the probability of positive label)
auc = roc_auc_score(y_true=validation_labels, y_score=predictions[:,1])

# results:
# n_estimators=10, max_depth=10 , max_features="sqrt": 0.742, 20sec
# n_estimators=100, max_depth=10 , max_features="sqrt": 0.803, 40sec
# n_estimators=300, max_depth=10 , max_features="sqrt": 0.813, 2min 
# n_estimators=100, max_depth=15 , max_features="sqrt": 0.814, 1min 
# n_estimators=100, max_depth=25 , max_features="sqrt": 0.819, 2min 
# n_estimators=100, max_depth=15 , max_features="sqrt", class_weight={0:1,1:9}: 0.757, 2min 
# n_estimators=300, max_depth=15 , max_features="sqrt", class_weight={0:1,1:9}: 0.793, 3min 
# n_estimators=500, max_depth=25, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=7: 0.822, 6min
# n_estimators=1000, max_depth=42, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=7: 0.835, 15min
# n_estimators=2000, max_depth=42, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=7: 0.837, 23min
# n_estimators=1000, max_depth=42, max_features=40, class_weight={0:1,1:9}, n_jobs=7: 0.809, 35min
# n_estimators=2000, max_depth=25, max_features=40, class_weight={0:1,1:9}, n_jobs=7: 0.815, 55min
print(auc)
#+END_SRC

#+RESULTS:
: 0.8147160562178322

Let's draw the simple tree

#+BEGIN_SRC python :# export_graphviz(rf.estimators_[0], out_file=dotfile)
session :results output
#+END_SRC


** What if we scale our data? 
#+BEGIN_SRC python :session :results output
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# scale according to training data
train_features_scaled = scaler.fit_transform(train_features)

# apply the same transformation to validation data
val_features_scaled = scaler.transform(validation_features)

#+END_SRC

#+RESULTS:



# scaling

#+BEGIN_SRC python :session :results output
# set up model parameters
rf = RandomForestClassifier(n_estimators=100, max_depth=15, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=-1)

# Train the model on training data
rf.fit(train_features_scaled, train_labels)

# makes predictions on validation and print auc 
predictions = rf.predict_proba(val_features_scaled)
auc = roc_auc_score(y_true=validation_labels, y_score=predictions[:,1])

# results: 

print(auc)
#+END_SRC

#+RESULTS:
: 0.7583061694212846


Could purposefully samply less of the negative examples.
 
* DONE GBT in tensorflow (abandoned)   
CLOSED: [2020-02-03 Mon 05:47]

Let's also benchmark with a basic GBT implementation. We follow the tensorflow tutorial at https://www.tensorflow.org/tutorials/estimator/boosted_trees.


#+BEGIN_SRC python :session :results output
# useful shorthand to reduce clutter
fc = tf.feature_column
# start by using all features
NUMERIC_COLUMNS = [f'var_{i}' for i in range(20)]

feature_columns = []

for feature_name in NUMERIC_COLUMNS:
    feature_columns.append(fc.numeric_column(feature_name, dtype=tf.float32))
    
#+END_SRC

#+RESULTS:

Input functions

#+BEGIN_SRC python :session :results output
def make_input_fn(feature_dataframe,
                  target_dataframe,
                  batch_size=1, n_epochs=None,
                  shuffle=True):

    """Args:
        feature_dataframe: pandas dataframe
        target_dataframe: pandas dataframe
        n_epochs: 'None' results in using as many epochs as needed
    """

    SHUFFLE_BUFFER_SIZE = 10000
    def input_fn():
        dataset = tf.data.Dataset.from_tensor_slices((dict(feature_dataframe), target_dataframe))
        dataset = dataset.batch(batch_size)
        dataset = dataset.repeat(n_epochs)
        if shuffle:
            dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)
        return dataset
    return input_fn

#+END_SRC

#+RESULTS:

Initaliaze GBT Classifier, train, and make predictions on validation data:

#+BEGIN_SRC python :session :results output
def train_GBT_classifier(feature_columns,
                         train_dataframe,
                         train_labels_dataframe,
                         validation_dataframe,
                         validation_labels_dataframe,
                         n_trees=100,
                         max_depth=6,
                         batch_size=100,
                         n_batches_per_layer=1):
    """Args:
        feature_columns: pandas dataframe
        train_dataframe: pandas dataframe
        train_labels_dataframe: pandas dataframe
        validation_dataframe: pandas dataframe
        validation_labels_dataframe: pandas dataframe
    """

    # Initialize the classifier
    GBT_classifier = tf.estimator.BoostedTreesClassifier(feature_columns,
                                                         n_trees=n_trees,
                                                         max_depth=max_depth,
                                                         n_batches_per_layer=n_batches_per_layer)

    # Train and validation input functions
    train_input_fn = make_input_fn(train_dataframe, train_labels_dataframe, batch_size, n_epochs=10)
    # Vaidation input function uses default batch_size=1
    val_input_fn = make_input_fn(validation_dataframe, validation_labels_dataframe,
                                 shuffle=False, n_epochs=1)

    # Train
    GBT_classifier.train(train_input_fn, max_steps=100)

    # Evaluate and print results
    result = GBT_classifier.evaluate(val_input_fn)
    print(pd.Series(result))

    # return classifier so we can make other predictions if needed
    return GBT_classifier

GBT_classifier = train_GBT_classifier(feature_columns,
                                      train_dataframe,
                                      train_labels_dataframe,
                                      validation_dataframe,
                                      validation_labels_dataframe,
                                      n_trees=10,
                                      max_depth=6,
                                      batch_size=1)

#+END_SRC

#+RESULTS:
#+begin_example
INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpymp5cnym
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpymp5cnym', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
WARNING:tensorflow:From /home/jonathan/.pyenv/versions/tensorflow_env/lib/python3.8/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1628: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /home/jonathan/.pyenv/versions/tensorflow_env/lib/python3.8/site-packages/tensorflow_core/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
2020-02-03 03:00:39.480198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-02-03 03:00:39.669985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:39.670331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GT 755M computeCapability: 3.0
coreClock: 1.0195GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 80.47GiB/s
2020-02-03 03:00:39.670362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2020-02-03 03:00:40.284232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-03 03:00:40.443930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-02-03 03:00:40.746711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-02-03 03:00:41.021440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-02-03 03:00:41.057930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-02-03 03:00:41.122727: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-02-03 03:00:41.123034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:41.123835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:41.124448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Graph was finalized.
2020-02-03 03:00:43.808162: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2395110000 Hz
2020-02-03 03:00:43.808721: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563bb9ecd630 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-03 03:00:43.808757: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-02-03 03:00:43.809025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:43.809507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GT 755M computeCapability: 3.0
coreClock: 1.0195GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 80.47GiB/s
2020-02-03 03:00:43.809563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2020-02-03 03:00:43.809597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-03 03:00:43.809627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-02-03 03:00:43.809654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-02-03 03:00:43.809682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-02-03 03:00:43.809710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-02-03 03:00:43.809739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-02-03 03:00:43.809825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:43.810297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:43.810697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-02-03 03:00:43.810750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2020-02-03 03:00:44.133690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-03 03:00:44.133725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-02-03 03:00:44.133732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-02-03 03:00:44.133918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:44.134301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:44.134658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:44.134990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1691 MB memory) -> physical GPU (device: 0, name: GeForce GT 755M, pci bus id: 0000:01:00.0, compute capability: 3.0)
2020-02-03 03:00:44.136577: I tensorflow/compiler/xla/service/platform_util.cc:205] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0
2020-02-03 03:00:44.136652: I tensorflow/compiler/jit/xla_gpu_device.cc:136] Ignoring visible XLA_GPU_JIT device. Device number is 0, reason: Internal: no supported devices found for platform CUDA
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpymp5cnym/model.ckpt.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:loss = 0.6931472, step = 0
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
INFO:tensorflow:Saving checkpoints for 60 into /tmp/tmpymp5cnym/model.ckpt.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Loss for final step: 0.08353096.
INFO:tensorflow:Calling model_fn.
WARNING:tensorflow:From /home/jonathan/.pyenv/versions/tensorflow_env/lib/python3.8/site-packages/tensorflow_core/python/ops/metrics_impl.py:2029: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING:tensorflow:From /home/jonathan/.pyenv/versions/tensorflow_env/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/head.py:617: auc (from tensorflow.python.ops.metrics_impl) is deprecated and will be removed in a future version.
Instructions for updating:
The value of AUC returned by this may race with the update so this is deprected. Please use tf.keras.metrics.AUC instead.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to "careful_interpolation" instead.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to "careful_interpolation" instead.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2020-02-03T03:01:30Z
INFO:tensorflow:Graph was finalized.
2020-02-03 03:01:30.822953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:01:30.844028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GT 755M computeCapability: 3.0
coreClock: 1.0195GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 80.47GiB/s
2020-02-03 03:01:30.857993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2020-02-03 03:01:30.884146: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-03 03:01:30.884308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-02-03 03:01:30.884376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-02-03 03:01:30.884433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-02-03 03:01:30.884487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-02-03 03:01:30.884567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-02-03 03:01:30.884761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:01:30.885395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:01:30.885863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-02-03 03:01:30.885921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-03 03:01:30.885946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-02-03 03:01:30.902420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-02-03 03:01:30.902808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:01:30.903429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:01:30.903929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1691 MB memory) -> physical GPU (device: 0, name: GeForce GT 755M, pci bus id: 0000:01:00.0, compute capability: 3.0)
2020-02-03 03:01:30.910430: I tensorflow/compiler/xla/service/platform_util.cc:205] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0
2020-02-03 03:01:30.910581: I tensorflow/compiler/jit/xla_gpu_device.cc:136] Ignoring visible XLA_GPU_JIT device. Device number is 0, reason: Internal: no supported devices found for platform CUDA
INFO:tensorflow:Restoring parameters from /tmp/tmpymp5cnym/model.ckpt-60
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Inference Time : 101.28555s
INFO:tensorflow:Finished evaluation at 2020-02-03-03:03:11
INFO:tensorflow:Saving dict for global step 60: accuracy = 0.90178126, accuracy_baseline = 0.90178126, auc = 0.5, auc_precision_recall = 0.5491094, average_loss = 0.32557878, global_step = 60, label/mean = 0.09821875, loss = 0.32557878, precision = 0.0, prediction/mean = 0.07250305, recall = 0.0
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 60: /tmp/tmpymp5cnym/model.ckpt-60
accuracy                 0.901781
accuracy_baseline        0.901781
auc                      0.500000
auc_precision_recall     0.549109
average_loss             0.325579
label/mean               0.098219
loss                     0.325579
precision                0.000000
prediction/mean          0.072503
recall                   0.000000
global_step             60.000000
dtype: float64
#+end_example

* DONE GBT in XGBoost (abandoned)
CLOSED: [2020-02-04 Tue 00:34]

Load our numpy arrays into DMatrixes:

#+BEGIN_SRC python :session :results output
xgb_train = xgb.DMatrix(train_features, label=train_labels)
xgb_validation = xgb.DMatrix(validation_features, label=validation_labels)
#+END_SRC

#+RESULTS:

Set Booster parameters:

#+BEGIN_SRC python :session :results output
param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}
param['nthread'] = 2 
param['eval_metric'] = 'auc'

# Our gpu is Cuda compute 3.0, so we can only use the CPU with XGBoost
param['predictor'] = 'cpu_predictor'

evallist = [(xgb_validation, 'eval'), (xgb_train, 'train')]
#+END_SRC

#+RESULTS:

Check environment variables

#+BEGIN_SRC python :session :results output

#+END_SRC


Train the model

#+BEGIN_SRC python :session :results output
num_round = 1
xgb_model = xgb.train(param, xgb_train, num_round, evallist)
#+END_SRC

* DONE GBT in LightGBM
CLOSED: [2020-02-04 Tue 18:16]

From LightGBM documentation: best to use for larger datasets to avoid overfitting (> 10,000 rows).

#+BEGIN_SRC python :session :results output
import lightgbm as lgb

# create dataset for lightgbm
lgb_train = lgb.Dataset(train_features, train_labels)
lgb_eval = lgb.Dataset(validation_features,
                       validation_labels,
                       reference=lgb_train)
#+END_SRC

#+RESULTS:

Build the model

#+BEGIN_SRC python :session :results output
params = {
    # default num_trees=100
    'num_trees': 1000,
    'objective': 'binary',
    'metric': 'auc',
    'learning_rate': 0.05,
    # Percentage of features to be used for each tree
    'feature_fraction': 1.0,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.8,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 7
}

print('Starting training...')

# train
gbm = lgb.train(params,
                lgb_train,
                valid_sets=lgb_eval,
                early_stopping_rounds=100)

print('Done Training.')
#+END_SRC

#+RESULTS:
#+begin_example
Starting training...
[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true.
This may cause significantly different results comparing to the previous versions of LightGBM.
Try to set boost_from_average=false, if your old models produce bad results
[LightGBM] [Info] Number of positive: 12794, number of negative: 115206
[LightGBM] [Info] Total Bins 51000
[LightGBM] [Info] Number of data: 128000, number of used features: 200
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.099953 -> initscore=-2.197746
[LightGBM] [Info] Start training from score -2.197746
[1]	valid_0's auc: 0.661083
Training until validation scores don't improve for 100 rounds
[2]	valid_0's auc: 0.691879
[3]	valid_0's auc: 0.707244
[4]	valid_0's auc: 0.720795
[5]	valid_0's auc: 0.729838
[6]	valid_0's auc: 0.739292
[7]	valid_0's auc: 0.749324
[8]	valid_0's auc: 0.755149
[9]	valid_0's auc: 0.760964
[10]	valid_0's auc: 0.766117
[11]	valid_0's auc: 0.770555
[12]	valid_0's auc: 0.773792
[13]	valid_0's auc: 0.777026
[14]	valid_0's auc: 0.780276
[15]	valid_0's auc: 0.783147
[16]	valid_0's auc: 0.786443
[17]	valid_0's auc: 0.789546
[18]	valid_0's auc: 0.790874
[19]	valid_0's auc: 0.792706
[20]	valid_0's auc: 0.793463
[21]	valid_0's auc: 0.795205
[22]	valid_0's auc: 0.797141
[23]	valid_0's auc: 0.799221
[24]	valid_0's auc: 0.800802
[25]	valid_0's auc: 0.802979
[26]	valid_0's auc: 0.804168
[27]	valid_0's auc: 0.804549
[28]	valid_0's auc: 0.805273
[29]	valid_0's auc: 0.806124
[30]	valid_0's auc: 0.807128
[31]	valid_0's auc: 0.809319
[32]	valid_0's auc: 0.810269
[33]	valid_0's auc: 0.812024
[34]	valid_0's auc: 0.813068
[35]	valid_0's auc: 0.814215
[36]	valid_0's auc: 0.81558
[37]	valid_0's auc: 0.816381
[38]	valid_0's auc: 0.817267
[39]	valid_0's auc: 0.818257
[40]	valid_0's auc: 0.819627
[41]	valid_0's auc: 0.819894
[42]	valid_0's auc: 0.820737
[43]	valid_0's auc: 0.821583
[44]	valid_0's auc: 0.822142
[45]	valid_0's auc: 0.822652
[46]	valid_0's auc: 0.823296
[47]	valid_0's auc: 0.824416
[48]	valid_0's auc: 0.825283
[49]	valid_0's auc: 0.825824
[50]	valid_0's auc: 0.826804
[51]	valid_0's auc: 0.827888
[52]	valid_0's auc: 0.828525
[53]	valid_0's auc: 0.829099
[54]	valid_0's auc: 0.829841
[55]	valid_0's auc: 0.830522
[56]	valid_0's auc: 0.83139
[57]	valid_0's auc: 0.831995
[58]	valid_0's auc: 0.832594
[59]	valid_0's auc: 0.833039
[60]	valid_0's auc: 0.833789
[61]	valid_0's auc: 0.834299
[62]	valid_0's auc: 0.834549
[63]	valid_0's auc: 0.83531
[64]	valid_0's auc: 0.836194
[65]	valid_0's auc: 0.836694
[66]	valid_0's auc: 0.837402
[67]	valid_0's auc: 0.837979
[68]	valid_0's auc: 0.83841
[69]	valid_0's auc: 0.838866
[70]	valid_0's auc: 0.839571
[71]	valid_0's auc: 0.840001
[72]	valid_0's auc: 0.840717
[73]	valid_0's auc: 0.84099
[74]	valid_0's auc: 0.841516
[75]	valid_0's auc: 0.8418
[76]	valid_0's auc: 0.842484
[77]	valid_0's auc: 0.842988
[78]	valid_0's auc: 0.843683
[79]	valid_0's auc: 0.844147
[80]	valid_0's auc: 0.844888
[81]	valid_0's auc: 0.845363
[82]	valid_0's auc: 0.845547
[83]	valid_0's auc: 0.845904
[84]	valid_0's auc: 0.846165
[85]	valid_0's auc: 0.846505
[86]	valid_0's auc: 0.846924
[87]	valid_0's auc: 0.847252
[88]	valid_0's auc: 0.847625
[89]	valid_0's auc: 0.848026
[90]	valid_0's auc: 0.848325
[91]	valid_0's auc: 0.848607
[92]	valid_0's auc: 0.848863
[93]	valid_0's auc: 0.849305
[94]	valid_0's auc: 0.849781
[95]	valid_0's auc: 0.850011
[96]	valid_0's auc: 0.85028
[97]	valid_0's auc: 0.850948
[98]	valid_0's auc: 0.851335
[99]	valid_0's auc: 0.851622
[100]	valid_0's auc: 0.851949
[101]	valid_0's auc: 0.852458
[102]	valid_0's auc: 0.852844
[103]	valid_0's auc: 0.852972
[104]	valid_0's auc: 0.853099
[105]	valid_0's auc: 0.853374
[106]	valid_0's auc: 0.85374
[107]	valid_0's auc: 0.853903
[108]	valid_0's auc: 0.85429
[109]	valid_0's auc: 0.854605
[110]	valid_0's auc: 0.854842
[111]	valid_0's auc: 0.855263
[112]	valid_0's auc: 0.855593
[113]	valid_0's auc: 0.8558
[114]	valid_0's auc: 0.856154
[115]	valid_0's auc: 0.856483
[116]	valid_0's auc: 0.856697
[117]	valid_0's auc: 0.85693
[118]	valid_0's auc: 0.857322
[119]	valid_0's auc: 0.857799
[120]	valid_0's auc: 0.858246
[121]	valid_0's auc: 0.8583
[122]	valid_0's auc: 0.858581
[123]	valid_0's auc: 0.85884
[124]	valid_0's auc: 0.85902
[125]	valid_0's auc: 0.859169
[126]	valid_0's auc: 0.859384
[127]	valid_0's auc: 0.859807
[128]	valid_0's auc: 0.860023
[129]	valid_0's auc: 0.860214
[130]	valid_0's auc: 0.860311
[131]	valid_0's auc: 0.860583
[132]	valid_0's auc: 0.860752
[133]	valid_0's auc: 0.860889
[134]	valid_0's auc: 0.861049
[135]	valid_0's auc: 0.861321
[136]	valid_0's auc: 0.861537
[137]	valid_0's auc: 0.861887
[138]	valid_0's auc: 0.861968
[139]	valid_0's auc: 0.862037
[140]	valid_0's auc: 0.86234
[141]	valid_0's auc: 0.862543
[142]	valid_0's auc: 0.86281
[143]	valid_0's auc: 0.86308
[144]	valid_0's auc: 0.863332
[145]	valid_0's auc: 0.86343
[146]	valid_0's auc: 0.86364
[147]	valid_0's auc: 0.86393
[148]	valid_0's auc: 0.86419
[149]	valid_0's auc: 0.864365
[150]	valid_0's auc: 0.864571
[151]	valid_0's auc: 0.86468
[152]	valid_0's auc: 0.864991
[153]	valid_0's auc: 0.865157
[154]	valid_0's auc: 0.865266
[155]	valid_0's auc: 0.865478
[156]	valid_0's auc: 0.865712
[157]	valid_0's auc: 0.865775
[158]	valid_0's auc: 0.865916
[159]	valid_0's auc: 0.866088
[160]	valid_0's auc: 0.866188
[161]	valid_0's auc: 0.866532
[162]	valid_0's auc: 0.866622
[163]	valid_0's auc: 0.866929
[164]	valid_0's auc: 0.867023
[165]	valid_0's auc: 0.86723
[166]	valid_0's auc: 0.867471
[167]	valid_0's auc: 0.867765
[168]	valid_0's auc: 0.86792
[169]	valid_0's auc: 0.868084
[170]	valid_0's auc: 0.868251
[171]	valid_0's auc: 0.868613
[172]	valid_0's auc: 0.868705
[173]	valid_0's auc: 0.868818
[174]	valid_0's auc: 0.868912
[175]	valid_0's auc: 0.869184
[176]	valid_0's auc: 0.869504
[177]	valid_0's auc: 0.869744
[178]	valid_0's auc: 0.869976
[179]	valid_0's auc: 0.870146
[180]	valid_0's auc: 0.870213
[181]	valid_0's auc: 0.870412
[182]	valid_0's auc: 0.870643
[183]	valid_0's auc: 0.870755
[184]	valid_0's auc: 0.870892
[185]	valid_0's auc: 0.871069
[186]	valid_0's auc: 0.871234
[187]	valid_0's auc: 0.871539
[188]	valid_0's auc: 0.871609
[189]	valid_0's auc: 0.871737
[190]	valid_0's auc: 0.87193
[191]	valid_0's auc: 0.87209
[192]	valid_0's auc: 0.872283
[193]	valid_0's auc: 0.872523
[194]	valid_0's auc: 0.872583
[195]	valid_0's auc: 0.872757
[196]	valid_0's auc: 0.87278
[197]	valid_0's auc: 0.87292
[198]	valid_0's auc: 0.872948
[199]	valid_0's auc: 0.873051
[200]	valid_0's auc: 0.87319
[201]	valid_0's auc: 0.873239
[202]	valid_0's auc: 0.873328
[203]	valid_0's auc: 0.873434
[204]	valid_0's auc: 0.873506
[205]	valid_0's auc: 0.873646
[206]	valid_0's auc: 0.873673
[207]	valid_0's auc: 0.873666
[208]	valid_0's auc: 0.873805
[209]	valid_0's auc: 0.873915
[210]	valid_0's auc: 0.874127
[211]	valid_0's auc: 0.87435
[212]	valid_0's auc: 0.874471
[213]	valid_0's auc: 0.874609
[214]	valid_0's auc: 0.874675
[215]	valid_0's auc: 0.874739
[216]	valid_0's auc: 0.874835
[217]	valid_0's auc: 0.874963
[218]	valid_0's auc: 0.875107
[219]	valid_0's auc: 0.875177
[220]	valid_0's auc: 0.87525
[221]	valid_0's auc: 0.875456
[222]	valid_0's auc: 0.875564
[223]	valid_0's auc: 0.875717
[224]	valid_0's auc: 0.875908
[225]	valid_0's auc: 0.875944
[226]	valid_0's auc: 0.876036
[227]	valid_0's auc: 0.876152
[228]	valid_0's auc: 0.876309
[229]	valid_0's auc: 0.876397
[230]	valid_0's auc: 0.876484
[231]	valid_0's auc: 0.876579
[232]	valid_0's auc: 0.87668
[233]	valid_0's auc: 0.876783
[234]	valid_0's auc: 0.87685
[235]	valid_0's auc: 0.876924
[236]	valid_0's auc: 0.877034
[237]	valid_0's auc: 0.87722
[238]	valid_0's auc: 0.87729
[239]	valid_0's auc: 0.877389
[240]	valid_0's auc: 0.877467
[241]	valid_0's auc: 0.877464
[242]	valid_0's auc: 0.877483
[243]	valid_0's auc: 0.877595
[244]	valid_0's auc: 0.877681
[245]	valid_0's auc: 0.877742
[246]	valid_0's auc: 0.877834
[247]	valid_0's auc: 0.877837
[248]	valid_0's auc: 0.877961
[249]	valid_0's auc: 0.878068
[250]	valid_0's auc: 0.878042
[251]	valid_0's auc: 0.878176
[252]	valid_0's auc: 0.878406
[253]	valid_0's auc: 0.878464
[254]	valid_0's auc: 0.878544
[255]	valid_0's auc: 0.878604
[256]	valid_0's auc: 0.878681
[257]	valid_0's auc: 0.878757
[258]	valid_0's auc: 0.878876
[259]	valid_0's auc: 0.878897
[260]	valid_0's auc: 0.878938
[261]	valid_0's auc: 0.87904
[262]	valid_0's auc: 0.879074
[263]	valid_0's auc: 0.879106
[264]	valid_0's auc: 0.879232
[265]	valid_0's auc: 0.879267
[266]	valid_0's auc: 0.879277
[267]	valid_0's auc: 0.879445
[268]	valid_0's auc: 0.879487
[269]	valid_0's auc: 0.879598
[270]	valid_0's auc: 0.879627
[271]	valid_0's auc: 0.879712
[272]	valid_0's auc: 0.879808
[273]	valid_0's auc: 0.879843
[274]	valid_0's auc: 0.879941
[275]	valid_0's auc: 0.879935
[276]	valid_0's auc: 0.879994
[277]	valid_0's auc: 0.880084
[278]	valid_0's auc: 0.880163
[279]	valid_0's auc: 0.880225
[280]	valid_0's auc: 0.880322
[281]	valid_0's auc: 0.880426
[282]	valid_0's auc: 0.880562
[283]	valid_0's auc: 0.880548
[284]	valid_0's auc: 0.88065
[285]	valid_0's auc: 0.880692
[286]	valid_0's auc: 0.880679
[287]	valid_0's auc: 0.880737
[288]	valid_0's auc: 0.880759
[289]	valid_0's auc: 0.880767
[290]	valid_0's auc: 0.88078
[291]	valid_0's auc: 0.880819
[292]	valid_0's auc: 0.880904
[293]	valid_0's auc: 0.880952
[294]	valid_0's auc: 0.881121
[295]	valid_0's auc: 0.881122
[296]	valid_0's auc: 0.881123
[297]	valid_0's auc: 0.881165
[298]	valid_0's auc: 0.881284
[299]	valid_0's auc: 0.881272
[300]	valid_0's auc: 0.881354
[301]	valid_0's auc: 0.881488
[302]	valid_0's auc: 0.881506
[303]	valid_0's auc: 0.881631
[304]	valid_0's auc: 0.881659
[305]	valid_0's auc: 0.881662
[306]	valid_0's auc: 0.881762
[307]	valid_0's auc: 0.881822
[308]	valid_0's auc: 0.881863
[309]	valid_0's auc: 0.881915
[310]	valid_0's auc: 0.88198
[311]	valid_0's auc: 0.882063
[312]	valid_0's auc: 0.88214
[313]	valid_0's auc: 0.882241
[314]	valid_0's auc: 0.882275
[315]	valid_0's auc: 0.882318
[316]	valid_0's auc: 0.882365
[317]	valid_0's auc: 0.882467
[318]	valid_0's auc: 0.882526
[319]	valid_0's auc: 0.882484
[320]	valid_0's auc: 0.882503
[321]	valid_0's auc: 0.882604
[322]	valid_0's auc: 0.882675
[323]	valid_0's auc: 0.882784
[324]	valid_0's auc: 0.882803
[325]	valid_0's auc: 0.882839
[326]	valid_0's auc: 0.882869
[327]	valid_0's auc: 0.88292
[328]	valid_0's auc: 0.882992
[329]	valid_0's auc: 0.883026
[330]	valid_0's auc: 0.883026
[331]	valid_0's auc: 0.883105
[332]	valid_0's auc: 0.883145
[333]	valid_0's auc: 0.883168
[334]	valid_0's auc: 0.883275
[335]	valid_0's auc: 0.883376
[336]	valid_0's auc: 0.883448
[337]	valid_0's auc: 0.883525
[338]	valid_0's auc: 0.883516
[339]	valid_0's auc: 0.883512
[340]	valid_0's auc: 0.883549
[341]	valid_0's auc: 0.883671
[342]	valid_0's auc: 0.883683
[343]	valid_0's auc: 0.883694
[344]	valid_0's auc: 0.883703
[345]	valid_0's auc: 0.883736
[346]	valid_0's auc: 0.883814
[347]	valid_0's auc: 0.883816
[348]	valid_0's auc: 0.883877
[349]	valid_0's auc: 0.883883
[350]	valid_0's auc: 0.883879
[351]	valid_0's auc: 0.883949
[352]	valid_0's auc: 0.883985
[353]	valid_0's auc: 0.884005
[354]	valid_0's auc: 0.884049
[355]	valid_0's auc: 0.884114
[356]	valid_0's auc: 0.884214
[357]	valid_0's auc: 0.884269
[358]	valid_0's auc: 0.884265
[359]	valid_0's auc: 0.884213
[360]	valid_0's auc: 0.884229
[361]	valid_0's auc: 0.884282
[362]	valid_0's auc: 0.884395
[363]	valid_0's auc: 0.884449
[364]	valid_0's auc: 0.88448
[365]	valid_0's auc: 0.884518
[366]	valid_0's auc: 0.884541
[367]	valid_0's auc: 0.884608
[368]	valid_0's auc: 0.884642
[369]	valid_0's auc: 0.884668
[370]	valid_0's auc: 0.884668
[371]	valid_0's auc: 0.884677
[372]	valid_0's auc: 0.884631
[373]	valid_0's auc: 0.884649
[374]	valid_0's auc: 0.884732
[375]	valid_0's auc: 0.884777
[376]	valid_0's auc: 0.884792
[377]	valid_0's auc: 0.884777
[378]	valid_0's auc: 0.884803
[379]	valid_0's auc: 0.884826
[380]	valid_0's auc: 0.884938
[381]	valid_0's auc: 0.884951
[382]	valid_0's auc: 0.885003
[383]	valid_0's auc: 0.88498
[384]	valid_0's auc: 0.885015
[385]	valid_0's auc: 0.885021
[386]	valid_0's auc: 0.885097
[387]	valid_0's auc: 0.885064
[388]	valid_0's auc: 0.885115
[389]	valid_0's auc: 0.88512
[390]	valid_0's auc: 0.885147
[391]	valid_0's auc: 0.885221
[392]	valid_0's auc: 0.885292
[393]	valid_0's auc: 0.885319
[394]	valid_0's auc: 0.885356
[395]	valid_0's auc: 0.885429
[396]	valid_0's auc: 0.885487
[397]	valid_0's auc: 0.885555
[398]	valid_0's auc: 0.885591
[399]	valid_0's auc: 0.885658
[400]	valid_0's auc: 0.885695
[401]	valid_0's auc: 0.885773
[402]	valid_0's auc: 0.885782
[403]	valid_0's auc: 0.885856
[404]	valid_0's auc: 0.885918
[405]	valid_0's auc: 0.885943
[406]	valid_0's auc: 0.88603
[407]	valid_0's auc: 0.886119
[408]	valid_0's auc: 0.886211
[409]	valid_0's auc: 0.886231
[410]	valid_0's auc: 0.886183
[411]	valid_0's auc: 0.886211
[412]	valid_0's auc: 0.886227
[413]	valid_0's auc: 0.886268
[414]	valid_0's auc: 0.886299
[415]	valid_0's auc: 0.886355
[416]	valid_0's auc: 0.886378
[417]	valid_0's auc: 0.886388
[418]	valid_0's auc: 0.88634
[419]	valid_0's auc: 0.886393
[420]	valid_0's auc: 0.886389
[421]	valid_0's auc: 0.886374
[422]	valid_0's auc: 0.886376
[423]	valid_0's auc: 0.886407
[424]	valid_0's auc: 0.886401
[425]	valid_0's auc: 0.8864
[426]	valid_0's auc: 0.886411
[427]	valid_0's auc: 0.886418
[428]	valid_0's auc: 0.88644
[429]	valid_0's auc: 0.886473
[430]	valid_0's auc: 0.886515
[431]	valid_0's auc: 0.886522
[432]	valid_0's auc: 0.886508
[433]	valid_0's auc: 0.886472
[434]	valid_0's auc: 0.886524
[435]	valid_0's auc: 0.886582
[436]	valid_0's auc: 0.886662
[437]	valid_0's auc: 0.886657
[438]	valid_0's auc: 0.886736
[439]	valid_0's auc: 0.886751
[440]	valid_0's auc: 0.886782
[441]	valid_0's auc: 0.886757
[442]	valid_0's auc: 0.886714
[443]	valid_0's auc: 0.886692
[444]	valid_0's auc: 0.886744
[445]	valid_0's auc: 0.886803
[446]	valid_0's auc: 0.886824
[447]	valid_0's auc: 0.886841
[448]	valid_0's auc: 0.886897
[449]	valid_0's auc: 0.886911
[450]	valid_0's auc: 0.886913
[451]	valid_0's auc: 0.886927
[452]	valid_0's auc: 0.886896
[453]	valid_0's auc: 0.886908
[454]	valid_0's auc: 0.886945
[455]	valid_0's auc: 0.886989
[456]	valid_0's auc: 0.887054
[457]	valid_0's auc: 0.887073
[458]	valid_0's auc: 0.887105
[459]	valid_0's auc: 0.887104
[460]	valid_0's auc: 0.887103
[461]	valid_0's auc: 0.887127
[462]	valid_0's auc: 0.887096
[463]	valid_0's auc: 0.887122
[464]	valid_0's auc: 0.887119
[465]	valid_0's auc: 0.887128
[466]	valid_0's auc: 0.887132
[467]	valid_0's auc: 0.887087
[468]	valid_0's auc: 0.887061
[469]	valid_0's auc: 0.887089
[470]	valid_0's auc: 0.887081
[471]	valid_0's auc: 0.887106
[472]	valid_0's auc: 0.887151
[473]	valid_0's auc: 0.887209
[474]	valid_0's auc: 0.887283
[475]	valid_0's auc: 0.887246
[476]	valid_0's auc: 0.887297
[477]	valid_0's auc: 0.887328
[478]	valid_0's auc: 0.887305
[479]	valid_0's auc: 0.887308
[480]	valid_0's auc: 0.887346
[481]	valid_0's auc: 0.887355
[482]	valid_0's auc: 0.887302
[483]	valid_0's auc: 0.887291
[484]	valid_0's auc: 0.887331
[485]	valid_0's auc: 0.887322
[486]	valid_0's auc: 0.887274
[487]	valid_0's auc: 0.887278
[488]	valid_0's auc: 0.887268
[489]	valid_0's auc: 0.887254
[490]	valid_0's auc: 0.887228
[491]	valid_0's auc: 0.887275
[492]	valid_0's auc: 0.887305
[493]	valid_0's auc: 0.887347
[494]	valid_0's auc: 0.887386
[495]	valid_0's auc: 0.887399
[496]	valid_0's auc: 0.88745
[497]	valid_0's auc: 0.887476
[498]	valid_0's auc: 0.887463
[499]	valid_0's auc: 0.887512
[500]	valid_0's auc: 0.887514
[501]	valid_0's auc: 0.88755
[502]	valid_0's auc: 0.887617
[503]	valid_0's auc: 0.887638
[504]	valid_0's auc: 0.887669
[505]	valid_0's auc: 0.887673
[506]	valid_0's auc: 0.887687
[507]	valid_0's auc: 0.887667
[508]	valid_0's auc: 0.887711
[509]	valid_0's auc: 0.887689
[510]	valid_0's auc: 0.887709
[511]	valid_0's auc: 0.887728
[512]	valid_0's auc: 0.887737
[513]	valid_0's auc: 0.887705
[514]	valid_0's auc: 0.887702
[515]	valid_0's auc: 0.887754
[516]	valid_0's auc: 0.887736
[517]	valid_0's auc: 0.887745
[518]	valid_0's auc: 0.887756
[519]	valid_0's auc: 0.887768
[520]	valid_0's auc: 0.887799
[521]	valid_0's auc: 0.887829
[522]	valid_0's auc: 0.887859
[523]	valid_0's auc: 0.88785
[524]	valid_0's auc: 0.887887
[525]	valid_0's auc: 0.887901
[526]	valid_0's auc: 0.88791
[527]	valid_0's auc: 0.887872
[528]	valid_0's auc: 0.887866
[529]	valid_0's auc: 0.887901
[530]	valid_0's auc: 0.887902
[531]	valid_0's auc: 0.887943
[532]	valid_0's auc: 0.88795
[533]	valid_0's auc: 0.887976
[534]	valid_0's auc: 0.888008
[535]	valid_0's auc: 0.888045
[536]	valid_0's auc: 0.888095
[537]	valid_0's auc: 0.888066
[538]	valid_0's auc: 0.888082
[539]	valid_0's auc: 0.888104
[540]	valid_0's auc: 0.888093
[541]	valid_0's auc: 0.888128
[542]	valid_0's auc: 0.88812
[543]	valid_0's auc: 0.888089
[544]	valid_0's auc: 0.88808
[545]	valid_0's auc: 0.888096
[546]	valid_0's auc: 0.888093
[547]	valid_0's auc: 0.888117
[548]	valid_0's auc: 0.888162
[549]	valid_0's auc: 0.88816
[550]	valid_0's auc: 0.888211
[551]	valid_0's auc: 0.888164
[552]	valid_0's auc: 0.888178
[553]	valid_0's auc: 0.888213
[554]	valid_0's auc: 0.888244
[555]	valid_0's auc: 0.888189
[556]	valid_0's auc: 0.888195
[557]	valid_0's auc: 0.888217
[558]	valid_0's auc: 0.888192
[559]	valid_0's auc: 0.888237
[560]	valid_0's auc: 0.888277
[561]	valid_0's auc: 0.888286
[562]	valid_0's auc: 0.888247
[563]	valid_0's auc: 0.888275
[564]	valid_0's auc: 0.888318
[565]	valid_0's auc: 0.888305
[566]	valid_0's auc: 0.888331
[567]	valid_0's auc: 0.888326
[568]	valid_0's auc: 0.888351
[569]	valid_0's auc: 0.888322
[570]	valid_0's auc: 0.888307
[571]	valid_0's auc: 0.888346
[572]	valid_0's auc: 0.888335
[573]	valid_0's auc: 0.888371
[574]	valid_0's auc: 0.888365
[575]	valid_0's auc: 0.88839
[576]	valid_0's auc: 0.888389
[577]	valid_0's auc: 0.888408
[578]	valid_0's auc: 0.888422
[579]	valid_0's auc: 0.888419
[580]	valid_0's auc: 0.888432
[581]	valid_0's auc: 0.888496
[582]	valid_0's auc: 0.888521
[583]	valid_0's auc: 0.888548
[584]	valid_0's auc: 0.888569
[585]	valid_0's auc: 0.88861
[586]	valid_0's auc: 0.888611
[587]	valid_0's auc: 0.888619
[588]	valid_0's auc: 0.888647
[589]	valid_0's auc: 0.888669
[590]	valid_0's auc: 0.888644
[591]	valid_0's auc: 0.888662
[592]	valid_0's auc: 0.888603
[593]	valid_0's auc: 0.888613
[594]	valid_0's auc: 0.888594
[595]	valid_0's auc: 0.888598
[596]	valid_0's auc: 0.888602
[597]	valid_0's auc: 0.888617
[598]	valid_0's auc: 0.888601
[599]	valid_0's auc: 0.888628
[600]	valid_0's auc: 0.888664
[601]	valid_0's auc: 0.888683
[602]	valid_0's auc: 0.888699
[603]	valid_0's auc: 0.888746
[604]	valid_0's auc: 0.888774
[605]	valid_0's auc: 0.888786
[606]	valid_0's auc: 0.888797
[607]	valid_0's auc: 0.888765
[608]	valid_0's auc: 0.888817
[609]	valid_0's auc: 0.888815
[610]	valid_0's auc: 0.888852
[611]	valid_0's auc: 0.888865
[612]	valid_0's auc: 0.888858
[613]	valid_0's auc: 0.888864
[614]	valid_0's auc: 0.888886
[615]	valid_0's auc: 0.888932
[616]	valid_0's auc: 0.888945
[617]	valid_0's auc: 0.888984
[618]	valid_0's auc: 0.889009
[619]	valid_0's auc: 0.889078
[620]	valid_0's auc: 0.889036
[621]	valid_0's auc: 0.889005
[622]	valid_0's auc: 0.889028
[623]	valid_0's auc: 0.889035
[624]	valid_0's auc: 0.889061
[625]	valid_0's auc: 0.889053
[626]	valid_0's auc: 0.889055
[627]	valid_0's auc: 0.889052
[628]	valid_0's auc: 0.889057
[629]	valid_0's auc: 0.889007
[630]	valid_0's auc: 0.889033
[631]	valid_0's auc: 0.889087
[632]	valid_0's auc: 0.889091
[633]	valid_0's auc: 0.889085
[634]	valid_0's auc: 0.889125
[635]	valid_0's auc: 0.889172
[636]	valid_0's auc: 0.889181
[637]	valid_0's auc: 0.889174
[638]	valid_0's auc: 0.889184
[639]	valid_0's auc: 0.889237
[640]	valid_0's auc: 0.889211
[641]	valid_0's auc: 0.889213
[642]	valid_0's auc: 0.889213
[643]	valid_0's auc: 0.889185
[644]	valid_0's auc: 0.889202
[645]	valid_0's auc: 0.889217
[646]	valid_0's auc: 0.889243
[647]	valid_0's auc: 0.889223
[648]	valid_0's auc: 0.889214
[649]	valid_0's auc: 0.889184
[650]	valid_0's auc: 0.88921
[651]	valid_0's auc: 0.889214
[652]	valid_0's auc: 0.889211
[653]	valid_0's auc: 0.889257
[654]	valid_0's auc: 0.889222
[655]	valid_0's auc: 0.889235
[656]	valid_0's auc: 0.889259
[657]	valid_0's auc: 0.889234
[658]	valid_0's auc: 0.889217
[659]	valid_0's auc: 0.889206
[660]	valid_0's auc: 0.889262
[661]	valid_0's auc: 0.889242
[662]	valid_0's auc: 0.889282
[663]	valid_0's auc: 0.889309
[664]	valid_0's auc: 0.889309
[665]	valid_0's auc: 0.889349
[666]	valid_0's auc: 0.889323
[667]	valid_0's auc: 0.889329
[668]	valid_0's auc: 0.889311
[669]	valid_0's auc: 0.889267
[670]	valid_0's auc: 0.889288
[671]	valid_0's auc: 0.889276
[672]	valid_0's auc: 0.889277
[673]	valid_0's auc: 0.889245
[674]	valid_0's auc: 0.889264
[675]	valid_0's auc: 0.889287
[676]	valid_0's auc: 0.889329
[677]	valid_0's auc: 0.889365
[678]	valid_0's auc: 0.889345
[679]	valid_0's auc: 0.88938
[680]	valid_0's auc: 0.889391
[681]	valid_0's auc: 0.889345
[682]	valid_0's auc: 0.889354
[683]	valid_0's auc: 0.889361
[684]	valid_0's auc: 0.889386
[685]	valid_0's auc: 0.889372
[686]	valid_0's auc: 0.889335
[687]	valid_0's auc: 0.889373
[688]	valid_0's auc: 0.889418
[689]	valid_0's auc: 0.889404
[690]	valid_0's auc: 0.889403
[691]	valid_0's auc: 0.889398
[692]	valid_0's auc: 0.889381
[693]	valid_0's auc: 0.889381
[694]	valid_0's auc: 0.88938
[695]	valid_0's auc: 0.889353
[696]	valid_0's auc: 0.889359
[697]	valid_0's auc: 0.889373
[698]	valid_0's auc: 0.889372
[699]	valid_0's auc: 0.889366
[700]	valid_0's auc: 0.889361
[701]	valid_0's auc: 0.889356
[702]	valid_0's auc: 0.889352
[703]	valid_0's auc: 0.889337
[704]	valid_0's auc: 0.889345
[705]	valid_0's auc: 0.889313
[706]	valid_0's auc: 0.889341
[707]	valid_0's auc: 0.889327
[708]	valid_0's auc: 0.889293
[709]	valid_0's auc: 0.88929
[710]	valid_0's auc: 0.889313
[711]	valid_0's auc: 0.88933
[712]	valid_0's auc: 0.889335
[713]	valid_0's auc: 0.889331
[714]	valid_0's auc: 0.889333
[715]	valid_0's auc: 0.889372
[716]	valid_0's auc: 0.889366
[717]	valid_0's auc: 0.889367
[718]	valid_0's auc: 0.889389
[719]	valid_0's auc: 0.889374
[720]	valid_0's auc: 0.889368
[721]	valid_0's auc: 0.889361
[722]	valid_0's auc: 0.889367
[723]	valid_0's auc: 0.889362
[724]	valid_0's auc: 0.889355
[725]	valid_0's auc: 0.889318
[726]	valid_0's auc: 0.889303
[727]	valid_0's auc: 0.889286
[728]	valid_0's auc: 0.889288
[729]	valid_0's auc: 0.88925
[730]	valid_0's auc: 0.889173
[731]	valid_0's auc: 0.889203
[732]	valid_0's auc: 0.8892
[733]	valid_0's auc: 0.889206
[734]	valid_0's auc: 0.88924
[735]	valid_0's auc: 0.889218
[736]	valid_0's auc: 0.889193
[737]	valid_0's auc: 0.889216
[738]	valid_0's auc: 0.889226
[739]	valid_0's auc: 0.889198
[740]	valid_0's auc: 0.889214
[741]	valid_0's auc: 0.889223
[742]	valid_0's auc: 0.889227
[743]	valid_0's auc: 0.889182
[744]	valid_0's auc: 0.889213
[745]	valid_0's auc: 0.889185
[746]	valid_0's auc: 0.88924
[747]	valid_0's auc: 0.889233
[748]	valid_0's auc: 0.889254
[749]	valid_0's auc: 0.889251
[750]	valid_0's auc: 0.889268
[751]	valid_0's auc: 0.889289
[752]	valid_0's auc: 0.889306
[753]	valid_0's auc: 0.889296
[754]	valid_0's auc: 0.88931
[755]	valid_0's auc: 0.8893
[756]	valid_0's auc: 0.889301
[757]	valid_0's auc: 0.8893
[758]	valid_0's auc: 0.889323
[759]	valid_0's auc: 0.889294
[760]	valid_0's auc: 0.88926
[761]	valid_0's auc: 0.889243
[762]	valid_0's auc: 0.889198
[763]	valid_0's auc: 0.889158
[764]	valid_0's auc: 0.889128
[765]	valid_0's auc: 0.889136
[766]	valid_0's auc: 0.88916
[767]	valid_0's auc: 0.889172
[768]	valid_0's auc: 0.889123
[769]	valid_0's auc: 0.889108
[770]	valid_0's auc: 0.889081
[771]	valid_0's auc: 0.889074
[772]	valid_0's auc: 0.889042
[773]	valid_0's auc: 0.889024
[774]	valid_0's auc: 0.88901
[775]	valid_0's auc: 0.889023
[776]	valid_0's auc: 0.889001
[777]	valid_0's auc: 0.888974
[778]	valid_0's auc: 0.888944
[779]	valid_0's auc: 0.888948
[780]	valid_0's auc: 0.889017
[781]	valid_0's auc: 0.889019
[782]	valid_0's auc: 0.888974
[783]	valid_0's auc: 0.888957
[784]	valid_0's auc: 0.888947
[785]	valid_0's auc: 0.888928
[786]	valid_0's auc: 0.888961
[787]	valid_0's auc: 0.888954
[788]	valid_0's auc: 0.888936
Early stopping, best iteration is:
[688]	valid_0's auc: 0.889418
Done Training.
#+end_example

Interestingly, upweighting positive examples didn't seem to improve auc (if anything it hurt). More on this.

Check feature importance:

#+BEGIN_SRC python :session :results file
# feature importances
# print('Feature importances:', list(gbm.feature_importance()))

lgb.plot_importance(gbm, max_num_features=20)
plt.savefig('feature_importance.png')
plt.close 
'feature_importance.png'
#+END_SRC

#+RESULTS:
[[file:feature_importance.png]]

* TODO Feature Engineering

We've noticed a lot of repeated entries within features, which perhaps indicates our anonymous features behave more like categorical variables than numerical variables. 

#+BEGIN_SRC python :session :results output 
preprocessed_train_dataframe = train_dataframe.copy()
preprocessed_validation_dataframe = validation_dataframe.copy()

def min_max_scale_dataframe(dataframe, train_dataframe):

    # scale each feature independently to the range [0,1] and apply the tranformation determined by 'train_dataframe' to 'dataframe'
    min = train_dataframe.min()
    max = train_dataframe.max()

    return (dataframe - min)/(max - min)

# scale train and validation according to data in train
#+END_SRC

#+RESULTS:

One-hot encoding:

#+BEGIN_SRC python :session :results output
def bin_and_one_hot(dataframe, num_bins_per_feature):
    """Bins each column of 'dataframe' into 'num_bins_per_feature' number of bins. Then returns a dataframe of one-hot encodings of bin membership."""

    # populate train_bins_dataframe with binned features
    bins_dataframe = pd.DataFrame()
    for column in dataframe.columns:
        bins_dataframe[column] = pd.cut(dataframe[column], bins=num_bins_per_feature)
    # one-hot encoding of our bins
    one_hot_bins_dataframe = pd.get_dummies(bins_dataframe)

    return one_hot_bins_dataframe
#+END_SRC

#+RESULTS:

Altogether in a pipeline:

#+BEGIN_SRC python :session :results output
def scale_bin_one_hot(dataframe, train_dataframe, num_bins_per_feature):
    """Scales 'dataframe' to approximately [0,1] according to train_dataframe min and max. Then bins each feature and one-hot encodes the binning."""

    scaled_dataframe = min_max_scale_dataframe(dataframe, train_dataframe)
    
    return bin_and_one_hot(scaled_dataframe, num_bins_per_feature)
#+END_SRC

#+RESULTS:



Let's see if we can simply run this through LightGBM and get sensible results:

#+BEGIN_SRC python :session :results output
import lightgbm as lgb

#number of bins per feature
num_bins = 8 

# scale features, bin, and one-hot encode bins
train_one_hot = scale_bin_one_hot(preprocessed_train_dataframe, preprocessed_train_dataframe, num_bins)
validation_one_hot = scale_bin_one_hot(preprocessed_validation_dataframe, preprocessed_train_dataframe, num_bins)

# add one-hot features to dataframe of original features
train_one_hot_and_original_features = np.array(pd.concat([train_dataframe, train_one_hot], axis=1))
validation_one_hot_and_original_features = np.array(pd.concat([validation_dataframe, validation_one_hot], axis=1))
#+END_SRC

#+RESULTS:

So far: our new features alone can get us auc 0.85, but combining them with old features shows feature importance dominated by old features.

NOTE: I have made a mistake on binning. Currently validation bins are binned according to themselves, not according to the training bins

#+BEGIN_SRC python :session :results output
# create dataset for lightgbm
lgb_train = lgb.Dataset(train_one_hot_and_original_features, train_labels)
lgb_eval = lgb.Dataset(validation_one_hot_and_original_features,
                       validation_labels,
                       reference=lgb_train)

params = {
    # default num_trees=100
    'num_trees': 1000,
    'objective': 'binary',
    'metric': 'auc',
    'learning_rate': 0.05,
    # Percentage of features to be used for each tree
    'feature_fraction': 0.9,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.8,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 7
}

print('Starting training...')

# train
gbm = lgb.train(params,
                lgb_train,
                valid_sets=lgb_eval,
                early_stopping_rounds=10)

print('Done Training.')
#+END_SRC

#+RESULTS:
#+begin_example
Starting training...
[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true.
This may cause significantly different results comparing to the previous versions of LightGBM.
Try to set boost_from_average=false, if your old models produce bad results
[LightGBM] [Info] Number of positive: 12839, number of negative: 115161
[LightGBM] [Info] Total Bins 54182
[LightGBM] [Info] Number of data: 128000, number of used features: 1791
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100305 -> initscore=-2.193844
[LightGBM] [Info] Start training from score -2.193844
[1]	valid_0's auc: 0.661916
Training until validation scores don't improve for 10 rounds
[2]	valid_0's auc: 0.676617
[3]	valid_0's auc: 0.695109
[4]	valid_0's auc: 0.701763
[5]	valid_0's auc: 0.710572
[6]	valid_0's auc: 0.717013
[7]	valid_0's auc: 0.722795
[8]	valid_0's auc: 0.724484
[9]	valid_0's auc: 0.728364
[10]	valid_0's auc: 0.735341
[11]	valid_0's auc: 0.742557
[12]	valid_0's auc: 0.746454
[13]	valid_0's auc: 0.751599
[14]	valid_0's auc: 0.75406
[15]	valid_0's auc: 0.757958
[16]	valid_0's auc: 0.760824
[17]	valid_0's auc: 0.765882
[18]	valid_0's auc: 0.767733
[19]	valid_0's auc: 0.771079
[20]	valid_0's auc: 0.773914
[21]	valid_0's auc: 0.776425
[22]	valid_0's auc: 0.778727
[23]	valid_0's auc: 0.781532
[24]	valid_0's auc: 0.783547
[25]	valid_0's auc: 0.785096
[26]	valid_0's auc: 0.78791
[27]	valid_0's auc: 0.78926
[28]	valid_0's auc: 0.790709
[29]	valid_0's auc: 0.792261
[30]	valid_0's auc: 0.794677
[31]	valid_0's auc: 0.796491
[32]	valid_0's auc: 0.797725
[33]	valid_0's auc: 0.799314
[34]	valid_0's auc: 0.799921
[35]	valid_0's auc: 0.800852
[36]	valid_0's auc: 0.802412
[37]	valid_0's auc: 0.803533
[38]	valid_0's auc: 0.804657
[39]	valid_0's auc: 0.806598
[40]	valid_0's auc: 0.807957
[41]	valid_0's auc: 0.808881
[42]	valid_0's auc: 0.809867
[43]	valid_0's auc: 0.81115
[44]	valid_0's auc: 0.812413
[45]	valid_0's auc: 0.813189
[46]	valid_0's auc: 0.814228
[47]	valid_0's auc: 0.816095
[48]	valid_0's auc: 0.817498
[49]	valid_0's auc: 0.81873
[50]	valid_0's auc: 0.81921
[51]	valid_0's auc: 0.820198
[52]	valid_0's auc: 0.820883
[53]	valid_0's auc: 0.821607
[54]	valid_0's auc: 0.822651
[55]	valid_0's auc: 0.823311
[56]	valid_0's auc: 0.824368
[57]	valid_0's auc: 0.825372
[58]	valid_0's auc: 0.826248
[59]	valid_0's auc: 0.827073
[60]	valid_0's auc: 0.828302
[61]	valid_0's auc: 0.828796
[62]	valid_0's auc: 0.82968
[63]	valid_0's auc: 0.830697
[64]	valid_0's auc: 0.831089
[65]	valid_0's auc: 0.831422
[66]	valid_0's auc: 0.832378
[67]	valid_0's auc: 0.833151
[68]	valid_0's auc: 0.833801
[69]	valid_0's auc: 0.834461
[70]	valid_0's auc: 0.835289
[71]	valid_0's auc: 0.835859
[72]	valid_0's auc: 0.836441
[73]	valid_0's auc: 0.83701
[74]	valid_0's auc: 0.837702
[75]	valid_0's auc: 0.83782
[76]	valid_0's auc: 0.838267
[77]	valid_0's auc: 0.838511
[78]	valid_0's auc: 0.838837
[79]	valid_0's auc: 0.839356
[80]	valid_0's auc: 0.839886
[81]	valid_0's auc: 0.840813
[82]	valid_0's auc: 0.841172
[83]	valid_0's auc: 0.841932
[84]	valid_0's auc: 0.842375
[85]	valid_0's auc: 0.842595
[86]	valid_0's auc: 0.843373
[87]	valid_0's auc: 0.84395
[88]	valid_0's auc: 0.844901
[89]	valid_0's auc: 0.845299
[90]	valid_0's auc: 0.845876
[91]	valid_0's auc: 0.846354
[92]	valid_0's auc: 0.84683
[93]	valid_0's auc: 0.846996
[94]	valid_0's auc: 0.847453
[95]	valid_0's auc: 0.847761
[96]	valid_0's auc: 0.848136
[97]	valid_0's auc: 0.848477
[98]	valid_0's auc: 0.8489
[99]	valid_0's auc: 0.849413
[100]	valid_0's auc: 0.849689
[101]	valid_0's auc: 0.850156
[102]	valid_0's auc: 0.850736
[103]	valid_0's auc: 0.851098
[104]	valid_0's auc: 0.851596
[105]	valid_0's auc: 0.851709
[106]	valid_0's auc: 0.852137
[107]	valid_0's auc: 0.852294
[108]	valid_0's auc: 0.852523
[109]	valid_0's auc: 0.852968
[110]	valid_0's auc: 0.853385
[111]	valid_0's auc: 0.853633
[112]	valid_0's auc: 0.853776
[113]	valid_0's auc: 0.854132
[114]	valid_0's auc: 0.854658
[115]	valid_0's auc: 0.855001
[116]	valid_0's auc: 0.855177
[117]	valid_0's auc: 0.855515
[118]	valid_0's auc: 0.85586
[119]	valid_0's auc: 0.85604
[120]	valid_0's auc: 0.85619
[121]	valid_0's auc: 0.856444
[122]	valid_0's auc: 0.856819
[123]	valid_0's auc: 0.85701
[124]	valid_0's auc: 0.857148
[125]	valid_0's auc: 0.857393
[126]	valid_0's auc: 0.857529
[127]	valid_0's auc: 0.857509
[128]	valid_0's auc: 0.857781
[129]	valid_0's auc: 0.858065
[130]	valid_0's auc: 0.858131
[131]	valid_0's auc: 0.858297
[132]	valid_0's auc: 0.858713
[133]	valid_0's auc: 0.858963
[134]	valid_0's auc: 0.85935
[135]	valid_0's auc: 0.859468
[136]	valid_0's auc: 0.859823
[137]	valid_0's auc: 0.860144
[138]	valid_0's auc: 0.860308
[139]	valid_0's auc: 0.860585
[140]	valid_0's auc: 0.860753
[141]	valid_0's auc: 0.860948
[142]	valid_0's auc: 0.861348
[143]	valid_0's auc: 0.861464
[144]	valid_0's auc: 0.861628
[145]	valid_0's auc: 0.861963
[146]	valid_0's auc: 0.862027
[147]	valid_0's auc: 0.862357
[148]	valid_0's auc: 0.862583
[149]	valid_0's auc: 0.862632
[150]	valid_0's auc: 0.862983
[151]	valid_0's auc: 0.863197
[152]	valid_0's auc: 0.863348
[153]	valid_0's auc: 0.863649
[154]	valid_0's auc: 0.863912
[155]	valid_0's auc: 0.864057
[156]	valid_0's auc: 0.864368
[157]	valid_0's auc: 0.864564
[158]	valid_0's auc: 0.864836
[159]	valid_0's auc: 0.865058
[160]	valid_0's auc: 0.865197
[161]	valid_0's auc: 0.865309
[162]	valid_0's auc: 0.865474
[163]	valid_0's auc: 0.865522
[164]	valid_0's auc: 0.865786
[165]	valid_0's auc: 0.866052
[166]	valid_0's auc: 0.866162
[167]	valid_0's auc: 0.866472
[168]	valid_0's auc: 0.866774
[169]	valid_0's auc: 0.866977
[170]	valid_0's auc: 0.867241
[171]	valid_0's auc: 0.867429
[172]	valid_0's auc: 0.86765
[173]	valid_0's auc: 0.867892
[174]	valid_0's auc: 0.868021
[175]	valid_0's auc: 0.868154
[176]	valid_0's auc: 0.868279
[177]	valid_0's auc: 0.868502
[178]	valid_0's auc: 0.868669
[179]	valid_0's auc: 0.868776
[180]	valid_0's auc: 0.868977
[181]	valid_0's auc: 0.869178
[182]	valid_0's auc: 0.869229
[183]	valid_0's auc: 0.869229
[184]	valid_0's auc: 0.86937
[185]	valid_0's auc: 0.869566
[186]	valid_0's auc: 0.869674
[187]	valid_0's auc: 0.869704
[188]	valid_0's auc: 0.869931
[189]	valid_0's auc: 0.870059
[190]	valid_0's auc: 0.870249
[191]	valid_0's auc: 0.870348
[192]	valid_0's auc: 0.870519
[193]	valid_0's auc: 0.870903
[194]	valid_0's auc: 0.871066
[195]	valid_0's auc: 0.871253
[196]	valid_0's auc: 0.871431
[197]	valid_0's auc: 0.871666
[198]	valid_0's auc: 0.87185
[199]	valid_0's auc: 0.872048
[200]	valid_0's auc: 0.87219
[201]	valid_0's auc: 0.872275
[202]	valid_0's auc: 0.872463
[203]	valid_0's auc: 0.872551
[204]	valid_0's auc: 0.872682
[205]	valid_0's auc: 0.872823
[206]	valid_0's auc: 0.872945
[207]	valid_0's auc: 0.873033
[208]	valid_0's auc: 0.873011
[209]	valid_0's auc: 0.873096
[210]	valid_0's auc: 0.873189
[211]	valid_0's auc: 0.873242
[212]	valid_0's auc: 0.873265
[213]	valid_0's auc: 0.873349
[214]	valid_0's auc: 0.873335
[215]	valid_0's auc: 0.873601
[216]	valid_0's auc: 0.873796
[217]	valid_0's auc: 0.873933
[218]	valid_0's auc: 0.874051
[219]	valid_0's auc: 0.874169
[220]	valid_0's auc: 0.874255
[221]	valid_0's auc: 0.874413
[222]	valid_0's auc: 0.874487
[223]	valid_0's auc: 0.874646
[224]	valid_0's auc: 0.874626
[225]	valid_0's auc: 0.874724
[226]	valid_0's auc: 0.874734
[227]	valid_0's auc: 0.874829
[228]	valid_0's auc: 0.874914
[229]	valid_0's auc: 0.874947
[230]	valid_0's auc: 0.875097
[231]	valid_0's auc: 0.875185
[232]	valid_0's auc: 0.87531
[233]	valid_0's auc: 0.875391
[234]	valid_0's auc: 0.875457
[235]	valid_0's auc: 0.875543
[236]	valid_0's auc: 0.875641
[237]	valid_0's auc: 0.87584
[238]	valid_0's auc: 0.875979
[239]	valid_0's auc: 0.876071
[240]	valid_0's auc: 0.876194
[241]	valid_0's auc: 0.876395
[242]	valid_0's auc: 0.876455
[243]	valid_0's auc: 0.876496
[244]	valid_0's auc: 0.876478
[245]	valid_0's auc: 0.876414
[246]	valid_0's auc: 0.876523
[247]	valid_0's auc: 0.876687
[248]	valid_0's auc: 0.8768
[249]	valid_0's auc: 0.876865
[250]	valid_0's auc: 0.876976
[251]	valid_0's auc: 0.877094
[252]	valid_0's auc: 0.877102
[253]	valid_0's auc: 0.8772
[254]	valid_0's auc: 0.877312
[255]	valid_0's auc: 0.877316
[256]	valid_0's auc: 0.877389
[257]	valid_0's auc: 0.877511
[258]	valid_0's auc: 0.877645
[259]	valid_0's auc: 0.877751
[260]	valid_0's auc: 0.877867
[261]	valid_0's auc: 0.877895
[262]	valid_0's auc: 0.877855
[263]	valid_0's auc: 0.877982
[264]	valid_0's auc: 0.87804
[265]	valid_0's auc: 0.878094
[266]	valid_0's auc: 0.878203
[267]	valid_0's auc: 0.87828
[268]	valid_0's auc: 0.878463
[269]	valid_0's auc: 0.878602
[270]	valid_0's auc: 0.878608
[271]	valid_0's auc: 0.878581
[272]	valid_0's auc: 0.878564
[273]	valid_0's auc: 0.878598
[274]	valid_0's auc: 0.878614
[275]	valid_0's auc: 0.878707
[276]	valid_0's auc: 0.87875
[277]	valid_0's auc: 0.878835
[278]	valid_0's auc: 0.878887
[279]	valid_0's auc: 0.878963
[280]	valid_0's auc: 0.879088
[281]	valid_0's auc: 0.879185
[282]	valid_0's auc: 0.879271
[283]	valid_0's auc: 0.879288
[284]	valid_0's auc: 0.879333
[285]	valid_0's auc: 0.879421
[286]	valid_0's auc: 0.879487
[287]	valid_0's auc: 0.879542
[288]	valid_0's auc: 0.879576
[289]	valid_0's auc: 0.879618
[290]	valid_0's auc: 0.879589
[291]	valid_0's auc: 0.87966
[292]	valid_0's auc: 0.879775
[293]	valid_0's auc: 0.87992
[294]	valid_0's auc: 0.879961
[295]	valid_0's auc: 0.880001
[296]	valid_0's auc: 0.880032
[297]	valid_0's auc: 0.880093
[298]	valid_0's auc: 0.880121
[299]	valid_0's auc: 0.880243
[300]	valid_0's auc: 0.880212
[301]	valid_0's auc: 0.880188
[302]	valid_0's auc: 0.880253
[303]	valid_0's auc: 0.88034
[304]	valid_0's auc: 0.880393
[305]	valid_0's auc: 0.880433
[306]	valid_0's auc: 0.88047
[307]	valid_0's auc: 0.880518
[308]	valid_0's auc: 0.880605
[309]	valid_0's auc: 0.880673
[310]	valid_0's auc: 0.880828
[311]	valid_0's auc: 0.880833
[312]	valid_0's auc: 0.880976
[313]	valid_0's auc: 0.881063
[314]	valid_0's auc: 0.881086
[315]	valid_0's auc: 0.881167
[316]	valid_0's auc: 0.881175
[317]	valid_0's auc: 0.881278
[318]	valid_0's auc: 0.881296
[319]	valid_0's auc: 0.881339
[320]	valid_0's auc: 0.88135
[321]	valid_0's auc: 0.881397
[322]	valid_0's auc: 0.881483
[323]	valid_0's auc: 0.881587
[324]	valid_0's auc: 0.881683
[325]	valid_0's auc: 0.88175
[326]	valid_0's auc: 0.88184
[327]	valid_0's auc: 0.881895
[328]	valid_0's auc: 0.881935
[329]	valid_0's auc: 0.881953
[330]	valid_0's auc: 0.881947
[331]	valid_0's auc: 0.881951
[332]	valid_0's auc: 0.882025
[333]	valid_0's auc: 0.882079
[334]	valid_0's auc: 0.882083
[335]	valid_0's auc: 0.882086
[336]	valid_0's auc: 0.882162
[337]	valid_0's auc: 0.88222
[338]	valid_0's auc: 0.882297
[339]	valid_0's auc: 0.882354
[340]	valid_0's auc: 0.882421
[341]	valid_0's auc: 0.882457
[342]	valid_0's auc: 0.882504
[343]	valid_0's auc: 0.882558
[344]	valid_0's auc: 0.882612
[345]	valid_0's auc: 0.882698
[346]	valid_0's auc: 0.88269
[347]	valid_0's auc: 0.882728
[348]	valid_0's auc: 0.882855
[349]	valid_0's auc: 0.882926
[350]	valid_0's auc: 0.883002
[351]	valid_0's auc: 0.883128
[352]	valid_0's auc: 0.883234
[353]	valid_0's auc: 0.883313
[354]	valid_0's auc: 0.883246
[355]	valid_0's auc: 0.883265
[356]	valid_0's auc: 0.883312
[357]	valid_0's auc: 0.883397
[358]	valid_0's auc: 0.883423
[359]	valid_0's auc: 0.883443
[360]	valid_0's auc: 0.883477
[361]	valid_0's auc: 0.883546
[362]	valid_0's auc: 0.883636
[363]	valid_0's auc: 0.883662
[364]	valid_0's auc: 0.883747
[365]	valid_0's auc: 0.883792
[366]	valid_0's auc: 0.883855
[367]	valid_0's auc: 0.883887
[368]	valid_0's auc: 0.883935
[369]	valid_0's auc: 0.883926
[370]	valid_0's auc: 0.883889
[371]	valid_0's auc: 0.883959
[372]	valid_0's auc: 0.884014
[373]	valid_0's auc: 0.884096
[374]	valid_0's auc: 0.884141
[375]	valid_0's auc: 0.884159
[376]	valid_0's auc: 0.884183
[377]	valid_0's auc: 0.884171
[378]	valid_0's auc: 0.884246
[379]	valid_0's auc: 0.884284
[380]	valid_0's auc: 0.884311
[381]	valid_0's auc: 0.884351
[382]	valid_0's auc: 0.884295
[383]	valid_0's auc: 0.884316
[384]	valid_0's auc: 0.884332
[385]	valid_0's auc: 0.884345
[386]	valid_0's auc: 0.884347
[387]	valid_0's auc: 0.884373
[388]	valid_0's auc: 0.884448
[389]	valid_0's auc: 0.884421
[390]	valid_0's auc: 0.884492
[391]	valid_0's auc: 0.884482
[392]	valid_0's auc: 0.88453
[393]	valid_0's auc: 0.88453
[394]	valid_0's auc: 0.884594
[395]	valid_0's auc: 0.884636
[396]	valid_0's auc: 0.884685
[397]	valid_0's auc: 0.884715
[398]	valid_0's auc: 0.884705
[399]	valid_0's auc: 0.884634
[400]	valid_0's auc: 0.884669
[401]	valid_0's auc: 0.884702
[402]	valid_0's auc: 0.88472
[403]	valid_0's auc: 0.884762
[404]	valid_0's auc: 0.884825
[405]	valid_0's auc: 0.88492
[406]	valid_0's auc: 0.884951
[407]	valid_0's auc: 0.884958
[408]	valid_0's auc: 0.885004
[409]	valid_0's auc: 0.885017
[410]	valid_0's auc: 0.885025
[411]	valid_0's auc: 0.885035
[412]	valid_0's auc: 0.885058
[413]	valid_0's auc: 0.885112
[414]	valid_0's auc: 0.885165
[415]	valid_0's auc: 0.885226
[416]	valid_0's auc: 0.885278
[417]	valid_0's auc: 0.885268
[418]	valid_0's auc: 0.885317
[419]	valid_0's auc: 0.885378
[420]	valid_0's auc: 0.885415
[421]	valid_0's auc: 0.885478
[422]	valid_0's auc: 0.885477
[423]	valid_0's auc: 0.885481
[424]	valid_0's auc: 0.885529
[425]	valid_0's auc: 0.885529
[426]	valid_0's auc: 0.885531
[427]	valid_0's auc: 0.885544
[428]	valid_0's auc: 0.88557
[429]	valid_0's auc: 0.885559
[430]	valid_0's auc: 0.88557
[431]	valid_0's auc: 0.885678
[432]	valid_0's auc: 0.885709
[433]	valid_0's auc: 0.885749
[434]	valid_0's auc: 0.88576
[435]	valid_0's auc: 0.885743
[436]	valid_0's auc: 0.885772
[437]	valid_0's auc: 0.885806
[438]	valid_0's auc: 0.885861
[439]	valid_0's auc: 0.885839
[440]	valid_0's auc: 0.885911
[441]	valid_0's auc: 0.88595
[442]	valid_0's auc: 0.885967
[443]	valid_0's auc: 0.88602
[444]	valid_0's auc: 0.885981
[445]	valid_0's auc: 0.88605
[446]	valid_0's auc: 0.88604
[447]	valid_0's auc: 0.88607
[448]	valid_0's auc: 0.886101
[449]	valid_0's auc: 0.886129
[450]	valid_0's auc: 0.886146
[451]	valid_0's auc: 0.886146
[452]	valid_0's auc: 0.886144
[453]	valid_0's auc: 0.886153
[454]	valid_0's auc: 0.886136
[455]	valid_0's auc: 0.886108
[456]	valid_0's auc: 0.886183
[457]	valid_0's auc: 0.886221
[458]	valid_0's auc: 0.886227
[459]	valid_0's auc: 0.88624
[460]	valid_0's auc: 0.886268
[461]	valid_0's auc: 0.886312
[462]	valid_0's auc: 0.886405
[463]	valid_0's auc: 0.886436
[464]	valid_0's auc: 0.886502
[465]	valid_0's auc: 0.886567
[466]	valid_0's auc: 0.886573
[467]	valid_0's auc: 0.886578
[468]	valid_0's auc: 0.886658
[469]	valid_0's auc: 0.886696
[470]	valid_0's auc: 0.886761
[471]	valid_0's auc: 0.886771
[472]	valid_0's auc: 0.886795
[473]	valid_0's auc: 0.886782
[474]	valid_0's auc: 0.886766
[475]	valid_0's auc: 0.886792
[476]	valid_0's auc: 0.886843
[477]	valid_0's auc: 0.886915
[478]	valid_0's auc: 0.886962
[479]	valid_0's auc: 0.886963
[480]	valid_0's auc: 0.887
[481]	valid_0's auc: 0.88703
[482]	valid_0's auc: 0.887066
[483]	valid_0's auc: 0.887061
[484]	valid_0's auc: 0.887062
[485]	valid_0's auc: 0.887046
[486]	valid_0's auc: 0.887041
[487]	valid_0's auc: 0.887113
[488]	valid_0's auc: 0.88716
[489]	valid_0's auc: 0.887249
[490]	valid_0's auc: 0.887286
[491]	valid_0's auc: 0.887322
[492]	valid_0's auc: 0.88732
[493]	valid_0's auc: 0.887402
[494]	valid_0's auc: 0.887405
[495]	valid_0's auc: 0.887444
[496]	valid_0's auc: 0.887462
[497]	valid_0's auc: 0.887503
[498]	valid_0's auc: 0.887514
[499]	valid_0's auc: 0.88754
[500]	valid_0's auc: 0.887571
[501]	valid_0's auc: 0.887605
[502]	valid_0's auc: 0.887606
[503]	valid_0's auc: 0.887646
[504]	valid_0's auc: 0.887674
[505]	valid_0's auc: 0.887697
[506]	valid_0's auc: 0.887685
[507]	valid_0's auc: 0.887684
[508]	valid_0's auc: 0.887677
[509]	valid_0's auc: 0.887701
[510]	valid_0's auc: 0.887647
[511]	valid_0's auc: 0.887685
[512]	valid_0's auc: 0.887685
[513]	valid_0's auc: 0.887708
[514]	valid_0's auc: 0.887755
[515]	valid_0's auc: 0.88777
[516]	valid_0's auc: 0.887775
[517]	valid_0's auc: 0.887789
[518]	valid_0's auc: 0.887801
[519]	valid_0's auc: 0.88782
[520]	valid_0's auc: 0.88781
[521]	valid_0's auc: 0.887884
[522]	valid_0's auc: 0.887879
[523]	valid_0's auc: 0.88789
[524]	valid_0's auc: 0.887919
[525]	valid_0's auc: 0.887941
[526]	valid_0's auc: 0.887976
[527]	valid_0's auc: 0.887986
[528]	valid_0's auc: 0.887992
[529]	valid_0's auc: 0.887941
[530]	valid_0's auc: 0.887962
[531]	valid_0's auc: 0.887985
[532]	valid_0's auc: 0.888036
[533]	valid_0's auc: 0.888044
[534]	valid_0's auc: 0.888036
[535]	valid_0's auc: 0.888057
[536]	valid_0's auc: 0.888103
[537]	valid_0's auc: 0.888114
[538]	valid_0's auc: 0.88813
[539]	valid_0's auc: 0.888121
[540]	valid_0's auc: 0.888152
[541]	valid_0's auc: 0.888173
[542]	valid_0's auc: 0.888177
[543]	valid_0's auc: 0.8882
[544]	valid_0's auc: 0.888196
[545]	valid_0's auc: 0.888225
[546]	valid_0's auc: 0.888245
[547]	valid_0's auc: 0.888226
[548]	valid_0's auc: 0.88822
[549]	valid_0's auc: 0.88823
[550]	valid_0's auc: 0.888204
[551]	valid_0's auc: 0.888216
[552]	valid_0's auc: 0.888281
[553]	valid_0's auc: 0.888302
[554]	valid_0's auc: 0.888345
[555]	valid_0's auc: 0.888344
[556]	valid_0's auc: 0.888359
[557]	valid_0's auc: 0.888384
[558]	valid_0's auc: 0.888411
[559]	valid_0's auc: 0.88844
[560]	valid_0's auc: 0.888439
[561]	valid_0's auc: 0.888458
[562]	valid_0's auc: 0.888487
[563]	valid_0's auc: 0.888524
[564]	valid_0's auc: 0.888515
[565]	valid_0's auc: 0.888506
[566]	valid_0's auc: 0.888503
[567]	valid_0's auc: 0.88849
[568]	valid_0's auc: 0.888504
[569]	valid_0's auc: 0.888533
[570]	valid_0's auc: 0.888538
[571]	valid_0's auc: 0.888552
[572]	valid_0's auc: 0.888539
[573]	valid_0's auc: 0.888521
[574]	valid_0's auc: 0.888543
[575]	valid_0's auc: 0.888633
[576]	valid_0's auc: 0.8887
[577]	valid_0's auc: 0.888725
[578]	valid_0's auc: 0.888777
[579]	valid_0's auc: 0.888779
[580]	valid_0's auc: 0.88874
[581]	valid_0's auc: 0.88878
[582]	valid_0's auc: 0.888775
[583]	valid_0's auc: 0.888822
[584]	valid_0's auc: 0.888867
[585]	valid_0's auc: 0.8889
[586]	valid_0's auc: 0.888942
[587]	valid_0's auc: 0.888974
[588]	valid_0's auc: 0.88894
[589]	valid_0's auc: 0.888972
[590]	valid_0's auc: 0.888986
[591]	valid_0's auc: 0.888965
[592]	valid_0's auc: 0.888953
[593]	valid_0's auc: 0.888971
[594]	valid_0's auc: 0.888972
[595]	valid_0's auc: 0.888969
[596]	valid_0's auc: 0.888938
[597]	valid_0's auc: 0.88897
[598]	valid_0's auc: 0.888951
[599]	valid_0's auc: 0.888947
[600]	valid_0's auc: 0.888931
Early stopping, best iteration is:
[590]	valid_0's auc: 0.888986
Done Training.
#+end_example

#+BEGIN_SRC python :session :results file
# feature importances
# print('Feature importances:', list(gbm.feature_importance()))

lgb.plot_importance(gbm, max_num_features=20)
plt.savefig('feature_importance.png')
plt.close 
'feature_importance.png'
#+END_SRC

#+RESULTS:
[[file:feature_importance.png]]
