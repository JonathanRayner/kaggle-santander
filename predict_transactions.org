* Project Description + Kaggle Link 
The competition main page is [[https://www.kaggle.com/c/santander-customer-transaction-prediction/overview][Kaggle: Santander Customer Transaction Predictions]]

We are told:

"In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem."

We are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column. The task is to predict the value of target column in the test set.

I liked this as a toy model competition because the task and data are relatively simple, but the description of the data is quite lacking, so I have to think.
* Preprocessing
** Import data and modules, split into train/test/validation  
Import the data. At this stage, we don't even import the unlabelled test data used for the competition, because we need labels to evaluate our model. So instead we'll split our training data as train/validation/test.

#+BEGIN_SRC python :session :results silent 
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
# import xgboost as xgb

# import tensorflow as tf
# from tensorflow import keras
#+END_SRC


#+BEGIN_SRC python :session :results output 
raw_dataframe = pd.read_csv('train.csv')

# The ID_code column contains no information, so we remove it
raw_dataframe.pop('ID_code')

# Shuffle and split the data into train/validation/test dataframes
train_dataframe, test_dataframe = train_test_split(raw_dataframe, test_size=0.2)
train_dataframe, validation_dataframe = train_test_split(train_dataframe, test_size=0.2)

# Form np arrays of labels and features.
train_labels = np.array(train_dataframe.pop('target'))
validation_labels = np.array(validation_dataframe.pop('target'))
test_labels = np.array(test_dataframe.pop('target'))

train_features = np.array(train_dataframe)
validation_features = np.array(validation_dataframe)
test_features = np.array(test_dataframe)

#+END_SRC

#+RESULTS:

** Explore the data 
*** Look at the data and summary statistics 
#+BEGIN_SRC python :session
raw_dataframe.head()
#+END_SRC

#+RESULTS:
:    ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6    var_7   var_8   var_9  var_10  ...  var_187  var_188  var_189  var_190  var_191  var_192  var_193  var_194  var_195  var_196  var_197  var_198  var_199
: 0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187  18.6266 -4.9200  5.7470  2.9252  ... -19.7159  17.5743   0.5857   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914
: 1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208  16.5338  3.1468  8.0851 -0.4032  ... -15.9319  13.3175  -0.3566   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   8.1267   8.7889  18.3560   1.9518
: 2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427  14.6155 -4.9193  5.9525 -0.3249  ...  -6.2660  10.1934  -0.8417   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965
: 3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428  14.9250 -5.8609  8.2450  2.3061  ... -12.8279  12.4124   1.8489   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996
: 4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405  19.2514  6.2654  7.6784 -9.4458  ...   5.9270  16.0201  -0.2829  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104
: 
: [5 rows x 202 columns]
 
And some statistics about the data

#+BEGIN_SRC python :session
raw_dataframe.describe()
#+END_SRC

#+RESULTS:
#+begin_example
              target          var_0          var_1          var_2          var_3          var_4          var_5  ...        var_193        var_194        var_195        var_196        var_197        var_198        var_199
count  200000.000000  200000.000000  200000.000000  200000.000000  200000.000000  200000.000000  200000.000000  ...  200000.000000  200000.000000  200000.000000  200000.000000  200000.000000  200000.000000  200000.000000
mean        0.100490      10.679914      -1.627622      10.715192       6.796529      11.078333      -5.065317  ...       3.331774      17.993784      -0.142088       2.303335       8.908158      15.870720      -3.326537
std         0.300653       3.040051       4.050044       2.640894       2.043319       1.623150       7.863267  ...       3.992030       3.135162       1.429372       5.454369       0.921625       3.010945      10.438015
min         0.000000       0.408400     -15.043400       2.117100      -0.040200       5.074800     -32.562600  ...     -11.783400       8.694400      -5.261000     -14.209600       5.960600       6.299300     -38.852800
25%         0.000000       8.453850      -4.740025       8.722475       5.254075       9.883175     -11.200350  ...       0.584600      15.629800      -1.170700      -1.946925       8.252800      13.829700     -11.208475
50%         0.000000      10.524750      -1.608050      10.580000       6.825000      11.108250      -4.833150  ...       3.396350      17.957950      -0.172700       2.408900       8.888200      15.934050      -2.819550
75%         0.000000      12.758200       1.358625      12.516700       8.324100      12.261125       0.924800  ...       6.205800      20.396525       0.829600       6.556725       9.593300      18.064725       4.836800
max         1.000000      20.315000      10.376800      19.353000      13.188300      16.671400      17.251600  ...      18.281800      27.928800       4.272900      18.321500      12.000400      26.079100      28.500700

[8 rows x 201 columns]
#+end_example

We can note some things so far (~10% of binary targets are 1, rest 0, so we have an imbalanced classification problem, we can note approximate upper and lower bounds on the data, rough idea of the width of the distributions, all numeric data so no need to process categorical variables). 

*** Which naive features correlate with the target and with each other?

#+BEGIN_SRC python :session :results file
# pick out some features to draw correlations, and prepend 'target'
some_features = [f'var_{i}' for i in range (10)]
some_features.insert(0,'target')

# draw correlation heatmap
raw_correlations = raw_dataframe[some_features].corr()
plt.figure(figsize=(10,10))
sns.heatmap(raw_correlations, annot=True)
plt.savefig('raw_correlations.png')
plt.close()
'raw_correlations.png'
#+END_SRC

#+RESULTS:
[[file:raw_correlations.png]]

We don't notice any strong linear correlations, so we need to do some feature engineering/avoid linear models

*** Plot some dataframe rows

Let's plot all of the features for a given target on the same set of axes (perhaps this represents a sequence of transactions in time or something like that).

#+BEGIN_SRC python :session :results file 
j=15
plt.plot(train_features[j])
print([i for i,x in enumerate(train_labels[:100]) if x==1]) 
plt.savefig('sample_features_plot.png')
plt.close()
'sample_features_plot.png'
#+END_SRC

#+RESULTS:
[[file:sample_features_plot.png]]

*** Plot some histograms 

Plot a histogram of a column:

#+BEGIN_SRC python :session :results file
train_df['var_196'].hist()
plt.savefig('hist.png')
plt.close()
'hist.png'
#+END_SRC

#+RESULTS:
[[file:hist.png]]
 
Let's look at what all rows with target value 1 look like

#+BEGIN_SRC python :session
  raw_dataframe.loc[raw_dataframe['target']==1]
#+END_SRC

#+RESULTS:
#+begin_example
             ID_code  target    var_0   var_1    var_2   var_3    var_4    var_5   var_6    var_7   var_8   var_9  ...  var_188  var_189  var_190  var_191  var_192  var_193  var_194  var_195  var_196  var_197  var_198  var_199
13          train_13       1  16.3699  1.5934  16.7395  7.3330  12.1450   5.9004  4.8222  20.9729  1.1064  8.6978  ...  11.9586  -0.5899   7.4002   7.4031   4.3989   4.0978  17.3638  -1.3022   9.6846   9.0419  15.6064 -10.8529
29          train_29       1   5.3301 -2.6064  13.1913  3.1193   6.6483  -6.5659  5.9064  15.2341  1.2915  9.1168  ...  18.6375   0.1734   5.9215   7.9676   2.3405   1.1482  23.2168  -2.0105   3.7600   9.4513  17.4105 -14.6897
63          train_63       1   7.7072  0.0183   9.9974  8.3524   9.2886 -13.3627  6.0425  10.1108  1.3999  6.6710  ...  10.0679   1.9046   1.5832   5.0039   3.8814   7.4241  21.4844  -0.8297  -3.0468   7.5790  15.7685   5.4769
65          train_65       1  10.5358 -2.5439   8.7394  6.7548  14.4099  -3.8724  5.1584  15.8381  5.8204  9.0358  ...  10.2542   1.5517   4.6648   6.4227   3.4025  -4.0882  14.1174  -0.2472   5.3847   8.6949  15.1340   3.8449
71          train_71       1   6.7547  2.5973  14.2141  8.3514   7.4942  -1.3055  4.2336  15.0243 -1.8922  9.1282  ...  13.8773  -0.0899   1.4677   3.5935   2.0013   1.5777  18.2820  -4.3408   6.8869   9.3567  18.9013  13.3447
...              ...     ...      ...     ...      ...     ...      ...      ...     ...      ...     ...     ...  ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...
199966  train_199966       1  13.5797  2.5526   6.0512  5.2730  12.2182  -3.4048  7.3623  17.8372 -3.5604  8.8837  ...  20.7649  -0.4363   3.9023   7.9986   0.5213   2.3442  14.5510  -1.1530   8.9883   8.3389   9.5440   4.2493
199976  train_199976       1   7.9663 -2.8485   9.0919  7.3298   9.6690 -16.7872  4.5094  12.4351 -0.0113  8.5394  ...  20.1372   0.3380  10.7930   4.3876   3.7257   7.7038  14.7384   0.1561   1.5794   8.4627  14.3604  -1.6688
199981  train_199981       1  12.8140  0.6386  14.1657  7.1044   8.9365  -0.3274  6.5949  14.6078 -1.0373  8.8974  ...   7.0611   1.5463   4.8208   4.9010   2.2513   0.7308  14.7155   1.1464   5.5158   8.6519  16.0341   7.3809
199986  train_199986       1  12.0298 -8.7800   7.7071  7.4015   9.2305 -16.2174  5.9064  17.9268  3.6489  7.3970  ...   9.3059  -1.0691  16.7461   3.1249  -0.3943   8.4059  14.3367   3.0991   4.3853   8.8019  15.0031  -0.3659
199990  train_199990       1  14.1475  1.8568  11.0066  3.6779  12.1944 -16.5936  5.3217  14.8508  3.3377  6.1650  ...  16.0983   0.8156  -6.4708   4.7287   1.9034   7.2324  20.6047   1.7170  -4.0032   9.1627  13.8077  -1.9646

[20098 rows x 202 columns]
#+end_example

Plot a histogram across features for a given row that has target value 1.

#+BEGIN_SRC python :session :results file 
  raw_dataframe.iloc[13,2:].hist()

  # We need to save the figure to display inline in org mode. We also should use plt.close() so that we can respawn new different images without issues.
  plt.savefig('hist1.png')
  plt.close()
  'hist1.png'
#+END_SRC

#+RESULTS:
[[file:hist1.png]]

** Preprocessing
None yet. 

* DONE Basic Neural Net model 
CLOSED: [2020-01-29 Wed 20:01]
** No class weights 

Let's follow https://www.tensorflow.org/tutorials/structured_data/imbalanced_data to implement a basic Neural Net in Tensorflow. We'll use a single layer for benchmarking and optimize later. Most of the code is copy-pasted from the tutorial. 

Define the model and metrics

#+BEGIN_SRC python :session :results output
  METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'),
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
  ]

  # Note the option to use bias initialization, see http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines
  # We modify the tutorial to allow for different numbers of hidden units
  def make_model(metrics = METRICS, output_bias=None, hidden_units = 16):
      if output_bias is not None:
          output_bias = tf.keras.initializers.Constant(output_bias)
      model = keras.Sequential([
          keras.layers.Dense(hidden_units, activation='relu',
                            input_shape=(train_features.shape[-1],)),
          keras.layers.Dropout(0.2),
          keras.layers.Dense(1, activation='sigmoid',
                            bias_initializer=output_bias)
        ])


      model.compile(
          optimizer=keras.optimizers.Adam(lr=1e-3),
          loss=keras.losses.BinaryCrossentropy(),
          metrics=metrics)
      return model
#+END_SRC

#+RESULTS:

Build the model

#+BEGIN_SRC python :session :results output
EPOCHS = 100
BATCH_SIZE = 2048

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc', 
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)

model = make_model()
model.summary()

#+END_SRC


#+RESULTS:
#+begin_example
Model: "sequential_8"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_24 (Dense)             (None, 16)                3216      
_________________________________________________________________
dropout_16 (Dropout)         (None, 16)                0         
_________________________________________________________________
dense_25 (Dense)             (None, 1)                 17        
=================================================================
Total params: 3,233
Trainable params: 3,233
Non-trainable params: 0
_________________________________________________________________
#+end_example

Test run with a small amount of data

#+BEGIN_SRC python :session :results output
# Input numpy as a numpy array
model.predict(train_features[:10])
#+END_SRC

#+RESULTS:
#+begin_example
2020-01-26 21:39:16.948460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
array([[0.9997596 ],
       [0.5061394 ],
       [0.9303511 ],
       [0.7892672 ],
       [0.9999958 ],
       [0.9980216 ],
       [0.33681375],
       [0.35988793],
       [0.9976675 ],
       [0.9999008 ]], dtype=float32)
#+end_example

So far so good, let's follow the tutorial to set the initial bias as Log(pos/neg)

#+BEGIN_SRC python :session 
initial_bias = np.log(1/9)

model = make_model(output_bias = initial_bias)
model.predict(train_features[:10])

#+END_SRC

#+RESULTS:
|    0.94163775 |
|    0.92257184 |
|     0.8388229 |
|  0.0018517158 |
| 4.5338511e-05 |
|    0.27973586 |
|  0.0094071003 |
|   0.029227791 |
|    0.47860023 |
|  0.0087348791 |


Train the model

#+BEGIN_SRC python :session :results silent 
initial_bias = np.log(1/9)

model = make_model(hidden_units = 16, output_bias = initial_bias)

# Features and labels input as numpy arrays
baseline_history = model.fit(
    train_features,
    train_labels,
    batch_size=2048,
    epochs=500,
    # callbacks = [early_stopping],
    validation_data=(validation_features, validation_labels))
#+END_SRC

#+RESULTS:
: 2020-01-27 23:12:17.650343: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 204800000 exceeds 10% of system memory.
: 2020-01-27 23:12:17.912503: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 51200000 exceeds 10% of system memory.
: Train on 128000 samples, validate on 32000 samples
:   2048/128000 [..............................] - ETA: 7:54 - loss: 22.8784 - tp: 205.0000 - fp: 1806.0000 - tn: 30.0000 - fn: 7.0000 - accuracy: 0.1147 - precision: 0.1019 - recall: 0.9670 - auc: 0.4867  8192/128000 [>.............................] - ETA: 1:53 - loss: 16.3919 - tp: 788.0000 - fp: 6764.0000 - tn: 579.0000 - fn: 61.0000 - accuracy: 0.1669 - precision: 0.1043 - recall: 0.9282 - auc: 0.4966 22528/128000 [====>.........................] - ETA: 36s - loss: 7.9990 - tp: 1208.0000 - fp: 10592.0000 - tn: 9610.0000 - fn: 1118.0000 - accuracy: 0.4802 - precision: 0.1024 - recall: 0.5193 - auc: 0.4953 36864/128000 [=======>......................] - ETA: 19s - loss: 5.8956 - tp: 1216.0000 - fp: 10686.0000 - tn: 22399.0000 - fn: 2563.0000 - accuracy: 0.6406 - precision: 0.1022 - recall: 0.3218 - auc: 0.4991 51200/128000 [===========>..................] - ETA: 11s - loss: 5.0086 - tp: 1219.0000 - fp: 10718.0000 - tn: 35259.0000 - fn: 4004.0000 - accuracy: 0.7125 - precision: 0.1021 - recall: 0.2334 - auc: 0.5007 65536/128000 [==============>...............] - ETA: 7s - loss: 4.4345 - tp: 1229.0000 - fp: 10819.0000 - tn: 47999.0000 - fn: 5489.0000 - accuracy: 0.7512 - precision: 0.1020 - recall: 0.1829 - auc: 0.5004  79872/128000 [=================>............] - ETA: 4s - loss: 3.9526 - tp: 1285.0000 - fp: 11237.0000 - tn: 60502.0000 - fn: 6848.0000 - accuracy: 0.7736 - precision: 0.1026 - recall: 0.1580 - auc: 0.5015 94208/128000 [=====================>........] - ETA: 2s - loss: 3.5805 - tp: 1436.0000 - fp: 12531.0000 - tn: 72058.0000 - fn: 8183.0000 - accuracy: 0.7801 - precision: 0.1028 - recall: 0.1493 - auc: 0.5016108544/128000 [========================>.....] - ETA: 1s - loss: 3.2964 - tp: 1675.0000 - fp: 14593.0000 - tn: 82898.0000 - fn: 9378.0000 - accuracy: 0.7792 - precision: 0.1030 - recall: 0.1515 - auc: 0.5016122880/128000 [===========================>..] - ETA: 0s - loss: 3.0659 - tp: 1852.0000 - fp: 16176.0000 - tn: 94222.0000 - fn: 10630.0000 - accuracy: 0.7819 - precision: 0.1027 - recall: 0.1484 - auc: 0.5008128000/128000 [==============================] - 9s 70us/sample - loss: 2.9920 - tp: 1909.0000 - fp: 16609.0000 - tn: 98408.0000 - fn: 11074.0000 - accuracy: 0.7837 - precision: 0.1031 - recall: 0.1470 - auc: 0.5016 - val_loss: 0.9531 - val_tp: 65.0000 - val_fp: 342.0000 - val_tn: 28514.0000 - val_fn: 3079.0000 - val_accuracy: 0.8931 - val_precision: 0.1597 - val_recall: 0.0207 - val_auc: 0.5117
** With class weights

#+BEGIN_SRC python :session :results output
# total/negative examples, total/positive examples, factor of 1/2 according to https://www.tensorflow.org/tutorials/structured_data/imbalanced_data
weight_for_0 = (10.0/9.0)*1/2.0 
weight_for_1 = 10.0/2.0

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))

#+END_SRC

#+RESULTS:
: Weight for class 0: 0.56
: Weight for class 1: 5.00

#+BEGIN_SRC python :session :results silent
initial_bias = np.log(1/9)

weighted_model = make_model(hidden_units = 16, output_bias = initial_bias)

# features and labels input as numpy arrays
weighted_history = weighted_model.fit(
    train_features,
    train_labels,
    batch_size=2048*4,
    epochs=500,
    # callbacks = [early_stopping],
    validation_data=(validation_features, validation_labels),
    # The class weights go here
    class_weight=class_weight) 
#+END_SRC

** Plot some metrics 

Define function to plot metrics

#+BEGIN_SRC python :session :results output 

import matplotlib as mpl
def plot_metrics(history):
    metrics =  ['loss', 'auc', 'precision', 'recall']
    mpl.rcParams['figure.figsize'] = (12, 10)
    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
    plt.figure(figsize=(6,4))
 
    for n, metric in enumerate(metrics):
        name = metric.replace("_"," ").capitalize()
        plt.subplot(2,2,n+1)
        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')
        plt.plot(history.epoch, history.history['val_'+metric], color=colors[0], linestyle="--", label='Val')
        plt.xlabel('Epoch')
        plt.ylabel(name)
        if metric == 'loss':
            plt.ylim([0, plt.ylim()[1]])
        elif metric == 'auc':
            plt.ylim([0.8,1])
        else:
            plt.ylim([0,1])
    
    plt.legend()
    plt.savefig('metrics.png')
    plt.close()
#+END_SRC

#+RESULTS:

Display metrics plot

#+BEGIN_SRC python :session :results file
plot_metrics(weighted_history)
'metrics.png':pr
#+END_SRC

#+RESULTS:
[[file:metrics.png]]

* DONE Decision trees 
CLOSED: [2020-02-02 Sun 16:19]

Let's first run a basic Random Forest from sklearn. We'll use a blend of tutorials, with the FastAI lecture http://course18.fast.ai/lessonsml1/lesson2.html as backbone.

#+BEGIN_SRC python :session :results output
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.tree import export_graphviz
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session :results output 
# set up model parameters - for a start we can train a single small tree, with no probabilistic sample (no bootstrap), and tell it to use all of our cores.
rf = RandomForestClassifier(n_estimators=100, max_depth=15, max_features="sqrt", n_jobs=7)

# Train the model on training data
rf.fit(train_features, train_labels)

# makes predictions of probabilities on validation data 
predictions = rf.predict_proba(validation_features)

# calculate auc (note we only need second column of prediction probabilites - the probability of positive label)
auc = roc_auc_score(y_true=validation_labels, y_score=predictions[:,1])

# results:
# n_estimators=10, max_depth=10 , max_features="sqrt": 0.742, 20sec
# n_estimators=100, max_depth=10 , max_features="sqrt": 0.803, 40sec
# n_estimators=300, max_depth=10 , max_features="sqrt": 0.813, 2min 
# n_estimators=100, max_depth=15 , max_features="sqrt": 0.814, 1min 
# n_estimators=100, max_depth=25 , max_features="sqrt": 0.819, 2min 
# n_estimators=100, max_depth=15 , max_features="sqrt", class_weight={0:1,1:9}: 0.757, 2min 
# n_estimators=300, max_depth=15 , max_features="sqrt", class_weight={0:1,1:9}: 0.793, 3min 
# n_estimators=500, max_depth=25, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=7: 0.822, 6min
# n_estimators=1000, max_depth=42, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=7: 0.835, 15min
# n_estimators=2000, max_depth=42, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=7: 0.837, 23min
# n_estimators=1000, max_depth=42, max_features=40, class_weight={0:1,1:9}, n_jobs=7: 0.809, 35min
# n_estimators=2000, max_depth=25, max_features=40, class_weight={0:1,1:9}, n_jobs=7: 0.815, 55min
print(auc)
#+END_SRC

#+RESULTS:
: 0.8147160562178322

Let's draw the simple tree

#+BEGIN_SRC python :# export_graphviz(rf.estimators_[0], out_file=dotfile)
session :results output
#+END_SRC


** What if we scale our data? 
#+BEGIN_SRC python :session :results output
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# scale according to training data
train_features_scaled = scaler.fit_transform(train_features)

# apply the same transformation to validation data
val_features_scaled = scaler.transform(validation_features)

#+END_SRC

#+RESULTS:



# scaling

#+BEGIN_SRC python :session :results output
# set up model parameters
rf = RandomForestClassifier(n_estimators=100, max_depth=15, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=-1)

# Train the model on training data
rf.fit(train_features_scaled, train_labels)

# makes predictions on validation and print auc 
predictions = rf.predict_proba(val_features_scaled)
auc = roc_auc_score(y_true=validation_labels, y_score=predictions[:,1])

# results: 

print(auc)
#+END_SRC

#+RESULTS:
: 0.7583061694212846


Could purposefully samply less of the negative examples.
 
** What if we balance the data?



** Boosted trees in tensorflow 
* DONE GBT in tensorflow (abandoned)   
CLOSED: [2020-02-03 Mon 05:47]

Let's also benchmark with a basic GBT implementation. We follow the tensorflow tutorial at https://www.tensorflow.org/tutorials/estimator/boosted_trees.


#+BEGIN_SRC python :session :results output
# useful shorthand to reduce clutter
fc = tf.feature_column
# start by using all features
NUMERIC_COLUMNS = [f'var_{i}' for i in range(20)]

feature_columns = []

for feature_name in NUMERIC_COLUMNS:
    feature_columns.append(fc.numeric_column(feature_name, dtype=tf.float32))
    
#+END_SRC

#+RESULTS:

Input functions

#+BEGIN_SRC python :session :results output
def make_input_fn(feature_dataframe,
                  target_dataframe,
                  batch_size=1, n_epochs=None,
                  shuffle=True):

    """Args:
        feature_dataframe: pandas dataframe
        target_dataframe: pandas dataframe
        n_epochs: 'None' results in using as many epochs as needed
    """

    SHUFFLE_BUFFER_SIZE = 10000
    def input_fn():
        dataset = tf.data.Dataset.from_tensor_slices((dict(feature_dataframe), target_dataframe))
        dataset = dataset.batch(batch_size)
        dataset = dataset.repeat(n_epochs)
        if shuffle:
            dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)
        return dataset
    return input_fn

#+END_SRC

#+RESULTS:

Initaliaze GBT Classifier, train, and make predictions on validation data:

#+BEGIN_SRC python :session :results output
def train_GBT_classifier(feature_columns,
                         train_dataframe,
                         train_labels_dataframe,
                         validation_dataframe,
                         validation_labels_dataframe,
                         n_trees=100,
                         max_depth=6,
                         batch_size=100,
                         n_batches_per_layer=1):
    """Args:
        feature_columns: pandas dataframe
        train_dataframe: pandas dataframe
        train_labels_dataframe: pandas dataframe
        validation_dataframe: pandas dataframe
        validation_labels_dataframe: pandas dataframe
    """

    # Initialize the classifier
    GBT_classifier = tf.estimator.BoostedTreesClassifier(feature_columns,
                                                         n_trees=n_trees,
                                                         max_depth=max_depth,
                                                         n_batches_per_layer=n_batches_per_layer)

    # Train and validation input functions
    train_input_fn = make_input_fn(train_dataframe, train_labels_dataframe, batch_size, n_epochs=10)
    # Vaidation input function uses default batch_size=1
    val_input_fn = make_input_fn(validation_dataframe, validation_labels_dataframe,
                                 shuffle=False, n_epochs=1)

    # Train
    GBT_classifier.train(train_input_fn, max_steps=100)

    # Evaluate and print results
    result = GBT_classifier.evaluate(val_input_fn)
    print(pd.Series(result))

    # return classifier so we can make other predictions if needed
    return GBT_classifier

GBT_classifier = train_GBT_classifier(feature_columns,
                                      train_dataframe,
                                      train_labels_dataframe,
                                      validation_dataframe,
                                      validation_labels_dataframe,
                                      n_trees=10,
                                      max_depth=6,
                                      batch_size=1)

#+END_SRC

#+RESULTS:
#+begin_example
INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpymp5cnym
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpymp5cnym', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
WARNING:tensorflow:From /home/jonathan/.pyenv/versions/tensorflow_env/lib/python3.8/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1628: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /home/jonathan/.pyenv/versions/tensorflow_env/lib/python3.8/site-packages/tensorflow_core/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
2020-02-03 03:00:39.480198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-02-03 03:00:39.669985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:39.670331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GT 755M computeCapability: 3.0
coreClock: 1.0195GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 80.47GiB/s
2020-02-03 03:00:39.670362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2020-02-03 03:00:40.284232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-03 03:00:40.443930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-02-03 03:00:40.746711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-02-03 03:00:41.021440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-02-03 03:00:41.057930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-02-03 03:00:41.122727: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-02-03 03:00:41.123034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:41.123835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:41.124448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Graph was finalized.
2020-02-03 03:00:43.808162: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2395110000 Hz
2020-02-03 03:00:43.808721: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563bb9ecd630 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-03 03:00:43.808757: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-02-03 03:00:43.809025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:43.809507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GT 755M computeCapability: 3.0
coreClock: 1.0195GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 80.47GiB/s
2020-02-03 03:00:43.809563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2020-02-03 03:00:43.809597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-03 03:00:43.809627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-02-03 03:00:43.809654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-02-03 03:00:43.809682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-02-03 03:00:43.809710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-02-03 03:00:43.809739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-02-03 03:00:43.809825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:43.810297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:43.810697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-02-03 03:00:43.810750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2020-02-03 03:00:44.133690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-03 03:00:44.133725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-02-03 03:00:44.133732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-02-03 03:00:44.133918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:44.134301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:44.134658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:00:44.134990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1691 MB memory) -> physical GPU (device: 0, name: GeForce GT 755M, pci bus id: 0000:01:00.0, compute capability: 3.0)
2020-02-03 03:00:44.136577: I tensorflow/compiler/xla/service/platform_util.cc:205] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0
2020-02-03 03:00:44.136652: I tensorflow/compiler/jit/xla_gpu_device.cc:136] Ignoring visible XLA_GPU_JIT device. Device number is 0, reason: Internal: no supported devices found for platform CUDA
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpymp5cnym/model.ckpt.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:loss = 0.6931472, step = 0
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
INFO:tensorflow:Saving checkpoints for 60 into /tmp/tmpymp5cnym/model.ckpt.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Loss for final step: 0.08353096.
INFO:tensorflow:Calling model_fn.
WARNING:tensorflow:From /home/jonathan/.pyenv/versions/tensorflow_env/lib/python3.8/site-packages/tensorflow_core/python/ops/metrics_impl.py:2029: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING:tensorflow:From /home/jonathan/.pyenv/versions/tensorflow_env/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/head.py:617: auc (from tensorflow.python.ops.metrics_impl) is deprecated and will be removed in a future version.
Instructions for updating:
The value of AUC returned by this may race with the update so this is deprected. Please use tf.keras.metrics.AUC instead.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to "careful_interpolation" instead.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to "careful_interpolation" instead.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2020-02-03T03:01:30Z
INFO:tensorflow:Graph was finalized.
2020-02-03 03:01:30.822953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:01:30.844028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GT 755M computeCapability: 3.0
coreClock: 1.0195GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 80.47GiB/s
2020-02-03 03:01:30.857993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2020-02-03 03:01:30.884146: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-03 03:01:30.884308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-02-03 03:01:30.884376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-02-03 03:01:30.884433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-02-03 03:01:30.884487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-02-03 03:01:30.884567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-02-03 03:01:30.884761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:01:30.885395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:01:30.885863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-02-03 03:01:30.885921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-03 03:01:30.885946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-02-03 03:01:30.902420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-02-03 03:01:30.902808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:01:30.903429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-03 03:01:30.903929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1691 MB memory) -> physical GPU (device: 0, name: GeForce GT 755M, pci bus id: 0000:01:00.0, compute capability: 3.0)
2020-02-03 03:01:30.910430: I tensorflow/compiler/xla/service/platform_util.cc:205] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0
2020-02-03 03:01:30.910581: I tensorflow/compiler/jit/xla_gpu_device.cc:136] Ignoring visible XLA_GPU_JIT device. Device number is 0, reason: Internal: no supported devices found for platform CUDA
INFO:tensorflow:Restoring parameters from /tmp/tmpymp5cnym/model.ckpt-60
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Inference Time : 101.28555s
INFO:tensorflow:Finished evaluation at 2020-02-03-03:03:11
INFO:tensorflow:Saving dict for global step 60: accuracy = 0.90178126, accuracy_baseline = 0.90178126, auc = 0.5, auc_precision_recall = 0.5491094, average_loss = 0.32557878, global_step = 60, label/mean = 0.09821875, loss = 0.32557878, precision = 0.0, prediction/mean = 0.07250305, recall = 0.0
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 60: /tmp/tmpymp5cnym/model.ckpt-60
accuracy                 0.901781
accuracy_baseline        0.901781
auc                      0.500000
auc_precision_recall     0.549109
average_loss             0.325579
label/mean               0.098219
loss                     0.325579
precision                0.000000
prediction/mean          0.072503
recall                   0.000000
global_step             60.000000
dtype: float64
#+end_example

* DONE GBT in XGBoost (abandoned)
CLOSED: [2020-02-04 Tue 00:34]

Load our numpy arrays into DMatrixes:

#+BEGIN_SRC python :session :results output
xgb_train = xgb.DMatrix(train_features, label=train_labels)
xgb_validation = xgb.DMatrix(validation_features, label=validation_labels)
#+END_SRC

#+RESULTS:

Set Booster parameters:

#+BEGIN_SRC python :session :results output
param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}
param['nthread'] = 2 
param['eval_metric'] = 'auc'

# Our gpu is Cuda compute 3.0, so we can only use the CPU with XGBoost
param['predictor'] = 'cpu_predictor'

evallist = [(xgb_validation, 'eval'), (xgb_train, 'train')]
#+END_SRC

#+RESULTS:

Check environment variables

#+BEGIN_SRC python :session :results output

#+END_SRC


Train the model

#+BEGIN_SRC python :session :results output
num_round = 1
xgb_model = xgb.train(param, xgb_train, num_round, evallist)
#+END_SRC

* TODO GBT in LightGBM

From LightGBM documentation: best to use for larger datasets to avoid overfitting (> 10,000 rows).

#+BEGIN_SRC python :session :results output
import lightgbm as lgb

# create dataset for lightgbm
lgb_train = lgb.Dataset(train_features, train_labels)
lgb_eval = lgb.Dataset(validation_features, validation_labels, reference=lgb_train)
#+END_SRC

#+RESULTS:

Build the model

#+BEGIN_SRC python :session :results output
params = {
    # default num_trees=100
    'num_trees': 100,
    'objective': 'binary',
    'metric': 'auc',
    'learning_rate': 0.05,
    # Percentage of features to be used for each tree
    'feature_fraction': 0.9,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.8,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 3
}

print('Starting training...')

# train
gbm = lgb.train(params,
                lgb_train,
                valid_sets=lgb_eval,
                early_stopping_rounds=5)

print('Done Training.')
#+END_SRC

#+RESULTS:
#+begin_example
Starting training...
[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true.
This may cause significantly different results comparing to the previous versions of LightGBM.
Try to set boost_from_average=false, if your old models produce bad results
[LightGBM] [Info] Number of positive: 12878, number of negative: 115122
[LightGBM] [Info] Total Bins 51000
[LightGBM] [Info] Number of data: 128000, number of used features: 200
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100609 -> initscore=-2.190472
[LightGBM] [Info] Start training from score -2.190472
[1]	valid_0's auc: 0.660695
Training until validation scores don't improve for 5 rounds
[2]	valid_0's auc: 0.677007
[3]	valid_0's auc: 0.693477
[4]	valid_0's auc: 0.700085
[5]	valid_0's auc: 0.703131
[6]	valid_0's auc: 0.710871
[7]	valid_0's auc: 0.718926
[8]	valid_0's auc: 0.721378
[9]	valid_0's auc: 0.725975
[10]	valid_0's auc: 0.729144
[11]	valid_0's auc: 0.73441
[12]	valid_0's auc: 0.740965
[13]	valid_0's auc: 0.743782
[14]	valid_0's auc: 0.749134
[15]	valid_0's auc: 0.752978
[16]	valid_0's auc: 0.756099
[17]	valid_0's auc: 0.758655
[18]	valid_0's auc: 0.761661
[19]	valid_0's auc: 0.763746
[20]	valid_0's auc: 0.766656
[21]	valid_0's auc: 0.769965
[22]	valid_0's auc: 0.771914
[23]	valid_0's auc: 0.77475
[24]	valid_0's auc: 0.776519
[25]	valid_0's auc: 0.779397
[26]	valid_0's auc: 0.781422
[27]	valid_0's auc: 0.783406
[28]	valid_0's auc: 0.786332
[29]	valid_0's auc: 0.788075
[30]	valid_0's auc: 0.789435
[31]	valid_0's auc: 0.791363
[32]	valid_0's auc: 0.792688
[33]	valid_0's auc: 0.794369
[34]	valid_0's auc: 0.796652
[35]	valid_0's auc: 0.79826
[36]	valid_0's auc: 0.800336
[37]	valid_0's auc: 0.801892
[38]	valid_0's auc: 0.803452
[39]	valid_0's auc: 0.804993
[40]	valid_0's auc: 0.806662
[41]	valid_0's auc: 0.808334
[42]	valid_0's auc: 0.809452
[43]	valid_0's auc: 0.810692
[44]	valid_0's auc: 0.811903
[45]	valid_0's auc: 0.812926
[46]	valid_0's auc: 0.813716
[47]	valid_0's auc: 0.814588
[48]	valid_0's auc: 0.815627
[49]	valid_0's auc: 0.816531
[50]	valid_0's auc: 0.817757
[51]	valid_0's auc: 0.818403
[52]	valid_0's auc: 0.81959
[53]	valid_0's auc: 0.820741
[54]	valid_0's auc: 0.821847
[55]	valid_0's auc: 0.822642
[56]	valid_0's auc: 0.823743
[57]	valid_0's auc: 0.824287
[58]	valid_0's auc: 0.825146
[59]	valid_0's auc: 0.826079
[60]	valid_0's auc: 0.826825
[61]	valid_0's auc: 0.827825
[62]	valid_0's auc: 0.828731
[63]	valid_0's auc: 0.829781
[64]	valid_0's auc: 0.830431
[65]	valid_0's auc: 0.831212
[66]	valid_0's auc: 0.832123
[67]	valid_0's auc: 0.832945
[68]	valid_0's auc: 0.833694
[69]	valid_0's auc: 0.834384
[70]	valid_0's auc: 0.835299
[71]	valid_0's auc: 0.835421
[72]	valid_0's auc: 0.835996
[73]	valid_0's auc: 0.836303
[74]	valid_0's auc: 0.836866
[75]	valid_0's auc: 0.837331
[76]	valid_0's auc: 0.837997
[77]	valid_0's auc: 0.838671
[78]	valid_0's auc: 0.839416
[79]	valid_0's auc: 0.840085
[80]	valid_0's auc: 0.84068
[81]	valid_0's auc: 0.841208
[82]	valid_0's auc: 0.842012
[83]	valid_0's auc: 0.842042
[84]	valid_0's auc: 0.842754
[85]	valid_0's auc: 0.843041
[86]	valid_0's auc: 0.843607
[87]	valid_0's auc: 0.844166
[88]	valid_0's auc: 0.844825
[89]	valid_0's auc: 0.845227
[90]	valid_0's auc: 0.845791
[91]	valid_0's auc: 0.846074
[92]	valid_0's auc: 0.846825
[93]	valid_0's auc: 0.847057
[94]	valid_0's auc: 0.847863
[95]	valid_0's auc: 0.848039
[96]	valid_0's auc: 0.848477
[97]	valid_0's auc: 0.848895
[98]	valid_0's auc: 0.849389
[99]	valid_0's auc: 0.849956
[100]	valid_0's auc: 0.850412
Did not meet early stopping. Best iteration is:
[100]	valid_0's auc: 0.850412
Done Training.
#+end_example

Predict

#+BEGIN_SRC python :session :results output
print('Starting predicting...')

# predict
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)

# eval
print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)
#+END_SRC





* TODO Feature Engineering

- Polynomial features (sklearn) -> correlation with targets
- Identify most import features (functions, and via decision trees)
- PCA then 1+2?

* TODO LSTM? 
