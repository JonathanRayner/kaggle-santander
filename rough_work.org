* Project Description + Kaggle Link 
The competition main page is [[https://www.kaggle.com/c/santander-customer-transaction-prediction/overview][Kaggle: Santander Customer Transaction Predictions]]

We are told:

"In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem."

We are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column. The task is to predict the value of target column in the test set.

* Preprocessing
** Import data and modules, split into train/test/validation  
Import the data. At this stage, we don't even import the unlabelled test data used for the competition, because we need labels to evaluate our model. So instead we'll split our training data as train/validation/test.

#+BEGIN_SRC python :session :results silent 
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import lightgbm as lgb
from matplotlib import pyplot as plt
for correlations heatmap in data exploration
import seaborn as sns
## don't want to import tensorflow unless we need to
# import tensorflow as tf
# from tensorflow import keras
#+END_SRC


#+BEGIN_SRC python :session :results output
train = pd.read_csv('train.csv')

# The ID_code column contains no information, so we remove it
train.drop('ID_code', axis=1, inplace=True)

train, test = train_test_split(train, test_size=0.2)
train, validation = train_test_split(train, test_size=0.2)

train_labels = train.pop('target')
validation_labels = validation.pop('target')
test_labels = test.pop('target')
#+END_SRC

#+RESULTS:

** Explore the data 
*** Look at the data and summary statistics 

#+BEGIN_SRC python :session
# Make a copy of our dataframe in case we want to make modifications
exploratory_dataframe = raw_dataframe.copy()

# Print the first 10 rows of our dataframe
exploratory_dataframe.head(10)
#+END_SRC

#+RESULTS:
#+begin_example
   target    var_0   var_1    var_2   var_3    var_4    var_5   var_6    var_7  ...  var_191  var_192  var_193  var_194  var_195  var_196  var_197  var_198  var_199
0       0   8.9255 -6.7863  11.9081  5.0930  11.4607  -9.2834  5.1187  18.6266  ...   3.9642   3.1364   1.6910  18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914
1       0  11.5006 -4.1473  13.8588  5.3890  12.3622   7.0433  5.6208  16.5338  ...   7.7214   2.5837  10.9516  15.4305   2.0339   8.1267   8.7889  18.3560   1.9518
2       0   8.6093 -2.7457  12.0805  7.8928  10.5825  -9.0837  6.9427  14.6155  ...   9.7905   1.6704   1.6858  21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965
3       0  11.0604 -2.1518   8.9522  7.1957  12.5846  -1.8361  5.8428  14.9250  ...   4.7433   0.7178   1.4214  23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996
4       0   9.8369 -1.4834  12.8746  6.6375  12.2772   2.4486  5.9405  19.2514  ...   9.5214  -0.1508   9.1942  13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104
5       0  11.4763 -2.3182  12.6080  8.6264  10.9621   3.5609  4.5322  15.2255  ...   6.6025   5.2912   0.4403  14.9452   1.0314  -3.6241   9.7670  12.5809  -4.7602
6       0  11.8091 -0.0832   9.3494  4.2916  11.1355  -8.0198  6.1961  12.0771  ...   6.4521   3.5325   0.1777  18.3314   0.5845   9.1104   9.1143  10.8869  -3.2097
7       0  13.5580 -7.9881  13.8776  7.5985   8.6543   0.8310  5.6890  22.3262  ...   6.5491   3.9906   5.8061  23.1407  -0.3776   4.2178   9.4237   8.6624   3.4806
8       0  16.1071  2.4426  13.9307  5.6327   8.8014   6.1630  4.4514  10.1854  ...  14.7510   1.6395   1.4181  14.8370  -1.9940  -1.0733   8.1975  19.5114   4.8453
9       0  12.5088  1.9743   8.8960  5.4508  13.6043 -16.2859  6.0637  16.8410  ...   6.3160   1.0371   3.6885  14.8344   0.4467  14.1287   7.9133  16.2375  14.2514

[10 rows x 201 columns]
#+end_example
 
And some statistics about the data

#+BEGIN_SRC python :session
exploratory_dataframe.describe()
#+END_SRC

#+RESULTS:
#+begin_example
              target          var_0          var_1          var_2          var_3  ...        var_195        var_196        var_197        var_198        var_199
count  200000.000000  200000.000000  200000.000000  200000.000000  200000.000000  ...  200000.000000  200000.000000  200000.000000  200000.000000  200000.000000
mean        0.100490      10.679914      -1.627622      10.715192       6.796529  ...      -0.142088       2.303335       8.908158      15.870720      -3.326537
std         0.300653       3.040051       4.050044       2.640894       2.043319  ...       1.429372       5.454369       0.921625       3.010945      10.438015
min         0.000000       0.408400     -15.043400       2.117100      -0.040200  ...      -5.261000     -14.209600       5.960600       6.299300     -38.852800
25%         0.000000       8.453850      -4.740025       8.722475       5.254075  ...      -1.170700      -1.946925       8.252800      13.829700     -11.208475
50%         0.000000      10.524750      -1.608050      10.580000       6.825000  ...      -0.172700       2.408900       8.888200      15.934050      -2.819550
75%         0.000000      12.758200       1.358625      12.516700       8.324100  ...       0.829600       6.556725       9.593300      18.064725       4.836800
max         1.000000      20.315000      10.376800      19.353000      13.188300  ...       4.272900      18.321500      12.000400      26.079100      28.500700

[8 rows x 201 columns]
#+end_example

Missing values check:

#+BEGIN_SRC python :session :results value 
# counts the number of entries in each column and checks if this number is equal across all columns
exploratory_dataframe.count().nunique()
#+END_SRC

#+RESULTS:
: 1

Every feature has has a datapoint for all 200,000 rows. Let's check for repeated entries:

#+BEGIN_SRC python :session :results value 
# count how many unique entries each row has
unique_row_values = exploratory_dataframe.nunique(axis='columns')

# output the number of unique entries
unique_row_values.unique()
#+END_SRC

#+RESULTS:
| 201 | 200 | 199 | 198 |

These are unusual numbers. Maybe the targets are interfering or maybe a value is repeated here or there by coincidence. Let's check:

#+BEGIN_SRC python :session :results silent 
# rows that have entries that are repeated in the row (ie. < 201 unique values)
rows_with_duplicates = exploratory_dataframe[unique_row_values != 201]

# note which entries are duplicated within a row 
duplicate_entries_boolean = rows_with_duplicates.apply(lambda x: x.duplicated(keep=False), axis = 1)
#+END_SRC

#+RESULTS:
#+begin_example
target    var_0   var_1    var_2  ...  var_196  var_197  var_198  var_199
14           0  13.8080  5.0514  17.2611  ...  -3.5323   9.3439  24.4479  -5.1110
22           0  10.2031  0.1925  14.0238  ...  -7.5486   9.5064   8.7281 -25.6523
26           0  15.6567 -4.4950  10.4867  ...   3.4319   7.8821  19.3055  -7.5090
68           0   8.5576  1.4385  10.6548  ...   1.0236   8.1925  18.2969 -16.2097
85           0   5.5511 -6.0495   6.8957  ...  13.8302   9.7335  11.1988  -0.7338
...      ...     ...      ...  ...      ...      ...      ...      ...
199877       1  12.3381 -3.0178  10.9429  ...   6.8868   9.2086  16.3833   9.6348
199888       0  12.6929 -4.9290  10.7029  ...  14.1003  10.6589  19.0044  11.3123
199908       0  12.4229  1.8738  10.5611  ...   0.0101   9.2432  19.8261  -3.6446
199910       0  10.7423  0.2901   9.0327  ...   3.8474   7.9792  20.8257  -0.6774
199935       0  15.6192 -2.2020  11.0134  ...   1.1220  10.1649  17.7713  -9.2515

[12975 rows x 201 columns]
        target  var_0  var_1  var_2  ...  var_196  var_197  var_198  var_199
14       False  False  False  False  ...    False    False    False    False
22       False  False  False  False  ...    False    False    False    False
26       False  False  False  False  ...    False    False    False    False
68       False  False  False  False  ...    False    False    False    False
85       False  False  False  False  ...    False    False    False    False
...    ...    ...    ...  ...      ...      ...      ...      ...
199877   False  False  False  False  ...    False    False    False    False
199888   False  False  False  False  ...    False    False    False    False
199908   False  False  False  False  ...    False    False    False    False
199910   False  False  False  False  ...    False    False    False    False
199935   False  False  False   True  ...    False    False    False    False

[12975 rows x 201 columns]
#+end_example

Let's look at repeated values across columns and their corresponding features:

#+BEGIN_SRC python :session :results output  
# there are thousands of rows with dulicates, so let's only output the first 10
for row in rows_with_duplicates.index[0:10]:
    
    # For each row, find which entries are repeated entries (duplicate_entries_boolean == True) and then output these values with their corresponding columns headings 
    print(rows_with_duplicates.loc[row,
                                    duplicate_entries_boolean.loc[row] == True])
#+END_SRC

#+RESULTS:
#+begin_example
var_109    16.4421
var_153    16.4421
Name: 14, dtype: float64
var_31     13.8222
var_104    13.8222
Name: 22, dtype: float64
var_12    13.972
var_81    13.972
Name: 26, dtype: float64
var_33    18.6714
var_92    18.6714
Name: 68, dtype: float64
var_23    3.1413
var_64    3.1413
Name: 85, dtype: float64
var_63    -3.6868
var_180   -3.6868
Name: 113, dtype: float64
var_98     2.5277
var_124    2.5277
Name: 114, dtype: float64
var_63     3.2496
var_105    3.2496
Name: 141, dtype: float64
var_46     9.3496
var_139    9.3496
Name: 193, dtype: float64
var_1    -1.1508
var_65   -1.1508
Name: 196, dtype: float64
#+end_example

Let's check for repeated values in each feature (within each dataframe column).  

#+BEGIN_SRC python :session :results values 
# find the n features with largest correlation with the target
indices = exploratory_dataframe.corr()[['target']].nlargest(10,'target').index

# check how many unique entries appear for these n features
exploratory_dataframe[indices].nunique()
#+END_SRC

#+RESULTS:
#+begin_example
target          2
var_6       38599
var_110    106121
var_53      33460
var_26     127089
var_22      90660
var_99      69300
var_190    114959
var_2       86555
var_133     19236
dtype: int64
#+end_example

Let's check for columns that have the most repeated features

#+BEGIN_SRC python :session :results values
# print column that has the most repeated values
print(exploratory_dataframe['var_68'])

# print n features with the most repeated values
exploratory_dataframe.nunique().nsmallest(20)
#+END_SRC

#+RESULTS:
#+begin_example
target         2
var_68       451
var_91      7962
var_108     8525
var_103     9376
var_12      9561
var_148    10608
var_161    11071
var_71     13527
var_25     14853
var_43     15188
var_125    16059
var_166    17902
var_169    18242
var_133    19236
var_15     19810
var_131    21464
var_23     24913
var_34     25164
var_93     26708
dtype: int64
#+end_example




We can note some things so far (~10% of binary targets are 1, rest 0, so we have an imbalanced classification problem, we can note approximate upper and lower bounds on the data, rough idea of the width of the distributions, all numeric data so no need to process categorical variables). 

*** Which naive features correlate with the target and with each other?

#+BEGIN_SRC python :session :results output 
# pick out some features to draw correlations, and prepend 'target'
num_random_features = 10 
some_features = [f'var_{i}' for i in range (num_random_features)]
some_features.insert(0,'target')

# correlations
feature_correlations = exploratory_dataframe[some_features].corr()


# calculate correlations of all features with the target, find n largest entries
print(exploratory_dataframe.corr()[['target']].nlargest(20,'target'))
#+END_SRC

#+RESULTS:
#+begin_example
target
target   1.000000
var_6    0.066731
var_110  0.064275
var_53   0.063399
var_26   0.062422
var_22   0.060558
var_99   0.058367
var_190  0.055973
var_2    0.055870
var_133  0.054548
var_0    0.052390
var_1    0.050343
var_179  0.050002
var_40   0.049530
var_184  0.048315
var_78   0.048245
var_170  0.047973
var_191  0.047114
var_94   0.046296
var_67   0.044673
#+end_example

#+BEGIN_SRC python :session :results file

# draw correlation heatmap
plt.figure(figsize=(10,10))
sns.heatmap(feature_correlations, annot=True)
plt.savefig('feature_correlations.png')
plt.close()
'feature_correlations.png'
#+END_SRC

#+RESULTS:
[[file:raw_correlations.png]]

We don't notice any strong linear correlations, so we probably need to do some feature engineering and/or use nonlinear models.

*** Plot some dataframe rows

Let's plot all of the features for a given target on the same set of axes (perhaps this represents a sequence of transactions in time or something like that).

#+BEGIN_SRC python :session :results file 
j=15
plt.plot(train_features[j])
print([i for i,x in enumerate(train_labels[:100]) if x==1]) 
plt.savefig('sample_row_plot.png')
plt.close()
'sample_row_plot.png'
#+END_SRC

#+RESULTS:
[[file:sample_features_plot.png]]

Let's plot a column of interest that has a lot of repeated values

#+BEGIN_SRC python :session :results file 
# print(np.array(kexploratory_dataframe['var_68'])
exploratory_dataframe.plot(y='var_68', style='o', markersize=1) 
plt.savefig('sample_feature_plot.png')
plt.close()
'sample_feature_plot.png'
#+END_SRC

#+RESULTS:
[[file:sample_feature_plot.png]]

*** Plot some histograms 

Plot a histogram of a column:

#+BEGIN_SRC python :session :results file
exploratory_dataframe['var_68'].hist()
plt.savefig('feature_68_hist.png')
plt.close()
'feature_68_hist.png'
#+END_SRC

#+RESULTS:
[[file:feature_68_hist.png]]
 
Let's look at what all rows with target value 1 look like

#+BEGIN_SRC python :session
exploratory_dataframe.loc[exploratory_dataframe['target']==1]
#+END_SRC

#+RESULTS:
#+begin_example
             ID_code  target    var_0   var_1    var_2   var_3    var_4    var_5   var_6    var_7   var_8   var_9  ...  var_188  var_189  var_190  var_191  var_192  var_193  var_194  var_195  var_196  var_197  var_198  var_199
13          train_13       1  16.3699  1.5934  16.7395  7.3330  12.1450   5.9004  4.8222  20.9729  1.1064  8.6978  ...  11.9586  -0.5899   7.4002   7.4031   4.3989   4.0978  17.3638  -1.3022   9.6846   9.0419  15.6064 -10.8529
29          train_29       1   5.3301 -2.6064  13.1913  3.1193   6.6483  -6.5659  5.9064  15.2341  1.2915  9.1168  ...  18.6375   0.1734   5.9215   7.9676   2.3405   1.1482  23.2168  -2.0105   3.7600   9.4513  17.4105 -14.6897
63          train_63       1   7.7072  0.0183   9.9974  8.3524   9.2886 -13.3627  6.0425  10.1108  1.3999  6.6710  ...  10.0679   1.9046   1.5832   5.0039   3.8814   7.4241  21.4844  -0.8297  -3.0468   7.5790  15.7685   5.4769
65          train_65       1  10.5358 -2.5439   8.7394  6.7548  14.4099  -3.8724  5.1584  15.8381  5.8204  9.0358  ...  10.2542   1.5517   4.6648   6.4227   3.4025  -4.0882  14.1174  -0.2472   5.3847   8.6949  15.1340   3.8449
71          train_71       1   6.7547  2.5973  14.2141  8.3514   7.4942  -1.3055  4.2336  15.0243 -1.8922  9.1282  ...  13.8773  -0.0899   1.4677   3.5935   2.0013   1.5777  18.2820  -4.3408   6.8869   9.3567  18.9013  13.3447
...              ...     ...      ...     ...      ...     ...      ...      ...     ...      ...     ...     ...  ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...
199966  train_199966       1  13.5797  2.5526   6.0512  5.2730  12.2182  -3.4048  7.3623  17.8372 -3.5604  8.8837  ...  20.7649  -0.4363   3.9023   7.9986   0.5213   2.3442  14.5510  -1.1530   8.9883   8.3389   9.5440   4.2493
199976  train_199976       1   7.9663 -2.8485   9.0919  7.3298   9.6690 -16.7872  4.5094  12.4351 -0.0113  8.5394  ...  20.1372   0.3380  10.7930   4.3876   3.7257   7.7038  14.7384   0.1561   1.5794   8.4627  14.3604  -1.6688
199981  train_199981       1  12.8140  0.6386  14.1657  7.1044   8.9365  -0.3274  6.5949  14.6078 -1.0373  8.8974  ...   7.0611   1.5463   4.8208   4.9010   2.2513   0.7308  14.7155   1.1464   5.5158   8.6519  16.0341   7.3809
199986  train_199986       1  12.0298 -8.7800   7.7071  7.4015   9.2305 -16.2174  5.9064  17.9268  3.6489  7.3970  ...   9.3059  -1.0691  16.7461   3.1249  -0.3943   8.4059  14.3367   3.0991   4.3853   8.8019  15.0031  -0.3659
199990  train_199990       1  14.1475  1.8568  11.0066  3.6779  12.1944 -16.5936  5.3217  14.8508  3.3377  6.1650  ...  16.0983   0.8156  -6.4708   4.7287   1.9034   7.2324  20.6047   1.7170  -4.0032   9.1627  13.8077  -1.9646

[20098 rows x 202 columns]
#+end_example

Plot a histogram across features for a given row that has target value 1.

#+BEGIN_SRC python :session :results file 
exploratory_dataframe.iloc[13,2:].hist()

# We need to save the figure to display inline in org mode. We also should use plt.close() so that we can respawn new different images without issues.
plt.savefig('row_hist1.png')
plt.close()
'row_hist1.png'
#+END_SRC

#+RESULTS:
[[file:hist1.png]]

* DONE Basic Neural Net model 
CLOSED: [2020-01-29 Wed 20:01]
** No class weights 

Let's follow https://www.tensorflow.org/tutorials/structured_data/imbalanced_data to implement a basic Neural Net in Tensorflow. We'll use a single layer for benchmarking and optimize later. Most of the code is copy-pasted from the tutorial. 

Define the model and metrics

#+BEGIN_SRC python :session :results output
  METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'),
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
  ]

  # Note the option to use bias initialization, see http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines
  # We modify the tutorial to allow for different numbers of hidden units
  def make_model(metrics = METRICS, output_bias=None, hidden_units = 16):
      if output_bias is not None:
          output_bias = tf.keras.initializers.Constant(output_bias)
      model = keras.Sequential([
          keras.layers.Dense(hidden_units, activation='relu',
                            input_shape=(train_features.shape[-1],)),
          keras.layers.Dropout(0.2),
          keras.layers.Dense(1, activation='sigmoid',
                            bias_initializer=output_bias)
        ])


      model.compile(
          optimizer=keras.optimizers.Adam(lr=1e-3),
          loss=keras.losses.BinaryCrossentropy(),
          metrics=metrics)
      return model
#+END_SRC

#+RESULTS:

Build the model

#+BEGIN_SRC python :session :results output
EPOCHS = 100
BATCH_SIZE = 2048

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc', 
    verbose=1,
    patience=50,
    mode='max',
    restore_best_weights=True)

model = make_model()
model.summary()

#+END_SRC


#+RESULTS:
#+begin_example
Model: "sequential_8"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_24 (Dense)             (None, 16)                3216      
_________________________________________________________________
dropout_16 (Dropout)         (None, 16)                0         
_________________________________________________________________
dense_25 (Dense)             (None, 1)                 17        
=================================================================
Total params: 3,233
Trainable params: 3,233
Non-trainable params: 0
_________________________________________________________________
#+end_example

Test run with a small amount of data

#+BEGIN_SRC python :session :results output
# Input numpy as a numpy array
model.predict(train_features[:10])
#+END_SRC

#+RESULTS:
#+begin_example
2020-01-26 21:39:16.948460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
array([[0.9997596 ],
       [0.5061394 ],
       [0.9303511 ],
       [0.7892672 ],
       [0.9999958 ],
       [0.9980216 ],
       [0.33681375],
       [0.35988793],
       [0.9976675 ],
       [0.9999008 ]], dtype=float32)
#+end_example

So far so good, let's follow the tutorial to set the initial bias as Log(pos/neg)

#+BEGIN_SRC python :session 
initial_bias = np.log(1/9)

model = make_model(output_bias = initial_bias)
model.predict(train_features[:10])

#+END_SRC

#+RESULTS:
|    0.94163775 |
|    0.92257184 |
|     0.8388229 |
|  0.0018517158 |
| 4.5338511e-05 |
|    0.27973586 |
|  0.0094071003 |
|   0.029227791 |
|    0.47860023 |
|  0.0087348791 |


Train the model

#+BEGIN_SRC python :session :results silent 
initial_bias = np.log(1/9)

model = make_model(hidden_units=16, output_bias=initial_bias)

# Features and labels input as numpy arrays
baseline_history = model.fit(
    train_features,
    train_labels,
    batch_size=2048,
    epochs=500,
    # callbacks=[early_stopping],
    validation_data=(validation_features, validation_labels))
#+END_SRC

** With class weights

#+BEGIN_SRC python :session :results output
# total/negative examples, total/positive examples, factor of 1/2 according to https://www.tensorflow.org/tutorials/structured_data/imbalanced_data
weight_for_0 = (10.0/9.0)*1/2.0 
weight_for_1 = 10.0/2.0

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))

#+END_SRC

#+RESULTS:
: Weight for class 0: 0.56
: Weight for class 1: 5.00

#+BEGIN_SRC python :session :results silent
initial_bias = np.log(1/9)

weighted_model = make_model(hidden_units = 16, output_bias = initial_bias)

# features and labels input as numpy arrays
weighted_history = weighted_model.fit(
    train_features,
    train_labels,
    batch_size=2048*4,
    epochs=500,
    # callbacks = [early_stopping],
    validation_data=(validation_features, validation_labels),
    # The class weights go here
    class_weight=class_weight) 
#+END_SRC

** Plot some metrics 

Define function to plot metrics

#+BEGIN_SRC python :session :results output 

import matplotlib as mpl
def plot_metrics(history):
    metrics =  ['loss', 'auc', 'precision', 'recall']
    mpl.rcParams['figure.figsize'] = (12, 10)
    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
    plt.figure(figsize=(6,4))
 
    for n, metric in enumerate(metrics):
        name = metric.replace("_"," ").capitalize()
        plt.subplot(2,2,n+1)
        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')
        plt.plot(history.epoch, history.history['val_'+metric], color=colors[0], linestyle="--", label='Val')
        plt.xlabel('Epoch')
        plt.ylabel(name)
        if metric == 'loss':
            plt.ylim([0, plt.ylim()[1]])
        elif metric == 'auc':
            plt.ylim([0.8,1])
        else:
            plt.ylim([0,1])
    
    plt.legend()
    plt.savefig('metrics.png')
    plt.close()
#+END_SRC

#+RESULTS:

Display metrics plot

#+BEGIN_SRC python :session :results file
plot_metrics(weighted_history)
'metrics.png':pr
#+END_SRC

#+RESULTS:
[[file:metrics.png]]

* DONE Basic Random Forest 
CLOSED: [2020-02-02 Sun 16:19]

Let's first run a basic Random Forest from sklearn. We'll use a blend of tutorials, with the FastAI lecture http://course18.fast.ai/lessonsml1/lesson2.html as backbone.

#+BEGIN_SRC python :session :results output
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.tree import export_graphviz
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session :results output 
# set up model parameters - for a start we can train a single small tree, with no probabilistic sample (no bootstrap), and tell it to use all of our cores.
rf = RandomForestClassifier(n_estimators=100, max_depth=15, max_features="sqrt", n_jobs=7)

# Train the model on training data
rf.fit(train_features, train_labels)

# makes predictions of probabilities on validation data 
predictions = rf.predict_proba(validation_features)

# calculate auc (note we only need second column of prediction probabilites - the probability of positive label)
auc = roc_auc_score(y_true=validation_labels, y_score=predictions[:,1])

# results:
# n_estimators=10, max_depth=10 , max_features="sqrt": 0.742, 20sec
# n_estimators=100, max_depth=10 , max_features="sqrt": 0.803, 40sec
# n_estimators=300, max_depth=10 , max_features="sqrt": 0.813, 2min 
# n_estimators=100, max_depth=15 , max_features="sqrt": 0.814, 1min 
# n_estimators=100, max_depth=25 , max_features="sqrt": 0.819, 2min 
# n_estimators=100, max_depth=15 , max_features="sqrt", class_weight={0:1,1:9}: 0.757, 2min 
# n_estimators=300, max_depth=15 , max_features="sqrt", class_weight={0:1,1:9}: 0.793, 3min 
# n_estimators=500, max_depth=25, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=7: 0.822, 6min
# n_estimators=1000, max_depth=42, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=7: 0.835, 15min
# n_estimators=2000, max_depth=42, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=7: 0.837, 23min
# n_estimators=1000, max_depth=42, max_features=40, class_weight={0:1,1:9}, n_jobs=7: 0.809, 35min
# n_estimators=2000, max_depth=25, max_features=40, class_weight={0:1,1:9}, n_jobs=7: 0.815, 55min
print(auc)
#+END_SRC

#+RESULTS:
: 0.8147160562178322

Let's draw the simple tree

#+BEGIN_SRC python :# export_graphviz(rf.estimators_[0], out_file=dotfile)
session :results output
#+END_SRC


** What if we scale our data? 
#+BEGIN_SRC python :session :results output
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# scale according to training data
train_features_scaled = scaler.fit_transform(train_features)

# apply the same transformation to validation data
val_features_scaled = scaler.transform(validation_features)

#+END_SRC

#+RESULTS:



# scaling

#+BEGIN_SRC python :session :results output
# set up model parameters
rf = RandomForestClassifier(n_estimators=100, max_depth=15, max_features="sqrt", class_weight={0:1,1:9}, n_jobs=-1)

# Train the model on training data
rf.fit(train_features_scaled, train_labels)

# makes predictions on validation and print auc 
predictions = rf.predict_proba(val_features_scaled)
auc = roc_auc_score(y_true=validation_labels, y_score=predictions[:,1])

# results: 

print(auc)
#+END_SRC

#+RESULTS:
: 0.7583061694212846


Could purposefully samply less of the negative examples.
 
* DONE GBT in LightGBM
CLOSED: [2020-02-04 Tue 18:16]

From LightGBM documentation: best to use for larger datasets to avoid overfitting (> 10,000 rows).

#+BEGIN_SRC python :session :results output
import lightgbm as lgb

# create dataset for lightgbm
lgb_train = lgb.Dataset(train_features, train_labels)
lgb_eval = lgb.Dataset(validation_features,
                       validation_labels,
                       reference=lgb_train)
#+END_SRC

#+RESULTS:

Build the model

#+BEGIN_SRC python :session :results output
random_state = 42

params = {
    # default num_trees=100
    'num_trees': 10000,
    'objective': 'binary',
    'metric': 'auc',
    'num_leaves': 4,
    'learning_rate': 0.02,
    'boost_from_average': 'false',
    # Percentage of features to be used for each tree
    'feature_fraction': 0.10,
    'min_data_in_leaf': 80,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.4,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 7,
    'bagging_seed' : random_state,
    'seed': random_state
}

print('Starting training...')

# train
gbm = lgb.train(params,
                train_stat_features,
                valid_sets=validation_stat_features,
                early_stopping_rounds=50)

print('Done Training.')
#+END_SRC

Interestingly, upweighting positive examples didn't seem to improve auc (if anything it hurt). More on this.

Check feature importance:

#+BEGIN_SRC python :session :results file
# feature importances
# print('Feature importances:', list(gbm.feature_importance()))

lgb.plot_importance(gbm, max_num_features=20)
plt.savefig('feature_importance.png')
plt.close 
'feature_importance.png'
#+END_SRC

#+RESULTS:
[[file:feature_importance.png]]

* DONE Feature Engineering
CLOSED: [2020-03-26 Thu 17:39]
** Bin and One-hot encoding 

Edit: the features created here are probably more useful in a NN model or input as categorical features. Standard decision trees don't seem to perform well with one-hot encoding and if one-hot features are concatenated with the original features, the original features dominate because of how decision trees work, not necessarily anything wrong with this particular dataset. 

We've noticed a lot of repeated entries within features, which perhaps indicates our anonymous features behave more like categorical variables than numerical variables. 

#+BEGIN_SRC python :session :results output 
train_df_copy = train_dataframe.copy()
validation_df_copy = validation_dataframe.copy()

# temp fix to help us manage memory
del test_dataframe
del raw_dataframe
#+END_SRC

#+RESULTS:

Don't necessarily need the next scaling function:

#+BEGIN_SRC python :session :results output 
# def min_max_scale_dataframe(dataframe, train_dataframe):

#     # scale each feature independently to the range [0,1] and apply the tranformation determined by 'train_dataframe' to 'dataframe'
#     min = train_dataframe.min()
#     max = train_dataframe.max()

#     return (dataframe - min)/(max - min)

# scale train and validation according to data in train
#+END_SRC

#+RESULTS:

Let's try binning and one-hot encoding our data:

#+BEGIN_SRC python :session :results output
class BinAndOneHot:
    """
    Tools to calculate bins for training data and then one-hot encode any any dataframe according to the bins determined by the training data.

    Attributes:
        num_bins (int): Number of bins to use for each feature (currently same number must be used for all features)
        bins (numpy ndarray): List of bins determined by training data
    """
    def __init__(self):
        self.num_bins = None
        self.bins = None

    def fit(self, train_dataframe, num_bins):
        """
        Calculates equal width bins for each feature of training data, to be used for consistently binning training, validation, and test data. First and last bins are extended to include +-infinity.

        Args:
            train_dataframe (pandas dataframe): Training data
            num_bins (int): Number of bins for each feature

        Attributes:
            self.num_bins (int): Number of bins is stored as this class Attribute
            self.bins (numpy ndarray): List of bins is assigned to this class Attribute

        Returns:
            None
        """
        # store 'num_bins' as a class attribute so that we use the same number of bins for other functions in this class
        self.num_bins = num_bins

        # populate bins_list with binned features by looping over 'train_dataframe columns'
        bins_list = []
        for column in train_dataframe.columns:

            # we don't need the binned dataframe, just the bins
            _, bins = pd.cut(train_dataframe[column], bins=self.num_bins, retbins=True)

            # extend first and last bins to include +-infinity
            bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))

            # store the result of binning this column in bins_list
            bins_list.append(bins)

        # Assign bins_list to class Attribute to be used in other functions in this class
        self.bins = bins_list

        print("Done calculating bins. List of bins stored as class attribute self.bins.")

        return None

    def transform(self, dataframe):
        """
        Bins each column of a dataframe into bins determined by the training data. Then creates new features one-hot encoding these bins.

        Args:
            dataframe (pandas dataframe): Dataframe to be transormed

        Returns:
            one_hot_bins_dataframe (pandas dataframe): Dataframe of features that one-hot encode our data according to training data bins
        """
        # bin dataframe according to self.bins (determined by training data)
        i = 0
        for column in dataframe.columns:
            dataframe[column] = pd.cut(dataframe[column], bins=self.bins[i])
            i += 1

        # one-hot encoding our binned data
        one_hot_bins_dataframe = pd.get_dummies(dataframe)

        return one_hot_bins_dataframe
#+END_SRC

#+RESULTS:

One-hot encode training and validation data:

#+BEGIN_SRC python :session :results output
bin_and_one_hot = BinAndOneHot()

# calculate binning
num_bins = 20
bin_and_one_hot.fit(train_dataframe=train_df_copy, num_bins=num_bins)

# bin and one-hot encode training and validation data
train_df_copy = bin_and_one_hot.transform(train_df_copy)
validation_df_copy = bin_and_one_hot.transform(validation_df_copy)
#+END_SRC

#+RESULTS:
: Done calculating bins. List of bins stored as class attribute self.bins.

LightGBM complains about commas in column names, so let's fix that

#+BEGIN_SRC python :session :output results
print(train_df_copy.head(), train_df_copy.describe(),
      validation_df_copy.head(), validation_df_copy.describe(), sep='\n')

# replaces all column name symbols that LightGBM doesn't like with "_'
# for dataframe in [one_hot_train, one_hot_validation]:
#     dataframe.columns = ["".join (c if c.isalnum() else "_" for c in str(x)) for x in dataframe.columns]

# print(one_hot_validation.head(), one_hot_validation.describe())
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session :results output
# add one-hot features to dataframe of original features
# train_df_copy = pd.concat([train_dataframe, train_df_copy], axis=1)
train_df_copy = np.array(train_df_copy, dtype=np.float32)

del train_dataframe
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session :results output
# validation_df_copy = pd.concat([validation_dataframe, validation_df_copy], axis=1)
validation_df_copy = np.array(validation_df_copy, dtype=np.float32)

del validation_dataframe
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session :results output
import lightgbm as lgb

# create dataset for lightgbm
train_df_copy = lgb.Dataset(train_df_copy, train_labels)
# validation_df_copy = lgb.Dataset(validation_df_copy,
#                        validation_labels,
#                        reference=train_df_copy)

train_df_copy.save_binary('train.bin')
# validation_df_copy.save_binary('validation.bin')
#+END_SRC

#+RESULTS:
: [LightGBM] [Info] Saving data to binary file train.bin



#+BEGIN_SRC python :session :results output
# create dataset for lightgbm
validation_df_copy = lgb.Dataset(validation_df_copy,
                       validation_labels,
                       reference=train_df_copy)

validation_df_copy.save_binary('validation.bin')

del train_df_copy
del validation_df_copy
#+END_SRC

#+RESULTS:
: [LightGBM] [Info] Saving data to binary file validation.bin



#+BEGIN_SRC python :session :results output
import lightgbm as lgb
import numpy as np

random_state = 42

# load binaries
train_data = lgb.Dataset('train.bin')
validation_data = lgb.Dataset('validation.bin')
#+END_SRC

#+RESULTS:

So far: our new features alone can get us auc 0.88, but combining them with old features shows feature importance dominated by old features.

update: original + one-hot features, 0.90, auc, 12 bins, 3000 trees, feature_fraction 0.10, memory management becoming a big problem

haven't been able to beat 0.895 auc with original + one-hot 16 bins
0.87 with 40 bins, only one-hot
0.883 with 20 bins, only one-hot

#+BEGIN_SRC python :session :results output
params = {
    # default num_trees=100
    'num_trees': 10000,
    'objective': 'binary',
    'metric': 'auc',
    'num_leaves': 4,
    'learning_rate': 0.02,
    'boost_from_average': 'false',
    # Percentage of features to be used for each tree
    'feature_fraction': 0.10,
    'min_data_in_leaf': 80,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.4,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 7,
    'bagging_seed' : random_state,
    'seed': random_state
}

print('Starting training...')

# train
gbm = lgb.train(params,
                train_data,
                valid_sets=validation_data,
                early_stopping_rounds=50)

print('Done Training.')
#+END_SRC

#+BEGIN_SRC python :session :results file
# feature importances
# print('Feature importances:', list(gbm.feature_importance()))
from matplotlib import pyplot as plt

lgb.plot_importance(gbm, max_num_features=20)
plt.savefig('feature_importance.png')
plt.close 
'feature_importance.png'
#+END_SRC

#+RESULTS:
[[file:feature_importance.png]]
** Statistical features (does nothing)

Let's create some new features such as Max, Min, Mean, std deviation, rolling average, skewness, kurtosis, number of unique values, for each row of the dataframe.

#+BEGIN_SRC python :session :results output 
train_df_copy = train_dataframe.copy()
validation_df_copy = validation_dataframe.copy()

# add new features
train_stat_features_df = pd.DataFrame()

train_stat_features_df['Mean'] = train_df_copy.mean(axis=1)
train_stat_features_df['Variance'] = train_df_copy.var(axis=1)
train_stat_features_df['Skew'] = train_df_copy.skew(axis=1)
train_stat_features_df['Kurtosis'] = train_df_copy.kurtosis(axis=1)
train_stat_features_df['Max'] = train_df_copy.max(axis=1)
train_stat_features_df['Min'] = train_df_copy.min(axis=1)

validation_stat_features_df = pd.DataFrame()

validation_stat_features_df['Mean'] = validation_df_copy.mean(axis=1)
validation_stat_features_df['Variance'] = validation_df_copy.var(axis=1)
validation_stat_features_df['Skew'] = validation_df_copy.skew(axis=1)
validation_stat_features_df['Kurtosis'] = validation_df_copy.kurtosis(axis=1)
validation_stat_features_df['Max'] = validation_df_copy.max(axis=1)
validation_stat_features_df['Min'] = validation_df_copy.min(axis=1)

train_stat_features_df = pd.concat([train_df_copy, train_stat_features_df],
                                   axis=1)
validation_stat_features_df = pd.concat([validation_df_copy, validation_stat_features_df],
                                   axis=1)
#+END_SRC

#+RESULTS:



#+BEGIN_SRC python :session :results output
import lightgbm as lgb

# create dataset for lightgbm
train_stat_features = lgb.Dataset(train_stat_features_df, train_labels)
validation_stat_features = lgb.Dataset(validation_stat_features_df,
                       validation_labels,
                       reference=train_stat_features)
#+END_SRC

#+RESULTS:

Best I can get is 0.901 auc, 6000 trees, feature fraction 0.05 or 0.10, num_leaves=4. New features seem to have no importance and slightly reduce auc no matter whether feature fraction 1.00 or 0.1 or 0.05. This probably makes sense because tree-based method already can learn the Mean/Max/Min etc.

I considered rounding all dataframe entries to 2nd decimal place because there seem to be many repeated values that agree to this many decimal places. But removing rounding increases auc.

After removing rounding, I seem to get slightly higher auc with stat features than without, fraction 0.10 (0.9001 -> 0.9007). I don't know if this is just the additional features reducing overfitting.

I'm going to copy these hyperparameters over to my earlier LightGBM baseline with no feature engineering.

#+BEGIN_SRC python :session :results output
random_state = 42

params = {
    # default num_trees=100
    'num_trees': 10000,
    'objective': 'binary',
    'metric': 'auc',
    'num_leaves': 4,
    'learning_rate': 0.02,
    'boost_from_average': 'false',
    # Percentage of features to be used for each tree
    'feature_fraction': 0.10,
    'min_data_in_leaf': 80,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.4,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 7,
    'bagging_seed' : random_state,
    'seed': random_state
}

print('Starting training...')

# train
gbm = lgb.train(params,
                train_stat_features,
                valid_sets=validation_stat_features,
                early_stopping_rounds=50)

print('Done Training.')
#+END_SRC

#+RESULTS:
#+begin_example
Starting training...
[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true.
This may cause significantly different results comparing to the previous versions of LightGBM.
Try to set boost_from_average=false, if your old models produce bad results
[LightGBM] [Info] Number of positive: 12872, number of negative: 115128
[LightGBM] [Info] Total Bins 52530
[LightGBM] [Info] Number of data: 128000, number of used features: 206
[1]	valid_0's auc: 0.558558
Training until validation scores don't improve for 50 rounds
[2]	valid_0's auc: 0.611673
[3]	valid_0's auc: 0.628139
[4]	valid_0's auc: 0.653913
[5]	valid_0's auc: 0.671005
[6]	valid_0's auc: 0.675455
[7]	valid_0's auc: 0.678505
[8]	valid_0's auc: 0.694971
[9]	valid_0's auc: 0.701592
[10]	valid_0's auc: 0.710719
[11]	valid_0's auc: 0.710029
[12]	valid_0's auc: 0.708434
[13]	valid_0's auc: 0.714958
[14]	valid_0's auc: 0.716928
[15]	valid_0's auc: 0.729484
[16]	valid_0's auc: 0.734052
[17]	valid_0's auc: 0.73832
[18]	valid_0's auc: 0.739882
[19]	valid_0's auc: 0.738891
[20]	valid_0's auc: 0.742232
[21]	valid_0's auc: 0.747181
[22]	valid_0's auc: 0.75195
[23]	valid_0's auc: 0.749747
[24]	valid_0's auc: 0.756632
[25]	valid_0's auc: 0.757242
[26]	valid_0's auc: 0.759266
[27]	valid_0's auc: 0.759003
[28]	valid_0's auc: 0.760036
[29]	valid_0's auc: 0.762287
[30]	valid_0's auc: 0.761765
[31]	valid_0's auc: 0.761842
[32]	valid_0's auc: 0.765756
[33]	valid_0's auc: 0.761711
[34]	valid_0's auc: 0.760912
[35]	valid_0's auc: 0.762829
[36]	valid_0's auc: 0.763096
[37]	valid_0's auc: 0.765632
[38]	valid_0's auc: 0.769692
[39]	valid_0's auc: 0.769142
[40]	valid_0's auc: 0.772041
[41]	valid_0's auc: 0.776201
[42]	valid_0's auc: 0.775897
[43]	valid_0's auc: 0.773906
[44]	valid_0's auc: 0.774595
[45]	valid_0's auc: 0.773126
[46]	valid_0's auc: 0.77373
[47]	valid_0's auc: 0.774714
[48]	valid_0's auc: 0.775296
[49]	valid_0's auc: 0.776408
[50]	valid_0's auc: 0.777156
[51]	valid_0's auc: 0.778861
[52]	valid_0's auc: 0.779429
[53]	valid_0's auc: 0.779697
[54]	valid_0's auc: 0.781851
[55]	valid_0's auc: 0.783238
[56]	valid_0's auc: 0.781068
[57]	valid_0's auc: 0.782545
[58]	valid_0's auc: 0.781934
[59]	valid_0's auc: 0.781818
[60]	valid_0's auc: 0.78257
[61]	valid_0's auc: 0.781832
[62]	valid_0's auc: 0.78342
[63]	valid_0's auc: 0.784864
[64]	valid_0's auc: 0.78581
[65]	valid_0's auc: 0.786022
[66]	valid_0's auc: 0.785109
[67]	valid_0's auc: 0.78611
[68]	valid_0's auc: 0.787389
[69]	valid_0's auc: 0.786589
[70]	valid_0's auc: 0.785327
[71]	valid_0's auc: 0.785527
[72]	valid_0's auc: 0.786927
[73]	valid_0's auc: 0.786403
[74]	valid_0's auc: 0.785841
[75]	valid_0's auc: 0.787732
[76]	valid_0's auc: 0.787591
[77]	valid_0's auc: 0.789193
[78]	valid_0's auc: 0.789664
[79]	valid_0's auc: 0.789289
[80]	valid_0's auc: 0.787919
[81]	valid_0's auc: 0.788609
[82]	valid_0's auc: 0.78973
[83]	valid_0's auc: 0.789737
[84]	valid_0's auc: 0.789075
[85]	valid_0's auc: 0.790582
[86]	valid_0's auc: 0.791192
[87]	valid_0's auc: 0.791826
[88]	valid_0's auc: 0.792642
[89]	valid_0's auc: 0.793406
[90]	valid_0's auc: 0.793052
[91]	valid_0's auc: 0.793287
[92]	valid_0's auc: 0.794383
[93]	valid_0's auc: 0.795422
[94]	valid_0's auc: 0.795346
[95]	valid_0's auc: 0.796801
[96]	valid_0's auc: 0.796291
[97]	valid_0's auc: 0.797759
[98]	valid_0's auc: 0.797003
[99]	valid_0's auc: 0.795289
[100]	valid_0's auc: 0.795562
[101]	valid_0's auc: 0.795267
[102]	valid_0's auc: 0.794796
[103]	valid_0's auc: 0.795331
[104]	valid_0's auc: 0.796319
[105]	valid_0's auc: 0.796112
[106]	valid_0's auc: 0.796246
[107]	valid_0's auc: 0.796695
[108]	valid_0's auc: 0.79812
[109]	valid_0's auc: 0.798766
[110]	valid_0's auc: 0.799893
[111]	valid_0's auc: 0.800285
[112]	valid_0's auc: 0.800806
[113]	valid_0's auc: 0.801181
[114]	valid_0's auc: 0.800889
[115]	valid_0's auc: 0.800631
[116]	valid_0's auc: 0.800578
[117]	valid_0's auc: 0.79994
[118]	valid_0's auc: 0.799307
[119]	valid_0's auc: 0.798864
[120]	valid_0's auc: 0.800018
[121]	valid_0's auc: 0.80086
[122]	valid_0's auc: 0.800808
[123]	valid_0's auc: 0.800097
[124]	valid_0's auc: 0.800477
[125]	valid_0's auc: 0.800942
[126]	valid_0's auc: 0.800939
[127]	valid_0's auc: 0.800217
[128]	valid_0's auc: 0.799709
[129]	valid_0's auc: 0.799864
[130]	valid_0's auc: 0.800049
[131]	valid_0's auc: 0.799836
[132]	valid_0's auc: 0.799103
[133]	valid_0's auc: 0.798873
[134]	valid_0's auc: 0.799193
[135]	valid_0's auc: 0.799558
[136]	valid_0's auc: 0.800153
[137]	valid_0's auc: 0.800177
[138]	valid_0's auc: 0.8012
[139]	valid_0's auc: 0.802269
[140]	valid_0's auc: 0.801676
[141]	valid_0's auc: 0.801009
[142]	valid_0's auc: 0.801327
[143]	valid_0's auc: 0.802273
[144]	valid_0's auc: 0.803166
[145]	valid_0's auc: 0.802956
[146]	valid_0's auc: 0.802167
[147]	valid_0's auc: 0.802189
[148]	valid_0's auc: 0.802361
[149]	valid_0's auc: 0.802075
[150]	valid_0's auc: 0.801656
[151]	valid_0's auc: 0.801562
[152]	valid_0's auc: 0.801444
[153]	valid_0's auc: 0.80203
[154]	valid_0's auc: 0.801932
[155]	valid_0's auc: 0.802216
[156]	valid_0's auc: 0.803036
[157]	valid_0's auc: 0.80308
[158]	valid_0's auc: 0.80264
[159]	valid_0's auc: 0.803323
[160]	valid_0's auc: 0.803796
[161]	valid_0's auc: 0.803786
[162]	valid_0's auc: 0.803881
[163]	valid_0's auc: 0.804075
[164]	valid_0's auc: 0.804198
[165]	valid_0's auc: 0.804619
[166]	valid_0's auc: 0.804884
[167]	valid_0's auc: 0.805585
[168]	valid_0's auc: 0.806002
[169]	valid_0's auc: 0.805884
[170]	valid_0's auc: 0.806605
[171]	valid_0's auc: 0.807127
[172]	valid_0's auc: 0.807972
[173]	valid_0's auc: 0.808441
[174]	valid_0's auc: 0.808109
[175]	valid_0's auc: 0.808282
[176]	valid_0's auc: 0.808955
[177]	valid_0's auc: 0.809248
[178]	valid_0's auc: 0.80941
[179]	valid_0's auc: 0.809355
[180]	valid_0's auc: 0.80932
[181]	valid_0's auc: 0.809176
[182]	valid_0's auc: 0.809222
[183]	valid_0's auc: 0.809512
[184]	valid_0's auc: 0.810207
[185]	valid_0's auc: 0.810794
[186]	valid_0's auc: 0.811335
[187]	valid_0's auc: 0.811632
[188]	valid_0's auc: 0.812038
[189]	valid_0's auc: 0.812541
[190]	valid_0's auc: 0.81291
[191]	valid_0's auc: 0.812972
[192]	valid_0's auc: 0.813247
[193]	valid_0's auc: 0.813654
[194]	valid_0's auc: 0.813581
[195]	valid_0's auc: 0.813862
[196]	valid_0's auc: 0.813802
[197]	valid_0's auc: 0.814222
[198]	valid_0's auc: 0.814324
[199]	valid_0's auc: 0.814743
[200]	valid_0's auc: 0.814813
[201]	valid_0's auc: 0.814778
[202]	valid_0's auc: 0.814381
[203]	valid_0's auc: 0.814635
[204]	valid_0's auc: 0.814568
[205]	valid_0's auc: 0.814902
[206]	valid_0's auc: 0.815062
[207]	valid_0's auc: 0.81512
[208]	valid_0's auc: 0.815271
[209]	valid_0's auc: 0.815045
[210]	valid_0's auc: 0.81465
[211]	valid_0's auc: 0.814918
[212]	valid_0's auc: 0.814751
[213]	valid_0's auc: 0.815041
[214]	valid_0's auc: 0.815113
[215]	valid_0's auc: 0.815619
[216]	valid_0's auc: 0.815688
[217]	valid_0's auc: 0.814936
[218]	valid_0's auc: 0.814879
[219]	valid_0's auc: 0.814854
[220]	valid_0's auc: 0.814687
[221]	valid_0's auc: 0.815237
[222]	valid_0's auc: 0.815271
[223]	valid_0's auc: 0.814585
[224]	valid_0's auc: 0.814568
[225]	valid_0's auc: 0.81476
[226]	valid_0's auc: 0.814603
[227]	valid_0's auc: 0.814796
[228]	valid_0's auc: 0.815112
[229]	valid_0's auc: 0.815329
[230]	valid_0's auc: 0.815473
[231]	valid_0's auc: 0.814975
[232]	valid_0's auc: 0.815049
[233]	valid_0's auc: 0.81514
[234]	valid_0's auc: 0.815158
[235]	valid_0's auc: 0.815357
[236]	valid_0's auc: 0.815633
[237]	valid_0's auc: 0.815976
[238]	valid_0's auc: 0.815908
[239]	valid_0's auc: 0.816023
[240]	valid_0's auc: 0.816381
[241]	valid_0's auc: 0.816199
[242]	valid_0's auc: 0.816591
[243]	valid_0's auc: 0.816828
[244]	valid_0's auc: 0.817232
[245]	valid_0's auc: 0.817349
[246]	valid_0's auc: 0.817684
[247]	valid_0's auc: 0.817755
[248]	valid_0's auc: 0.817653
[249]	valid_0's auc: 0.817519
[250]	valid_0's auc: 0.817682
[251]	valid_0's auc: 0.817918
[252]	valid_0's auc: 0.818086
[253]	valid_0's auc: 0.818068
[254]	valid_0's auc: 0.818516
[255]	valid_0's auc: 0.81892
[256]	valid_0's auc: 0.819108
[257]	valid_0's auc: 0.819248
[258]	valid_0's auc: 0.819295
[259]	valid_0's auc: 0.819617
[260]	valid_0's auc: 0.819944
[261]	valid_0's auc: 0.82021
[262]	valid_0's auc: 0.820634
[263]	valid_0's auc: 0.82073
[264]	valid_0's auc: 0.821005
[265]	valid_0's auc: 0.821228
[266]	valid_0's auc: 0.821494
[267]	valid_0's auc: 0.82177
[268]	valid_0's auc: 0.822115
[269]	valid_0's auc: 0.821954
[270]	valid_0's auc: 0.822158
[271]	valid_0's auc: 0.822041
[272]	valid_0's auc: 0.822317
[273]	valid_0's auc: 0.822468
[274]	valid_0's auc: 0.822859
[275]	valid_0's auc: 0.822886
[276]	valid_0's auc: 0.823127
[277]	valid_0's auc: 0.823108
[278]	valid_0's auc: 0.823294
[279]	valid_0's auc: 0.823627
[280]	valid_0's auc: 0.823578
[281]	valid_0's auc: 0.823737
[282]	valid_0's auc: 0.823486
[283]	valid_0's auc: 0.823694
[284]	valid_0's auc: 0.82389
[285]	valid_0's auc: 0.823641
[286]	valid_0's auc: 0.823479
[287]	valid_0's auc: 0.823469
[288]	valid_0's auc: 0.823555
[289]	valid_0's auc: 0.823581
[290]	valid_0's auc: 0.823576
[291]	valid_0's auc: 0.823526
[292]	valid_0's auc: 0.823481
[293]	valid_0's auc: 0.823442
[294]	valid_0's auc: 0.823335
[295]	valid_0's auc: 0.823606
[296]	valid_0's auc: 0.823838
[297]	valid_0's auc: 0.824004
[298]	valid_0's auc: 0.824257
[299]	valid_0's auc: 0.824101
[300]	valid_0's auc: 0.824354
[301]	valid_0's auc: 0.82445
[302]	valid_0's auc: 0.824515
[303]	valid_0's auc: 0.824635
[304]	valid_0's auc: 0.82479
[305]	valid_0's auc: 0.82471
[306]	valid_0's auc: 0.824624
[307]	valid_0's auc: 0.824331
[308]	valid_0's auc: 0.824407
[309]	valid_0's auc: 0.824522
[310]	valid_0's auc: 0.824627
[311]	valid_0's auc: 0.82494
[312]	valid_0's auc: 0.824922
[313]	valid_0's auc: 0.825154
[314]	valid_0's auc: 0.825305
[315]	valid_0's auc: 0.825296
[316]	valid_0's auc: 0.82544
[317]	valid_0's auc: 0.825741
[318]	valid_0's auc: 0.825932
[319]	valid_0's auc: 0.826122
[320]	valid_0's auc: 0.826235
[321]	valid_0's auc: 0.826361
[322]	valid_0's auc: 0.826737
[323]	valid_0's auc: 0.826888
[324]	valid_0's auc: 0.827032
[325]	valid_0's auc: 0.827178
[326]	valid_0's auc: 0.827239
[327]	valid_0's auc: 0.827203
[328]	valid_0's auc: 0.82715
[329]	valid_0's auc: 0.82687
[330]	valid_0's auc: 0.826972
[331]	valid_0's auc: 0.827059
[332]	valid_0's auc: 0.826999
[333]	valid_0's auc: 0.82714
[334]	valid_0's auc: 0.827222
[335]	valid_0's auc: 0.827331
[336]	valid_0's auc: 0.827479
[337]	valid_0's auc: 0.827483
[338]	valid_0's auc: 0.827587
[339]	valid_0's auc: 0.827537
[340]	valid_0's auc: 0.827538
[341]	valid_0's auc: 0.82757
[342]	valid_0's auc: 0.827613
[343]	valid_0's auc: 0.827871
[344]	valid_0's auc: 0.827743
[345]	valid_0's auc: 0.827874
[346]	valid_0's auc: 0.828118
[347]	valid_0's auc: 0.828136
[348]	valid_0's auc: 0.828446
[349]	valid_0's auc: 0.828433
[350]	valid_0's auc: 0.828415
[351]	valid_0's auc: 0.828729
[352]	valid_0's auc: 0.828686
[353]	valid_0's auc: 0.828483
[354]	valid_0's auc: 0.828475
[355]	valid_0's auc: 0.828581
[356]	valid_0's auc: 0.828832
[357]	valid_0's auc: 0.828902
[358]	valid_0's auc: 0.829043
[359]	valid_0's auc: 0.828898
[360]	valid_0's auc: 0.829015
[361]	valid_0's auc: 0.828962
[362]	valid_0's auc: 0.829174
[363]	valid_0's auc: 0.829114
[364]	valid_0's auc: 0.82921
[365]	valid_0's auc: 0.829367
[366]	valid_0's auc: 0.829589
[367]	valid_0's auc: 0.829611
[368]	valid_0's auc: 0.829411
[369]	valid_0's auc: 0.829649
[370]	valid_0's auc: 0.829745
[371]	valid_0's auc: 0.82999
[372]	valid_0's auc: 0.829989
[373]	valid_0's auc: 0.830032
[374]	valid_0's auc: 0.83025
[375]	valid_0's auc: 0.830298
[376]	valid_0's auc: 0.83035
[377]	valid_0's auc: 0.83041
[378]	valid_0's auc: 0.830451
[379]	valid_0's auc: 0.830719
[380]	valid_0's auc: 0.830642
[381]	valid_0's auc: 0.830739
[382]	valid_0's auc: 0.830894
[383]	valid_0's auc: 0.831021
[384]	valid_0's auc: 0.830938
[385]	valid_0's auc: 0.831158
[386]	valid_0's auc: 0.831083
[387]	valid_0's auc: 0.831246
[388]	valid_0's auc: 0.831469
[389]	valid_0's auc: 0.831625
[390]	valid_0's auc: 0.831598
[391]	valid_0's auc: 0.83176
[392]	valid_0's auc: 0.831778
[393]	valid_0's auc: 0.832012
[394]	valid_0's auc: 0.832093
[395]	valid_0's auc: 0.832269
[396]	valid_0's auc: 0.832244
[397]	valid_0's auc: 0.832274
[398]	valid_0's auc: 0.83234
[399]	valid_0's auc: 0.83242
[400]	valid_0's auc: 0.832515
[401]	valid_0's auc: 0.832808
[402]	valid_0's auc: 0.832663
[403]	valid_0's auc: 0.832744
[404]	valid_0's auc: 0.832832
[405]	valid_0's auc: 0.833018
[406]	valid_0's auc: 0.833089
[407]	valid_0's auc: 0.833202
[408]	valid_0's auc: 0.833355
[409]	valid_0's auc: 0.833471
[410]	valid_0's auc: 0.833744
[411]	valid_0's auc: 0.833944
[412]	valid_0's auc: 0.834097
[413]	valid_0's auc: 0.834147
[414]	valid_0's auc: 0.834039
[415]	valid_0's auc: 0.834104
[416]	valid_0's auc: 0.834165
[417]	valid_0's auc: 0.834309
[418]	valid_0's auc: 0.834344
[419]	valid_0's auc: 0.834555
[420]	valid_0's auc: 0.834687
[421]	valid_0's auc: 0.834798
[422]	valid_0's auc: 0.834918
[423]	valid_0's auc: 0.834936
[424]	valid_0's auc: 0.83515
[425]	valid_0's auc: 0.835209
[426]	valid_0's auc: 0.83528
[427]	valid_0's auc: 0.8356
[428]	valid_0's auc: 0.835636
[429]	valid_0's auc: 0.835661
[430]	valid_0's auc: 0.835809
[431]	valid_0's auc: 0.836042
[432]	valid_0's auc: 0.836046
[433]	valid_0's auc: 0.836219
[434]	valid_0's auc: 0.836198
[435]	valid_0's auc: 0.836282
[436]	valid_0's auc: 0.836207
[437]	valid_0's auc: 0.836207
[438]	valid_0's auc: 0.83632
[439]	valid_0's auc: 0.836391
[440]	valid_0's auc: 0.836411
[441]	valid_0's auc: 0.836511
[442]	valid_0's auc: 0.836808
[443]	valid_0's auc: 0.836853
[444]	valid_0's auc: 0.836972
[445]	valid_0's auc: 0.836969
[446]	valid_0's auc: 0.83679
[447]	valid_0's auc: 0.836849
[448]	valid_0's auc: 0.836988
[449]	valid_0's auc: 0.836993
[450]	valid_0's auc: 0.83698
[451]	valid_0's auc: 0.837059
[452]	valid_0's auc: 0.837108
[453]	valid_0's auc: 0.837245
[454]	valid_0's auc: 0.837362
[455]	valid_0's auc: 0.837379
[456]	valid_0's auc: 0.837612
[457]	valid_0's auc: 0.837872
[458]	valid_0's auc: 0.837998
[459]	valid_0's auc: 0.838037
[460]	valid_0's auc: 0.838071
[461]	valid_0's auc: 0.838256
[462]	valid_0's auc: 0.838286
[463]	valid_0's auc: 0.838335
[464]	valid_0's auc: 0.838402
[465]	valid_0's auc: 0.838528
[466]	valid_0's auc: 0.838684
[467]	valid_0's auc: 0.838836
[468]	valid_0's auc: 0.839046
[469]	valid_0's auc: 0.839131
[470]	valid_0's auc: 0.839286
[471]	valid_0's auc: 0.839199
[472]	valid_0's auc: 0.839391
[473]	valid_0's auc: 0.839541
[474]	valid_0's auc: 0.839709
[475]	valid_0's auc: 0.839922
[476]	valid_0's auc: 0.839908
[477]	valid_0's auc: 0.84016
[478]	valid_0's auc: 0.840207
[479]	valid_0's auc: 0.840296
[480]	valid_0's auc: 0.840489
[481]	valid_0's auc: 0.840388
[482]	valid_0's auc: 0.8406
[483]	valid_0's auc: 0.840646
[484]	valid_0's auc: 0.840706
[485]	valid_0's auc: 0.840843
[486]	valid_0's auc: 0.840914
[487]	valid_0's auc: 0.840961
[488]	valid_0's auc: 0.840977
[489]	valid_0's auc: 0.84102
[490]	valid_0's auc: 0.841122
[491]	valid_0's auc: 0.841216
[492]	valid_0's auc: 0.841292
[493]	valid_0's auc: 0.841477
[494]	valid_0's auc: 0.841664
[495]	valid_0's auc: 0.841878
[496]	valid_0's auc: 0.842028
[497]	valid_0's auc: 0.842049
[498]	valid_0's auc: 0.842141
[499]	valid_0's auc: 0.842256
[500]	valid_0's auc: 0.842312
[501]	valid_0's auc: 0.842396
[502]	valid_0's auc: 0.842389
[503]	valid_0's auc: 0.842524
[504]	valid_0's auc: 0.842679
[505]	valid_0's auc: 0.842606
[506]	valid_0's auc: 0.842704
[507]	valid_0's auc: 0.84289
[508]	valid_0's auc: 0.843011
[509]	valid_0's auc: 0.843008
[510]	valid_0's auc: 0.843171
[511]	valid_0's auc: 0.843332
[512]	valid_0's auc: 0.843475
[513]	valid_0's auc: 0.843517
[514]	valid_0's auc: 0.843672
[515]	valid_0's auc: 0.843792
[516]	valid_0's auc: 0.843855
[517]	valid_0's auc: 0.843993
[518]	valid_0's auc: 0.844088
[519]	valid_0's auc: 0.84413
[520]	valid_0's auc: 0.844218
[521]	valid_0's auc: 0.844374
[522]	valid_0's auc: 0.844514
[523]	valid_0's auc: 0.844613
[524]	valid_0's auc: 0.844526
[525]	valid_0's auc: 0.84459
[526]	valid_0's auc: 0.844684
[527]	valid_0's auc: 0.844798
[528]	valid_0's auc: 0.844773
[529]	valid_0's auc: 0.844833
[530]	valid_0's auc: 0.844862
[531]	valid_0's auc: 0.844941
[532]	valid_0's auc: 0.844941
[533]	valid_0's auc: 0.844874
[534]	valid_0's auc: 0.845027
[535]	valid_0's auc: 0.845207
[536]	valid_0's auc: 0.845247
[537]	valid_0's auc: 0.84519
[538]	valid_0's auc: 0.845281
[539]	valid_0's auc: 0.845343
[540]	valid_0's auc: 0.845375
[541]	valid_0's auc: 0.845402
[542]	valid_0's auc: 0.845571
[543]	valid_0's auc: 0.845533
[544]	valid_0's auc: 0.845565
[545]	valid_0's auc: 0.845611
[546]	valid_0's auc: 0.845723
[547]	valid_0's auc: 0.845745
[548]	valid_0's auc: 0.845847
[549]	valid_0's auc: 0.84589
[550]	valid_0's auc: 0.845953
[551]	valid_0's auc: 0.846194
[552]	valid_0's auc: 0.846258
[553]	valid_0's auc: 0.846409
[554]	valid_0's auc: 0.846491
[555]	valid_0's auc: 0.84645
[556]	valid_0's auc: 0.846499
[557]	valid_0's auc: 0.846633
[558]	valid_0's auc: 0.84672
[559]	valid_0's auc: 0.846674
[560]	valid_0's auc: 0.846725
[561]	valid_0's auc: 0.846838
[562]	valid_0's auc: 0.846747
[563]	valid_0's auc: 0.846737
[564]	valid_0's auc: 0.846773
[565]	valid_0's auc: 0.846896
[566]	valid_0's auc: 0.846997
[567]	valid_0's auc: 0.846873
[568]	valid_0's auc: 0.84696
[569]	valid_0's auc: 0.846926
[570]	valid_0's auc: 0.846953
[571]	valid_0's auc: 0.846996
[572]	valid_0's auc: 0.847013
[573]	valid_0's auc: 0.847044
[574]	valid_0's auc: 0.847102
[575]	valid_0's auc: 0.847111
[576]	valid_0's auc: 0.847164
[577]	valid_0's auc: 0.84718
[578]	valid_0's auc: 0.847236
[579]	valid_0's auc: 0.847278
[580]	valid_0's auc: 0.847345
[581]	valid_0's auc: 0.84738
[582]	valid_0's auc: 0.847477
[583]	valid_0's auc: 0.847575
[584]	valid_0's auc: 0.847606
[585]	valid_0's auc: 0.847635
[586]	valid_0's auc: 0.847709
[587]	valid_0's auc: 0.847712
[588]	valid_0's auc: 0.847667
[589]	valid_0's auc: 0.847705
[590]	valid_0's auc: 0.847826
[591]	valid_0's auc: 0.84784
[592]	valid_0's auc: 0.847866
[593]	valid_0's auc: 0.84793
[594]	valid_0's auc: 0.84791
[595]	valid_0's auc: 0.847976
[596]	valid_0's auc: 0.848024
[597]	valid_0's auc: 0.848098
[598]	valid_0's auc: 0.848202
[599]	valid_0's auc: 0.848163
[600]	valid_0's auc: 0.8483
[601]	valid_0's auc: 0.848405
[602]	valid_0's auc: 0.848427
[603]	valid_0's auc: 0.848435
[604]	valid_0's auc: 0.8486
[605]	valid_0's auc: 0.848655
[606]	valid_0's auc: 0.848734
[607]	valid_0's auc: 0.848771
[608]	valid_0's auc: 0.848828
[609]	valid_0's auc: 0.848908
[610]	valid_0's auc: 0.848904
[611]	valid_0's auc: 0.849034
[612]	valid_0's auc: 0.849183
[613]	valid_0's auc: 0.849238
[614]	valid_0's auc: 0.84917
[615]	valid_0's auc: 0.849249
[616]	valid_0's auc: 0.849326
[617]	valid_0's auc: 0.849331
[618]	valid_0's auc: 0.849278
[619]	valid_0's auc: 0.849307
[620]	valid_0's auc: 0.849315
[621]	valid_0's auc: 0.849359
[622]	valid_0's auc: 0.849469
[623]	valid_0's auc: 0.849507
[624]	valid_0's auc: 0.849585
[625]	valid_0's auc: 0.849632
[626]	valid_0's auc: 0.849733
[627]	valid_0's auc: 0.849786
[628]	valid_0's auc: 0.849756
[629]	valid_0's auc: 0.849866
[630]	valid_0's auc: 0.849814
[631]	valid_0's auc: 0.849917
[632]	valid_0's auc: 0.850017
[633]	valid_0's auc: 0.850047
[634]	valid_0's auc: 0.850175
[635]	valid_0's auc: 0.850289
[636]	valid_0's auc: 0.850376
[637]	valid_0's auc: 0.850384
[638]	valid_0's auc: 0.850484
[639]	valid_0's auc: 0.850583
[640]	valid_0's auc: 0.850676
[641]	valid_0's auc: 0.850844
[642]	valid_0's auc: 0.850965
[643]	valid_0's auc: 0.850996
[644]	valid_0's auc: 0.851077
[645]	valid_0's auc: 0.85108
[646]	valid_0's auc: 0.851188
[647]	valid_0's auc: 0.851329
[648]	valid_0's auc: 0.851424
[649]	valid_0's auc: 0.851476
[650]	valid_0's auc: 0.851464
[651]	valid_0's auc: 0.851537
[652]	valid_0's auc: 0.851639
[653]	valid_0's auc: 0.851745
[654]	valid_0's auc: 0.851862
[655]	valid_0's auc: 0.851939
[656]	valid_0's auc: 0.852038
[657]	valid_0's auc: 0.85205
[658]	valid_0's auc: 0.852167
[659]	valid_0's auc: 0.852172
[660]	valid_0's auc: 0.852266
[661]	valid_0's auc: 0.852287
[662]	valid_0's auc: 0.852336
[663]	valid_0's auc: 0.85245
[664]	valid_0's auc: 0.852574
[665]	valid_0's auc: 0.852677
[666]	valid_0's auc: 0.852724
[667]	valid_0's auc: 0.852742
[668]	valid_0's auc: 0.852729
[669]	valid_0's auc: 0.852764
[670]	valid_0's auc: 0.852825
[671]	valid_0's auc: 0.852902
[672]	valid_0's auc: 0.852853
[673]	valid_0's auc: 0.852912
[674]	valid_0's auc: 0.853003
[675]	valid_0's auc: 0.853079
[676]	valid_0's auc: 0.853146
[677]	valid_0's auc: 0.853189
[678]	valid_0's auc: 0.853188
[679]	valid_0's auc: 0.853223
[680]	valid_0's auc: 0.853243
[681]	valid_0's auc: 0.853279
[682]	valid_0's auc: 0.85332
[683]	valid_0's auc: 0.853426
[684]	valid_0's auc: 0.853563
[685]	valid_0's auc: 0.853589
[686]	valid_0's auc: 0.853686
[687]	valid_0's auc: 0.853675
[688]	valid_0's auc: 0.853783
[689]	valid_0's auc: 0.853736
[690]	valid_0's auc: 0.853803
[691]	valid_0's auc: 0.853891
[692]	valid_0's auc: 0.853957
[693]	valid_0's auc: 0.854077
[694]	valid_0's auc: 0.854071
[695]	valid_0's auc: 0.854116
[696]	valid_0's auc: 0.85415
[697]	valid_0's auc: 0.854224
[698]	valid_0's auc: 0.854266
[699]	valid_0's auc: 0.854334
[700]	valid_0's auc: 0.854323
[701]	valid_0's auc: 0.854343
[702]	valid_0's auc: 0.854392
[703]	valid_0's auc: 0.854445
[704]	valid_0's auc: 0.854529
[705]	valid_0's auc: 0.854545
[706]	valid_0's auc: 0.85457
[707]	valid_0's auc: 0.854501
[708]	valid_0's auc: 0.854568
[709]	valid_0's auc: 0.854652
[710]	valid_0's auc: 0.854742
[711]	valid_0's auc: 0.854827
[712]	valid_0's auc: 0.854869
[713]	valid_0's auc: 0.854946
[714]	valid_0's auc: 0.854966
[715]	valid_0's auc: 0.855043
[716]	valid_0's auc: 0.855072
[717]	valid_0's auc: 0.855163
[718]	valid_0's auc: 0.85524
[719]	valid_0's auc: 0.855251
[720]	valid_0's auc: 0.855298
[721]	valid_0's auc: 0.855363
[722]	valid_0's auc: 0.855455
[723]	valid_0's auc: 0.855418
[724]	valid_0's auc: 0.855419
[725]	valid_0's auc: 0.855518
[726]	valid_0's auc: 0.855606
[727]	valid_0's auc: 0.855703
[728]	valid_0's auc: 0.855727
[729]	valid_0's auc: 0.855832
[730]	valid_0's auc: 0.855866
[731]	valid_0's auc: 0.855894
[732]	valid_0's auc: 0.856036
[733]	valid_0's auc: 0.85606
[734]	valid_0's auc: 0.856006
[735]	valid_0's auc: 0.856114
[736]	valid_0's auc: 0.856149
[737]	valid_0's auc: 0.856178
[738]	valid_0's auc: 0.856165
[739]	valid_0's auc: 0.856184
[740]	valid_0's auc: 0.856256
[741]	valid_0's auc: 0.856344
[742]	valid_0's auc: 0.85636
[743]	valid_0's auc: 0.856423
[744]	valid_0's auc: 0.856466
[745]	valid_0's auc: 0.856488
[746]	valid_0's auc: 0.856644
[747]	valid_0's auc: 0.856695
[748]	valid_0's auc: 0.856783
[749]	valid_0's auc: 0.856851
[750]	valid_0's auc: 0.856935
[751]	valid_0's auc: 0.857011
[752]	valid_0's auc: 0.857028
[753]	valid_0's auc: 0.857039
[754]	valid_0's auc: 0.857117
[755]	valid_0's auc: 0.857209
[756]	valid_0's auc: 0.857301
[757]	valid_0's auc: 0.857286
[758]	valid_0's auc: 0.857317
[759]	valid_0's auc: 0.857366
[760]	valid_0's auc: 0.857463
[761]	valid_0's auc: 0.857379
[762]	valid_0's auc: 0.857438
[763]	valid_0's auc: 0.857389
[764]	valid_0's auc: 0.857347
[765]	valid_0's auc: 0.857444
[766]	valid_0's auc: 0.857541
[767]	valid_0's auc: 0.857615
[768]	valid_0's auc: 0.857679
[769]	valid_0's auc: 0.857731
[770]	valid_0's auc: 0.857815
[771]	valid_0's auc: 0.857939
[772]	valid_0's auc: 0.857992
[773]	valid_0's auc: 0.858025
[774]	valid_0's auc: 0.858134
[775]	valid_0's auc: 0.858178
[776]	valid_0's auc: 0.858254
[777]	valid_0's auc: 0.858328
[778]	valid_0's auc: 0.858389
[779]	valid_0's auc: 0.858424
[780]	valid_0's auc: 0.858419
[781]	valid_0's auc: 0.858452
[782]	valid_0's auc: 0.858517
[783]	valid_0's auc: 0.858569
[784]	valid_0's auc: 0.858598
[785]	valid_0's auc: 0.858618
[786]	valid_0's auc: 0.858607
[787]	valid_0's auc: 0.858616
[788]	valid_0's auc: 0.858588
[789]	valid_0's auc: 0.858625
[790]	valid_0's auc: 0.858625
[791]	valid_0's auc: 0.858608
[792]	valid_0's auc: 0.858657
[793]	valid_0's auc: 0.858692
[794]	valid_0's auc: 0.858778
[795]	valid_0's auc: 0.858767
[796]	valid_0's auc: 0.858888
[797]	valid_0's auc: 0.85894
[798]	valid_0's auc: 0.858975
[799]	valid_0's auc: 0.859047
[800]	valid_0's auc: 0.859064
[801]	valid_0's auc: 0.859166
[802]	valid_0's auc: 0.859225
[803]	valid_0's auc: 0.859274
[804]	valid_0's auc: 0.859316
[805]	valid_0's auc: 0.859365
[806]	valid_0's auc: 0.859451
[807]	valid_0's auc: 0.859465
[808]	valid_0's auc: 0.85952
[809]	valid_0's auc: 0.859602
[810]	valid_0's auc: 0.85971
[811]	valid_0's auc: 0.859672
[812]	valid_0's auc: 0.859755
[813]	valid_0's auc: 0.85985
[814]	valid_0's auc: 0.859946
[815]	valid_0's auc: 0.859967
[816]	valid_0's auc: 0.860117
[817]	valid_0's auc: 0.860165
[818]	valid_0's auc: 0.860332
[819]	valid_0's auc: 0.860377
[820]	valid_0's auc: 0.860391
[821]	valid_0's auc: 0.8604
[822]	valid_0's auc: 0.860475
[823]	valid_0's auc: 0.860579
[824]	valid_0's auc: 0.86056
[825]	valid_0's auc: 0.860631
[826]	valid_0's auc: 0.860691
[827]	valid_0's auc: 0.860824
[828]	valid_0's auc: 0.86089
[829]	valid_0's auc: 0.860968
[830]	valid_0's auc: 0.861002
[831]	valid_0's auc: 0.861064
[832]	valid_0's auc: 0.86113
[833]	valid_0's auc: 0.861179
[834]	valid_0's auc: 0.8612
[835]	valid_0's auc: 0.861272
[836]	valid_0's auc: 0.861321
[837]	valid_0's auc: 0.861355
[838]	valid_0's auc: 0.861412
[839]	valid_0's auc: 0.861448
[840]	valid_0's auc: 0.861499
[841]	valid_0's auc: 0.861541
[842]	valid_0's auc: 0.861541
[843]	valid_0's auc: 0.861579
[844]	valid_0's auc: 0.861622
[845]	valid_0's auc: 0.861669
[846]	valid_0's auc: 0.86175
[847]	valid_0's auc: 0.861801
[848]	valid_0's auc: 0.861885
[849]	valid_0's auc: 0.861935
[850]	valid_0's auc: 0.861956
[851]	valid_0's auc: 0.862035
[852]	valid_0's auc: 0.862068
[853]	valid_0's auc: 0.862173
[854]	valid_0's auc: 0.86221
[855]	valid_0's auc: 0.862288
[856]	valid_0's auc: 0.862304
[857]	valid_0's auc: 0.862362
[858]	valid_0's auc: 0.8624
[859]	valid_0's auc: 0.862459
[860]	valid_0's auc: 0.862465
[861]	valid_0's auc: 0.862544
[862]	valid_0's auc: 0.862501
[863]	valid_0's auc: 0.862543
[864]	valid_0's auc: 0.862583
[865]	valid_0's auc: 0.862653
[866]	valid_0's auc: 0.862648
[867]	valid_0's auc: 0.862689
[868]	valid_0's auc: 0.86271
[869]	valid_0's auc: 0.862762
[870]	valid_0's auc: 0.862757
[871]	valid_0's auc: 0.862876
[872]	valid_0's auc: 0.862979
[873]	valid_0's auc: 0.863038
[874]	valid_0's auc: 0.863077
[875]	valid_0's auc: 0.863093
[876]	valid_0's auc: 0.863163
[877]	valid_0's auc: 0.863254
[878]	valid_0's auc: 0.863291
[879]	valid_0's auc: 0.863326
[880]	valid_0's auc: 0.863369
[881]	valid_0's auc: 0.863429
[882]	valid_0's auc: 0.863487
[883]	valid_0's auc: 0.863543
[884]	valid_0's auc: 0.863585
[885]	valid_0's auc: 0.863608
[886]	valid_0's auc: 0.863646
[887]	valid_0's auc: 0.863693
[888]	valid_0's auc: 0.863749
[889]	valid_0's auc: 0.863768
[890]	valid_0's auc: 0.863816
[891]	valid_0's auc: 0.863894
[892]	valid_0's auc: 0.863922
[893]	valid_0's auc: 0.863997
[894]	valid_0's auc: 0.864065
[895]	valid_0's auc: 0.864029
[896]	valid_0's auc: 0.864067
[897]	valid_0's auc: 0.864144
[898]	valid_0's auc: 0.864149
[899]	valid_0's auc: 0.864147
[900]	valid_0's auc: 0.864175
[901]	valid_0's auc: 0.864203
[902]	valid_0's auc: 0.864256
[903]	valid_0's auc: 0.864285
[904]	valid_0's auc: 0.864327
[905]	valid_0's auc: 0.864371
[906]	valid_0's auc: 0.864426
[907]	valid_0's auc: 0.864486
[908]	valid_0's auc: 0.864515
[909]	valid_0's auc: 0.864459
[910]	valid_0's auc: 0.864482
[911]	valid_0's auc: 0.864518
[912]	valid_0's auc: 0.864539
[913]	valid_0's auc: 0.864579
[914]	valid_0's auc: 0.864638
[915]	valid_0's auc: 0.864655
[916]	valid_0's auc: 0.8647
[917]	valid_0's auc: 0.864738
[918]	valid_0's auc: 0.864753
[919]	valid_0's auc: 0.864819
[920]	valid_0's auc: 0.864842
[921]	valid_0's auc: 0.864885
[922]	valid_0's auc: 0.864906
[923]	valid_0's auc: 0.864959
[924]	valid_0's auc: 0.864966
[925]	valid_0's auc: 0.865021
[926]	valid_0's auc: 0.865048
[927]	valid_0's auc: 0.865079
[928]	valid_0's auc: 0.865098
[929]	valid_0's auc: 0.865138
[930]	valid_0's auc: 0.865065
[931]	valid_0's auc: 0.865155
[932]	valid_0's auc: 0.865224
[933]	valid_0's auc: 0.865259
[934]	valid_0's auc: 0.865241
[935]	valid_0's auc: 0.865321
[936]	valid_0's auc: 0.865387
[937]	valid_0's auc: 0.865392
[938]	valid_0's auc: 0.865426
[939]	valid_0's auc: 0.865489
[940]	valid_0's auc: 0.865528
[941]	valid_0's auc: 0.865534
[942]	valid_0's auc: 0.865579
[943]	valid_0's auc: 0.865626
[944]	valid_0's auc: 0.865694
[945]	valid_0's auc: 0.865685
[946]	valid_0's auc: 0.865784
[947]	valid_0's auc: 0.865801
[948]	valid_0's auc: 0.865902
[949]	valid_0's auc: 0.865858
[950]	valid_0's auc: 0.865838
[951]	valid_0's auc: 0.865883
[952]	valid_0's auc: 0.865877
[953]	valid_0's auc: 0.865956
[954]	valid_0's auc: 0.865999
[955]	valid_0's auc: 0.866044
[956]	valid_0's auc: 0.866072
[957]	valid_0's auc: 0.866136
[958]	valid_0's auc: 0.866172
[959]	valid_0's auc: 0.866245
[960]	valid_0's auc: 0.866303
[961]	valid_0's auc: 0.866383
[962]	valid_0's auc: 0.866426
[963]	valid_0's auc: 0.866485
[964]	valid_0's auc: 0.866532
[965]	valid_0's auc: 0.866507
[966]	valid_0's auc: 0.866591
[967]	valid_0's auc: 0.866604
[968]	valid_0's auc: 0.866676
[969]	valid_0's auc: 0.866682
[970]	valid_0's auc: 0.866743
[971]	valid_0's auc: 0.866764
[972]	valid_0's auc: 0.866761
[973]	valid_0's auc: 0.866768
[974]	valid_0's auc: 0.866824
[975]	valid_0's auc: 0.866861
[976]	valid_0's auc: 0.866905
[977]	valid_0's auc: 0.86693
[978]	valid_0's auc: 0.866946
[979]	valid_0's auc: 0.866977
[980]	valid_0's auc: 0.866965
[981]	valid_0's auc: 0.867008
[982]	valid_0's auc: 0.867044
[983]	valid_0's auc: 0.86706
[984]	valid_0's auc: 0.867149
[985]	valid_0's auc: 0.86718
[986]	valid_0's auc: 0.867187
[987]	valid_0's auc: 0.867234
[988]	valid_0's auc: 0.867277
[989]	valid_0's auc: 0.867266
[990]	valid_0's auc: 0.867272
[991]	valid_0's auc: 0.867325
[992]	valid_0's auc: 0.86742
[993]	valid_0's auc: 0.867457
[994]	valid_0's auc: 0.867526
[995]	valid_0's auc: 0.867553
[996]	valid_0's auc: 0.867565
[997]	valid_0's auc: 0.867557
[998]	valid_0's auc: 0.867605
[999]	valid_0's auc: 0.867644
[1000]	valid_0's auc: 0.867672
[1001]	valid_0's auc: 0.867695
[1002]	valid_0's auc: 0.867686
[1003]	valid_0's auc: 0.867733
[1004]	valid_0's auc: 0.867754
[1005]	valid_0's auc: 0.86776
[1006]	valid_0's auc: 0.867809
[1007]	valid_0's auc: 0.867852
[1008]	valid_0's auc: 0.867928
[1009]	valid_0's auc: 0.867999
[1010]	valid_0's auc: 0.86806
[1011]	valid_0's auc: 0.868094
[1012]	valid_0's auc: 0.868114
[1013]	valid_0's auc: 0.868119
[1014]	valid_0's auc: 0.868102
[1015]	valid_0's auc: 0.868117
[1016]	valid_0's auc: 0.868175
[1017]	valid_0's auc: 0.868197
[1018]	valid_0's auc: 0.868291
[1019]	valid_0's auc: 0.868292
[1020]	valid_0's auc: 0.868292
[1021]	valid_0's auc: 0.868354
[1022]	valid_0's auc: 0.868369
[1023]	valid_0's auc: 0.868405
[1024]	valid_0's auc: 0.868463
[1025]	valid_0's auc: 0.868495
[1026]	valid_0's auc: 0.86854
[1027]	valid_0's auc: 0.868592
[1028]	valid_0's auc: 0.868676
[1029]	valid_0's auc: 0.868738
[1030]	valid_0's auc: 0.868741
[1031]	valid_0's auc: 0.868771
[1032]	valid_0's auc: 0.868831
[1033]	valid_0's auc: 0.868899
[1034]	valid_0's auc: 0.868932
[1035]	valid_0's auc: 0.868991
[1036]	valid_0's auc: 0.869034
[1037]	valid_0's auc: 0.86909
[1038]	valid_0's auc: 0.86913
[1039]	valid_0's auc: 0.869211
[1040]	valid_0's auc: 0.869254
[1041]	valid_0's auc: 0.869336
[1042]	valid_0's auc: 0.869338
[1043]	valid_0's auc: 0.869384
[1044]	valid_0's auc: 0.869428
[1045]	valid_0's auc: 0.869485
[1046]	valid_0's auc: 0.869501
[1047]	valid_0's auc: 0.869535
[1048]	valid_0's auc: 0.86958
[1049]	valid_0's auc: 0.869626
[1050]	valid_0's auc: 0.869614
[1051]	valid_0's auc: 0.86964
[1052]	valid_0's auc: 0.869673
[1053]	valid_0's auc: 0.869725
[1054]	valid_0's auc: 0.869753
[1055]	valid_0's auc: 0.86977
[1056]	valid_0's auc: 0.869822
[1057]	valid_0's auc: 0.86988
[1058]	valid_0's auc: 0.869909
[1059]	valid_0's auc: 0.869992
[1060]	valid_0's auc: 0.870052
[1061]	valid_0's auc: 0.870141
[1062]	valid_0's auc: 0.87017
[1063]	valid_0's auc: 0.870122
[1064]	valid_0's auc: 0.870079
[1065]	valid_0's auc: 0.870127
[1066]	valid_0's auc: 0.870154
[1067]	valid_0's auc: 0.870174
[1068]	valid_0's auc: 0.870179
[1069]	valid_0's auc: 0.870211
[1070]	valid_0's auc: 0.870272
[1071]	valid_0's auc: 0.870302
[1072]	valid_0's auc: 0.870349
[1073]	valid_0's auc: 0.870402
[1074]	valid_0's auc: 0.870432
[1075]	valid_0's auc: 0.870477
[1076]	valid_0's auc: 0.870524
[1077]	valid_0's auc: 0.870589
[1078]	valid_0's auc: 0.870654
[1079]	valid_0's auc: 0.870654
[1080]	valid_0's auc: 0.870679
[1081]	valid_0's auc: 0.87075
[1082]	valid_0's auc: 0.870806
[1083]	valid_0's auc: 0.870821
[1084]	valid_0's auc: 0.870856
[1085]	valid_0's auc: 0.870856
[1086]	valid_0's auc: 0.870898
[1087]	valid_0's auc: 0.870905
[1088]	valid_0's auc: 0.870944
[1089]	valid_0's auc: 0.870956
[1090]	valid_0's auc: 0.871006
[1091]	valid_0's auc: 0.871017
[1092]	valid_0's auc: 0.871
[1093]	valid_0's auc: 0.871043
[1094]	valid_0's auc: 0.871032
[1095]	valid_0's auc: 0.871079
[1096]	valid_0's auc: 0.871069
[1097]	valid_0's auc: 0.871128
[1098]	valid_0's auc: 0.871186
[1099]	valid_0's auc: 0.871211
[1100]	valid_0's auc: 0.871195
[1101]	valid_0's auc: 0.871225
[1102]	valid_0's auc: 0.871246
[1103]	valid_0's auc: 0.87124
[1104]	valid_0's auc: 0.871232
[1105]	valid_0's auc: 0.871279
[1106]	valid_0's auc: 0.871283
[1107]	valid_0's auc: 0.871345
[1108]	valid_0's auc: 0.871376
[1109]	valid_0's auc: 0.871389
[1110]	valid_0's auc: 0.871408
[1111]	valid_0's auc: 0.871399
[1112]	valid_0's auc: 0.871438
[1113]	valid_0's auc: 0.87145
[1114]	valid_0's auc: 0.87147
[1115]	valid_0's auc: 0.871486
[1116]	valid_0's auc: 0.871539
[1117]	valid_0's auc: 0.871585
[1118]	valid_0's auc: 0.871578
[1119]	valid_0's auc: 0.871601
[1120]	valid_0's auc: 0.871612
[1121]	valid_0's auc: 0.871604
[1122]	valid_0's auc: 0.871643
[1123]	valid_0's auc: 0.871662
[1124]	valid_0's auc: 0.871717
[1125]	valid_0's auc: 0.871765
[1126]	valid_0's auc: 0.871798
[1127]	valid_0's auc: 0.871824
[1128]	valid_0's auc: 0.871897
[1129]	valid_0's auc: 0.871976
[1130]	valid_0's auc: 0.872077
[1131]	valid_0's auc: 0.872127
[1132]	valid_0's auc: 0.872185
[1133]	valid_0's auc: 0.872229
[1134]	valid_0's auc: 0.872272
[1135]	valid_0's auc: 0.872263
[1136]	valid_0's auc: 0.872282
[1137]	valid_0's auc: 0.87236
[1138]	valid_0's auc: 0.87237
[1139]	valid_0's auc: 0.872408
[1140]	valid_0's auc: 0.872457
[1141]	valid_0's auc: 0.872474
[1142]	valid_0's auc: 0.872502
[1143]	valid_0's auc: 0.872498
[1144]	valid_0's auc: 0.872504
[1145]	valid_0's auc: 0.872536
[1146]	valid_0's auc: 0.87261
[1147]	valid_0's auc: 0.872624
[1148]	valid_0's auc: 0.872591
[1149]	valid_0's auc: 0.872623
[1150]	valid_0's auc: 0.872636
[1151]	valid_0's auc: 0.872698
[1152]	valid_0's auc: 0.87272
[1153]	valid_0's auc: 0.872749
[1154]	valid_0's auc: 0.872762
[1155]	valid_0's auc: 0.872808
[1156]	valid_0's auc: 0.872782
[1157]	valid_0's auc: 0.872807
[1158]	valid_0's auc: 0.872844
[1159]	valid_0's auc: 0.872895
[1160]	valid_0's auc: 0.872918
[1161]	valid_0's auc: 0.87294
[1162]	valid_0's auc: 0.87303
[1163]	valid_0's auc: 0.873056
[1164]	valid_0's auc: 0.873076
[1165]	valid_0's auc: 0.873131
[1166]	valid_0's auc: 0.873148
[1167]	valid_0's auc: 0.873146
[1168]	valid_0's auc: 0.873183
[1169]	valid_0's auc: 0.873226
[1170]	valid_0's auc: 0.873285
[1171]	valid_0's auc: 0.87331
[1172]	valid_0's auc: 0.873363
[1173]	valid_0's auc: 0.873389
[1174]	valid_0's auc: 0.873404
[1175]	valid_0's auc: 0.873412
[1176]	valid_0's auc: 0.873406
[1177]	valid_0's auc: 0.87343
[1178]	valid_0's auc: 0.873455
[1179]	valid_0's auc: 0.873509
[1180]	valid_0's auc: 0.873564
[1181]	valid_0's auc: 0.873589
[1182]	valid_0's auc: 0.873623
[1183]	valid_0's auc: 0.873641
[1184]	valid_0's auc: 0.87365
[1185]	valid_0's auc: 0.873652
[1186]	valid_0's auc: 0.873669
[1187]	valid_0's auc: 0.873728
[1188]	valid_0's auc: 0.873747
[1189]	valid_0's auc: 0.873749
[1190]	valid_0's auc: 0.873782
[1191]	valid_0's auc: 0.873759
[1192]	valid_0's auc: 0.873788
[1193]	valid_0's auc: 0.873819
[1194]	valid_0's auc: 0.873902
[1195]	valid_0's auc: 0.873923
[1196]	valid_0's auc: 0.87395
[1197]	valid_0's auc: 0.87397
[1198]	valid_0's auc: 0.874042
[1199]	valid_0's auc: 0.874085
[1200]	valid_0's auc: 0.87413
[1201]	valid_0's auc: 0.874118
[1202]	valid_0's auc: 0.874159
[1203]	valid_0's auc: 0.874212
[1204]	valid_0's auc: 0.874276
[1205]	valid_0's auc: 0.87433
[1206]	valid_0's auc: 0.874298
[1207]	valid_0's auc: 0.874329
[1208]	valid_0's auc: 0.87435
[1209]	valid_0's auc: 0.874375
[1210]	valid_0's auc: 0.874391
[1211]	valid_0's auc: 0.87445
[1212]	valid_0's auc: 0.874467
[1213]	valid_0's auc: 0.874473
[1214]	valid_0's auc: 0.874507
[1215]	valid_0's auc: 0.874507
[1216]	valid_0's auc: 0.874512
[1217]	valid_0's auc: 0.874526
[1218]	valid_0's auc: 0.874562
[1219]	valid_0's auc: 0.874558
[1220]	valid_0's auc: 0.874571
[1221]	valid_0's auc: 0.874587
[1222]	valid_0's auc: 0.874637
[1223]	valid_0's auc: 0.874679
[1224]	valid_0's auc: 0.874692
[1225]	valid_0's auc: 0.874765
[1226]	valid_0's auc: 0.874829
[1227]	valid_0's auc: 0.874846
[1228]	valid_0's auc: 0.874839
[1229]	valid_0's auc: 0.874887
[1230]	valid_0's auc: 0.874905
[1231]	valid_0's auc: 0.874919
[1232]	valid_0's auc: 0.874909
[1233]	valid_0's auc: 0.874963
[1234]	valid_0's auc: 0.874994
[1235]	valid_0's auc: 0.87504
[1236]	valid_0's auc: 0.875027
[1237]	valid_0's auc: 0.875065
[1238]	valid_0's auc: 0.875115
[1239]	valid_0's auc: 0.875124
[1240]	valid_0's auc: 0.875106
[1241]	valid_0's auc: 0.875157
[1242]	valid_0's auc: 0.875189
[1243]	valid_0's auc: 0.875212
[1244]	valid_0's auc: 0.875214
[1245]	valid_0's auc: 0.875251
[1246]	valid_0's auc: 0.875266
[1247]	valid_0's auc: 0.875268
[1248]	valid_0's auc: 0.875318
[1249]	valid_0's auc: 0.875289
[1250]	valid_0's auc: 0.875384
[1251]	valid_0's auc: 0.875426
[1252]	valid_0's auc: 0.875465
[1253]	valid_0's auc: 0.875483
[1254]	valid_0's auc: 0.875493
[1255]	valid_0's auc: 0.875551
[1256]	valid_0's auc: 0.875554
[1257]	valid_0's auc: 0.875583
[1258]	valid_0's auc: 0.875592
[1259]	valid_0's auc: 0.875625
[1260]	valid_0's auc: 0.875622
[1261]	valid_0's auc: 0.875612
[1262]	valid_0's auc: 0.875672
[1263]	valid_0's auc: 0.875697
[1264]	valid_0's auc: 0.875683
[1265]	valid_0's auc: 0.875723
[1266]	valid_0's auc: 0.875757
[1267]	valid_0's auc: 0.875795
[1268]	valid_0's auc: 0.875796
[1269]	valid_0's auc: 0.875808
[1270]	valid_0's auc: 0.875841
[1271]	valid_0's auc: 0.875882
[1272]	valid_0's auc: 0.875919
[1273]	valid_0's auc: 0.875948
[1274]	valid_0's auc: 0.875986
[1275]	valid_0's auc: 0.876003
[1276]	valid_0's auc: 0.875993
[1277]	valid_0's auc: 0.876041
[1278]	valid_0's auc: 0.876091
[1279]	valid_0's auc: 0.876115
[1280]	valid_0's auc: 0.876169
[1281]	valid_0's auc: 0.876216
[1282]	valid_0's auc: 0.876236
[1283]	valid_0's auc: 0.876292
[1284]	valid_0's auc: 0.876339
[1285]	valid_0's auc: 0.876357
[1286]	valid_0's auc: 0.87638
[1287]	valid_0's auc: 0.876406
[1288]	valid_0's auc: 0.876404
[1289]	valid_0's auc: 0.876392
[1290]	valid_0's auc: 0.876436
[1291]	valid_0's auc: 0.876464
[1292]	valid_0's auc: 0.876482
[1293]	valid_0's auc: 0.876496
[1294]	valid_0's auc: 0.87653
[1295]	valid_0's auc: 0.876522
[1296]	valid_0's auc: 0.87658
[1297]	valid_0's auc: 0.876581
[1298]	valid_0's auc: 0.876621
[1299]	valid_0's auc: 0.876646
[1300]	valid_0's auc: 0.876678
[1301]	valid_0's auc: 0.876724
[1302]	valid_0's auc: 0.876752
[1303]	valid_0's auc: 0.876774
[1304]	valid_0's auc: 0.876788
[1305]	valid_0's auc: 0.876812
[1306]	valid_0's auc: 0.87684
[1307]	valid_0's auc: 0.876873
[1308]	valid_0's auc: 0.87688
[1309]	valid_0's auc: 0.876901
[1310]	valid_0's auc: 0.876921
[1311]	valid_0's auc: 0.876953
[1312]	valid_0's auc: 0.876963
[1313]	valid_0's auc: 0.876993
[1314]	valid_0's auc: 0.877002
[1315]	valid_0's auc: 0.876992
[1316]	valid_0's auc: 0.876994
[1317]	valid_0's auc: 0.876986
[1318]	valid_0's auc: 0.877005
[1319]	valid_0's auc: 0.877027
[1320]	valid_0's auc: 0.877034
[1321]	valid_0's auc: 0.877049
[1322]	valid_0's auc: 0.877086
[1323]	valid_0's auc: 0.877104
[1324]	valid_0's auc: 0.877129
[1325]	valid_0's auc: 0.877167
[1326]	valid_0's auc: 0.877176
[1327]	valid_0's auc: 0.877203
[1328]	valid_0's auc: 0.877253
[1329]	valid_0's auc: 0.877297
[1330]	valid_0's auc: 0.877317
[1331]	valid_0's auc: 0.877373
[1332]	valid_0's auc: 0.877389
[1333]	valid_0's auc: 0.877381
[1334]	valid_0's auc: 0.877446
[1335]	valid_0's auc: 0.877478
[1336]	valid_0's auc: 0.87748
[1337]	valid_0's auc: 0.877527
[1338]	valid_0's auc: 0.877566
[1339]	valid_0's auc: 0.877592
[1340]	valid_0's auc: 0.877582
[1341]	valid_0's auc: 0.877611
[1342]	valid_0's auc: 0.877627
[1343]	valid_0's auc: 0.877659
[1344]	valid_0's auc: 0.877709
[1345]	valid_0's auc: 0.877758
[1346]	valid_0's auc: 0.877807
[1347]	valid_0's auc: 0.877841
[1348]	valid_0's auc: 0.877876
[1349]	valid_0's auc: 0.877883
[1350]	valid_0's auc: 0.877948
[1351]	valid_0's auc: 0.877977
[1352]	valid_0's auc: 0.877992
[1353]	valid_0's auc: 0.878023
[1354]	valid_0's auc: 0.878057
[1355]	valid_0's auc: 0.878089
[1356]	valid_0's auc: 0.878089
[1357]	valid_0's auc: 0.878095
[1358]	valid_0's auc: 0.878111
[1359]	valid_0's auc: 0.878117
[1360]	valid_0's auc: 0.878125
[1361]	valid_0's auc: 0.878138
[1362]	valid_0's auc: 0.878146
[1363]	valid_0's auc: 0.878183
[1364]	valid_0's auc: 0.878175
[1365]	valid_0's auc: 0.8782
[1366]	valid_0's auc: 0.878194
[1367]	valid_0's auc: 0.87818
[1368]	valid_0's auc: 0.878193
[1369]	valid_0's auc: 0.878174
[1370]	valid_0's auc: 0.878193
[1371]	valid_0's auc: 0.878199
[1372]	valid_0's auc: 0.878237
[1373]	valid_0's auc: 0.87824
[1374]	valid_0's auc: 0.878265
[1375]	valid_0's auc: 0.878304
[1376]	valid_0's auc: 0.878288
[1377]	valid_0's auc: 0.878319
[1378]	valid_0's auc: 0.878331
[1379]	valid_0's auc: 0.878355
[1380]	valid_0's auc: 0.878365
[1381]	valid_0's auc: 0.87839
[1382]	valid_0's auc: 0.878405
[1383]	valid_0's auc: 0.878453
[1384]	valid_0's auc: 0.878454
[1385]	valid_0's auc: 0.878473
[1386]	valid_0's auc: 0.878487
[1387]	valid_0's auc: 0.87849
[1388]	valid_0's auc: 0.878508
[1389]	valid_0's auc: 0.878529
[1390]	valid_0's auc: 0.878543
[1391]	valid_0's auc: 0.878592
[1392]	valid_0's auc: 0.878584
[1393]	valid_0's auc: 0.878557
[1394]	valid_0's auc: 0.878584
[1395]	valid_0's auc: 0.87862
[1396]	valid_0's auc: 0.878632
[1397]	valid_0's auc: 0.878651
[1398]	valid_0's auc: 0.878686
[1399]	valid_0's auc: 0.878705
[1400]	valid_0's auc: 0.878714
[1401]	valid_0's auc: 0.8787
[1402]	valid_0's auc: 0.878727
[1403]	valid_0's auc: 0.87875
[1404]	valid_0's auc: 0.878789
[1405]	valid_0's auc: 0.878825
[1406]	valid_0's auc: 0.87882
[1407]	valid_0's auc: 0.878827
[1408]	valid_0's auc: 0.878842
[1409]	valid_0's auc: 0.878874
[1410]	valid_0's auc: 0.878917
[1411]	valid_0's auc: 0.878947
[1412]	valid_0's auc: 0.878947
[1413]	valid_0's auc: 0.878949
[1414]	valid_0's auc: 0.878976
[1415]	valid_0's auc: 0.878996
[1416]	valid_0's auc: 0.879015
[1417]	valid_0's auc: 0.879017
[1418]	valid_0's auc: 0.878979
[1419]	valid_0's auc: 0.878994
[1420]	valid_0's auc: 0.879027
[1421]	valid_0's auc: 0.879073
[1422]	valid_0's auc: 0.879075
[1423]	valid_0's auc: 0.879096
[1424]	valid_0's auc: 0.879122
[1425]	valid_0's auc: 0.879128
[1426]	valid_0's auc: 0.879133
[1427]	valid_0's auc: 0.879175
[1428]	valid_0's auc: 0.879227
[1429]	valid_0's auc: 0.879274
[1430]	valid_0's auc: 0.879293
[1431]	valid_0's auc: 0.879316
[1432]	valid_0's auc: 0.879325
[1433]	valid_0's auc: 0.879338
[1434]	valid_0's auc: 0.87934
[1435]	valid_0's auc: 0.879374
[1436]	valid_0's auc: 0.8794
[1437]	valid_0's auc: 0.879409
[1438]	valid_0's auc: 0.879445
[1439]	valid_0's auc: 0.879462
[1440]	valid_0's auc: 0.879491
[1441]	valid_0's auc: 0.879512
[1442]	valid_0's auc: 0.879536
[1443]	valid_0's auc: 0.87955
[1444]	valid_0's auc: 0.879585
[1445]	valid_0's auc: 0.87962
[1446]	valid_0's auc: 0.879649
[1447]	valid_0's auc: 0.879658
[1448]	valid_0's auc: 0.87965
[1449]	valid_0's auc: 0.879656
[1450]	valid_0's auc: 0.87966
[1451]	valid_0's auc: 0.879688
[1452]	valid_0's auc: 0.879717
[1453]	valid_0's auc: 0.87974
[1454]	valid_0's auc: 0.879749
[1455]	valid_0's auc: 0.879753
[1456]	valid_0's auc: 0.879746
[1457]	valid_0's auc: 0.879744
[1458]	valid_0's auc: 0.879737
[1459]	valid_0's auc: 0.879776
[1460]	valid_0's auc: 0.87979
[1461]	valid_0's auc: 0.879811
[1462]	valid_0's auc: 0.879842
[1463]	valid_0's auc: 0.879839
[1464]	valid_0's auc: 0.879849
[1465]	valid_0's auc: 0.879865
[1466]	valid_0's auc: 0.879881
[1467]	valid_0's auc: 0.879948
[1468]	valid_0's auc: 0.879968
[1469]	valid_0's auc: 0.880008
[1470]	valid_0's auc: 0.879986
[1471]	valid_0's auc: 0.880014
[1472]	valid_0's auc: 0.880054
[1473]	valid_0's auc: 0.880059
[1474]	valid_0's auc: 0.880053
[1475]	valid_0's auc: 0.880096
[1476]	valid_0's auc: 0.880147
[1477]	valid_0's auc: 0.880182
[1478]	valid_0's auc: 0.88018
[1479]	valid_0's auc: 0.880178
[1480]	valid_0's auc: 0.880192
[1481]	valid_0's auc: 0.88022
[1482]	valid_0's auc: 0.880253
[1483]	valid_0's auc: 0.88027
[1484]	valid_0's auc: 0.880298
[1485]	valid_0's auc: 0.880315
[1486]	valid_0's auc: 0.880334
[1487]	valid_0's auc: 0.880365
[1488]	valid_0's auc: 0.880364
[1489]	valid_0's auc: 0.880355
[1490]	valid_0's auc: 0.880364
[1491]	valid_0's auc: 0.880406
[1492]	valid_0's auc: 0.880401
[1493]	valid_0's auc: 0.880427
[1494]	valid_0's auc: 0.880462
[1495]	valid_0's auc: 0.880477
[1496]	valid_0's auc: 0.880473
[1497]	valid_0's auc: 0.880508
[1498]	valid_0's auc: 0.880527
[1499]	valid_0's auc: 0.880506
[1500]	valid_0's auc: 0.880522
[1501]	valid_0's auc: 0.880569
[1502]	valid_0's auc: 0.880617
[1503]	valid_0's auc: 0.880659
[1504]	valid_0's auc: 0.880655
[1505]	valid_0's auc: 0.880711
[1506]	valid_0's auc: 0.880737
[1507]	valid_0's auc: 0.880745
[1508]	valid_0's auc: 0.880763
[1509]	valid_0's auc: 0.88079
[1510]	valid_0's auc: 0.880801
[1511]	valid_0's auc: 0.880816
[1512]	valid_0's auc: 0.880845
[1513]	valid_0's auc: 0.880852
[1514]	valid_0's auc: 0.880886
[1515]	valid_0's auc: 0.880902
[1516]	valid_0's auc: 0.880906
[1517]	valid_0's auc: 0.88095
[1518]	valid_0's auc: 0.880962
[1519]	valid_0's auc: 0.880957
[1520]	valid_0's auc: 0.880975
[1521]	valid_0's auc: 0.88099
[1522]	valid_0's auc: 0.881019
[1523]	valid_0's auc: 0.881041
[1524]	valid_0's auc: 0.881068
[1525]	valid_0's auc: 0.881085
[1526]	valid_0's auc: 0.881105
[1527]	valid_0's auc: 0.881123
[1528]	valid_0's auc: 0.881156
[1529]	valid_0's auc: 0.881208
[1530]	valid_0's auc: 0.88122
[1531]	valid_0's auc: 0.881259
[1532]	valid_0's auc: 0.881274
[1533]	valid_0's auc: 0.881281
[1534]	valid_0's auc: 0.8813
[1535]	valid_0's auc: 0.88133
[1536]	valid_0's auc: 0.881348
[1537]	valid_0's auc: 0.881373
[1538]	valid_0's auc: 0.881407
[1539]	valid_0's auc: 0.881424
[1540]	valid_0's auc: 0.881454
[1541]	valid_0's auc: 0.88149
[1542]	valid_0's auc: 0.881482
[1543]	valid_0's auc: 0.881497
[1544]	valid_0's auc: 0.88153
[1545]	valid_0's auc: 0.881544
[1546]	valid_0's auc: 0.881565
[1547]	valid_0's auc: 0.881589
[1548]	valid_0's auc: 0.881603
[1549]	valid_0's auc: 0.88162
[1550]	valid_0's auc: 0.881626
[1551]	valid_0's auc: 0.881662
[1552]	valid_0's auc: 0.881671
[1553]	valid_0's auc: 0.881698
[1554]	valid_0's auc: 0.881719
[1555]	valid_0's auc: 0.881751
[1556]	valid_0's auc: 0.88178
[1557]	valid_0's auc: 0.881788
[1558]	valid_0's auc: 0.881814
[1559]	valid_0's auc: 0.88181
[1560]	valid_0's auc: 0.881838
[1561]	valid_0's auc: 0.881871
[1562]	valid_0's auc: 0.881878
[1563]	valid_0's auc: 0.881906
[1564]	valid_0's auc: 0.881952
[1565]	valid_0's auc: 0.881949
[1566]	valid_0's auc: 0.881965
[1567]	valid_0's auc: 0.881951
[1568]	valid_0's auc: 0.881968
[1569]	valid_0's auc: 0.88197
[1570]	valid_0's auc: 0.881995
[1571]	valid_0's auc: 0.882031
[1572]	valid_0's auc: 0.882048
[1573]	valid_0's auc: 0.882055
[1574]	valid_0's auc: 0.882064
[1575]	valid_0's auc: 0.882072
[1576]	valid_0's auc: 0.88212
[1577]	valid_0's auc: 0.882153
[1578]	valid_0's auc: 0.882156
[1579]	valid_0's auc: 0.882221
[1580]	valid_0's auc: 0.882252
[1581]	valid_0's auc: 0.882271
[1582]	valid_0's auc: 0.88228
[1583]	valid_0's auc: 0.882328
[1584]	valid_0's auc: 0.882345
[1585]	valid_0's auc: 0.882371
[1586]	valid_0's auc: 0.882396
[1587]	valid_0's auc: 0.8824
[1588]	valid_0's auc: 0.882399
[1589]	valid_0's auc: 0.882417
[1590]	valid_0's auc: 0.88243
[1591]	valid_0's auc: 0.88244
[1592]	valid_0's auc: 0.882473
[1593]	valid_0's auc: 0.882491
[1594]	valid_0's auc: 0.88253
[1595]	valid_0's auc: 0.882554
[1596]	valid_0's auc: 0.882571
[1597]	valid_0's auc: 0.882591
[1598]	valid_0's auc: 0.882613
[1599]	valid_0's auc: 0.882625
[1600]	valid_0's auc: 0.882663
[1601]	valid_0's auc: 0.882687
[1602]	valid_0's auc: 0.882691
[1603]	valid_0's auc: 0.882713
[1604]	valid_0's auc: 0.882702
[1605]	valid_0's auc: 0.88275
[1606]	valid_0's auc: 0.882769
[1607]	valid_0's auc: 0.882794
[1608]	valid_0's auc: 0.882784
[1609]	valid_0's auc: 0.882795
[1610]	valid_0's auc: 0.882817
[1611]	valid_0's auc: 0.882827
[1612]	valid_0's auc: 0.882848
[1613]	valid_0's auc: 0.882864
[1614]	valid_0's auc: 0.88284
[1615]	valid_0's auc: 0.882852
[1616]	valid_0's auc: 0.882841
[1617]	valid_0's auc: 0.882857
[1618]	valid_0's auc: 0.882883
[1619]	valid_0's auc: 0.882895
[1620]	valid_0's auc: 0.882885
[1621]	valid_0's auc: 0.882914
[1622]	valid_0's auc: 0.882929
[1623]	valid_0's auc: 0.88295
[1624]	valid_0's auc: 0.882963
[1625]	valid_0's auc: 0.88299
[1626]	valid_0's auc: 0.883028
[1627]	valid_0's auc: 0.883044
[1628]	valid_0's auc: 0.883059
[1629]	valid_0's auc: 0.883098
[1630]	valid_0's auc: 0.883116
[1631]	valid_0's auc: 0.883131
[1632]	valid_0's auc: 0.883129
[1633]	valid_0's auc: 0.883129
[1634]	valid_0's auc: 0.883146
[1635]	valid_0's auc: 0.883152
[1636]	valid_0's auc: 0.883153
[1637]	valid_0's auc: 0.883178
[1638]	valid_0's auc: 0.883199
[1639]	valid_0's auc: 0.88319
[1640]	valid_0's auc: 0.883178
[1641]	valid_0's auc: 0.883203
[1642]	valid_0's auc: 0.883217
[1643]	valid_0's auc: 0.88322
[1644]	valid_0's auc: 0.883203
[1645]	valid_0's auc: 0.88319
[1646]	valid_0's auc: 0.883219
[1647]	valid_0's auc: 0.883241
[1648]	valid_0's auc: 0.883254
[1649]	valid_0's auc: 0.88328
[1650]	valid_0's auc: 0.883311
[1651]	valid_0's auc: 0.883333
[1652]	valid_0's auc: 0.883349
[1653]	valid_0's auc: 0.883351
[1654]	valid_0's auc: 0.883361
[1655]	valid_0's auc: 0.883368
[1656]	valid_0's auc: 0.883363
[1657]	valid_0's auc: 0.883383
[1658]	valid_0's auc: 0.883406
[1659]	valid_0's auc: 0.883442
[1660]	valid_0's auc: 0.883454
[1661]	valid_0's auc: 0.883484
[1662]	valid_0's auc: 0.883478
[1663]	valid_0's auc: 0.883498
[1664]	valid_0's auc: 0.883527
[1665]	valid_0's auc: 0.883545
[1666]	valid_0's auc: 0.883545
[1667]	valid_0's auc: 0.883563
[1668]	valid_0's auc: 0.883576
[1669]	valid_0's auc: 0.883551
[1670]	valid_0's auc: 0.883543
[1671]	valid_0's auc: 0.883562
[1672]	valid_0's auc: 0.883568
[1673]	valid_0's auc: 0.883607
[1674]	valid_0's auc: 0.883615
[1675]	valid_0's auc: 0.883632
[1676]	valid_0's auc: 0.883662
[1677]	valid_0's auc: 0.883664
[1678]	valid_0's auc: 0.883684
[1679]	valid_0's auc: 0.883725
[1680]	valid_0's auc: 0.883751
[1681]	valid_0's auc: 0.883761
[1682]	valid_0's auc: 0.883767
[1683]	valid_0's auc: 0.883775
[1684]	valid_0's auc: 0.883782
[1685]	valid_0's auc: 0.883798
[1686]	valid_0's auc: 0.883835
[1687]	valid_0's auc: 0.88387
[1688]	valid_0's auc: 0.883892
[1689]	valid_0's auc: 0.883887
[1690]	valid_0's auc: 0.883909
[1691]	valid_0's auc: 0.883917
[1692]	valid_0's auc: 0.883945
[1693]	valid_0's auc: 0.883978
[1694]	valid_0's auc: 0.883984
[1695]	valid_0's auc: 0.883981
[1696]	valid_0's auc: 0.884015
[1697]	valid_0's auc: 0.884019
[1698]	valid_0's auc: 0.884051
[1699]	valid_0's auc: 0.88407
[1700]	valid_0's auc: 0.884086
[1701]	valid_0's auc: 0.884125
[1702]	valid_0's auc: 0.884124
[1703]	valid_0's auc: 0.884138
[1704]	valid_0's auc: 0.88415
[1705]	valid_0's auc: 0.884168
[1706]	valid_0's auc: 0.884177
[1707]	valid_0's auc: 0.884191
[1708]	valid_0's auc: 0.884238
[1709]	valid_0's auc: 0.884243
[1710]	valid_0's auc: 0.884263
[1711]	valid_0's auc: 0.884274
[1712]	valid_0's auc: 0.884284
[1713]	valid_0's auc: 0.884308
[1714]	valid_0's auc: 0.884311
[1715]	valid_0's auc: 0.884286
[1716]	valid_0's auc: 0.884303
[1717]	valid_0's auc: 0.884311
[1718]	valid_0's auc: 0.884331
[1719]	valid_0's auc: 0.884358
[1720]	valid_0's auc: 0.884364
[1721]	valid_0's auc: 0.884401
[1722]	valid_0's auc: 0.884426
[1723]	valid_0's auc: 0.884454
[1724]	valid_0's auc: 0.884471
[1725]	valid_0's auc: 0.884515
[1726]	valid_0's auc: 0.884545
[1727]	valid_0's auc: 0.88455
[1728]	valid_0's auc: 0.88456
[1729]	valid_0's auc: 0.884577
[1730]	valid_0's auc: 0.88461
[1731]	valid_0's auc: 0.884617
[1732]	valid_0's auc: 0.884622
[1733]	valid_0's auc: 0.884633
[1734]	valid_0's auc: 0.884643
[1735]	valid_0's auc: 0.884661
[1736]	valid_0's auc: 0.884692
[1737]	valid_0's auc: 0.884705
[1738]	valid_0's auc: 0.884702
[1739]	valid_0's auc: 0.884727
[1740]	valid_0's auc: 0.884774
[1741]	valid_0's auc: 0.884808
[1742]	valid_0's auc: 0.884824
[1743]	valid_0's auc: 0.884874
[1744]	valid_0's auc: 0.884904
[1745]	valid_0's auc: 0.884927
[1746]	valid_0's auc: 0.884939
[1747]	valid_0's auc: 0.884961
[1748]	valid_0's auc: 0.884976
[1749]	valid_0's auc: 0.885012
[1750]	valid_0's auc: 0.885044
[1751]	valid_0's auc: 0.885055
[1752]	valid_0's auc: 0.88507
[1753]	valid_0's auc: 0.885101
[1754]	valid_0's auc: 0.885142
[1755]	valid_0's auc: 0.885151
[1756]	valid_0's auc: 0.88516
[1757]	valid_0's auc: 0.885181
[1758]	valid_0's auc: 0.885176
[1759]	valid_0's auc: 0.885188
[1760]	valid_0's auc: 0.885202
[1761]	valid_0's auc: 0.885194
[1762]	valid_0's auc: 0.885199
[1763]	valid_0's auc: 0.8852
[1764]	valid_0's auc: 0.885208
[1765]	valid_0's auc: 0.88521
[1766]	valid_0's auc: 0.885215
[1767]	valid_0's auc: 0.885221
[1768]	valid_0's auc: 0.885249
[1769]	valid_0's auc: 0.885263
[1770]	valid_0's auc: 0.885301
[1771]	valid_0's auc: 0.885285
[1772]	valid_0's auc: 0.885306
[1773]	valid_0's auc: 0.885337
[1774]	valid_0's auc: 0.885367
[1775]	valid_0's auc: 0.885371
[1776]	valid_0's auc: 0.8854
[1777]	valid_0's auc: 0.885424
[1778]	valid_0's auc: 0.885449
[1779]	valid_0's auc: 0.885453
[1780]	valid_0's auc: 0.885457
[1781]	valid_0's auc: 0.885477
[1782]	valid_0's auc: 0.885467
[1783]	valid_0's auc: 0.88548
[1784]	valid_0's auc: 0.885481
[1785]	valid_0's auc: 0.885498
[1786]	valid_0's auc: 0.885503
[1787]	valid_0's auc: 0.885518
[1788]	valid_0's auc: 0.885533
[1789]	valid_0's auc: 0.885546
[1790]	valid_0's auc: 0.88555
[1791]	valid_0's auc: 0.885572
[1792]	valid_0's auc: 0.885585
[1793]	valid_0's auc: 0.885572
[1794]	valid_0's auc: 0.885556
[1795]	valid_0's auc: 0.885568
[1796]	valid_0's auc: 0.885578
[1797]	valid_0's auc: 0.885578
[1798]	valid_0's auc: 0.885598
[1799]	valid_0's auc: 0.885609
[1800]	valid_0's auc: 0.885621
[1801]	valid_0's auc: 0.885644
[1802]	valid_0's auc: 0.885655
[1803]	valid_0's auc: 0.885691
[1804]	valid_0's auc: 0.885693
[1805]	valid_0's auc: 0.885721
[1806]	valid_0's auc: 0.88573
[1807]	valid_0's auc: 0.885742
[1808]	valid_0's auc: 0.885747
[1809]	valid_0's auc: 0.885752
[1810]	valid_0's auc: 0.885745
[1811]	valid_0's auc: 0.885789
[1812]	valid_0's auc: 0.885809
[1813]	valid_0's auc: 0.885834
[1814]	valid_0's auc: 0.88586
[1815]	valid_0's auc: 0.885877
[1816]	valid_0's auc: 0.885905
[1817]	valid_0's auc: 0.885929
[1818]	valid_0's auc: 0.885936
[1819]	valid_0's auc: 0.885938
[1820]	valid_0's auc: 0.885956
[1821]	valid_0's auc: 0.885969
[1822]	valid_0's auc: 0.885975
[1823]	valid_0's auc: 0.885995
[1824]	valid_0's auc: 0.886003
[1825]	valid_0's auc: 0.886038
[1826]	valid_0's auc: 0.886065
[1827]	valid_0's auc: 0.886039
[1828]	valid_0's auc: 0.886053
[1829]	valid_0's auc: 0.886051
[1830]	valid_0's auc: 0.88607
[1831]	valid_0's auc: 0.886087
[1832]	valid_0's auc: 0.886105
[1833]	valid_0's auc: 0.886121
[1834]	valid_0's auc: 0.886153
[1835]	valid_0's auc: 0.88618
[1836]	valid_0's auc: 0.886202
[1837]	valid_0's auc: 0.886213
[1838]	valid_0's auc: 0.886237
[1839]	valid_0's auc: 0.88625
[1840]	valid_0's auc: 0.886288
[1841]	valid_0's auc: 0.88628
[1842]	valid_0's auc: 0.886277
[1843]	valid_0's auc: 0.886269
[1844]	valid_0's auc: 0.886289
[1845]	valid_0's auc: 0.886294
[1846]	valid_0's auc: 0.886308
[1847]	valid_0's auc: 0.886332
[1848]	valid_0's auc: 0.886362
[1849]	valid_0's auc: 0.886395
[1850]	valid_0's auc: 0.886409
[1851]	valid_0's auc: 0.886417
[1852]	valid_0's auc: 0.886414
[1853]	valid_0's auc: 0.886435
[1854]	valid_0's auc: 0.886466
[1855]	valid_0's auc: 0.886498
[1856]	valid_0's auc: 0.886534
[1857]	valid_0's auc: 0.886553
[1858]	valid_0's auc: 0.886538
[1859]	valid_0's auc: 0.886543
[1860]	valid_0's auc: 0.886571
[1861]	valid_0's auc: 0.886616
[1862]	valid_0's auc: 0.88663
[1863]	valid_0's auc: 0.886631
[1864]	valid_0's auc: 0.88663
[1865]	valid_0's auc: 0.886638
[1866]	valid_0's auc: 0.886678
[1867]	valid_0's auc: 0.886705
[1868]	valid_0's auc: 0.886707
[1869]	valid_0's auc: 0.886702
[1870]	valid_0's auc: 0.886704
[1871]	valid_0's auc: 0.886726
[1872]	valid_0's auc: 0.886754
[1873]	valid_0's auc: 0.886784
[1874]	valid_0's auc: 0.886792
[1875]	valid_0's auc: 0.886788
[1876]	valid_0's auc: 0.886805
[1877]	valid_0's auc: 0.8868
[1878]	valid_0's auc: 0.886812
[1879]	valid_0's auc: 0.886828
[1880]	valid_0's auc: 0.886851
[1881]	valid_0's auc: 0.886864
[1882]	valid_0's auc: 0.886872
[1883]	valid_0's auc: 0.88688
[1884]	valid_0's auc: 0.886897
[1885]	valid_0's auc: 0.886921
[1886]	valid_0's auc: 0.886948
[1887]	valid_0's auc: 0.88696
[1888]	valid_0's auc: 0.886975
[1889]	valid_0's auc: 0.886971
[1890]	valid_0's auc: 0.886983
[1891]	valid_0's auc: 0.886976
[1892]	valid_0's auc: 0.887006
[1893]	valid_0's auc: 0.886999
[1894]	valid_0's auc: 0.88703
[1895]	valid_0's auc: 0.887073
[1896]	valid_0's auc: 0.887071
[1897]	valid_0's auc: 0.887092
[1898]	valid_0's auc: 0.887117
[1899]	valid_0's auc: 0.88711
[1900]	valid_0's auc: 0.887108
[1901]	valid_0's auc: 0.887117
[1902]	valid_0's auc: 0.887138
[1903]	valid_0's auc: 0.887149
[1904]	valid_0's auc: 0.887176
[1905]	valid_0's auc: 0.887174
[1906]	valid_0's auc: 0.887183
[1907]	valid_0's auc: 0.887193
[1908]	valid_0's auc: 0.887198
[1909]	valid_0's auc: 0.88719
[1910]	valid_0's auc: 0.887189
[1911]	valid_0's auc: 0.887189
[1912]	valid_0's auc: 0.887192
[1913]	valid_0's auc: 0.887198
[1914]	valid_0's auc: 0.88721
[1915]	valid_0's auc: 0.887227
[1916]	valid_0's auc: 0.887239
[1917]	valid_0's auc: 0.887262
[1918]	valid_0's auc: 0.887277
[1919]	valid_0's auc: 0.88729
[1920]	valid_0's auc: 0.887324
[1921]	valid_0's auc: 0.887312
[1922]	valid_0's auc: 0.887337
[1923]	valid_0's auc: 0.887358
[1924]	valid_0's auc: 0.887377
[1925]	valid_0's auc: 0.887392
[1926]	valid_0's auc: 0.887386
[1927]	valid_0's auc: 0.887388
[1928]	valid_0's auc: 0.887401
[1929]	valid_0's auc: 0.887411
[1930]	valid_0's auc: 0.887415
[1931]	valid_0's auc: 0.887408
[1932]	valid_0's auc: 0.887432
[1933]	valid_0's auc: 0.887434
[1934]	valid_0's auc: 0.887446
[1935]	valid_0's auc: 0.887456
[1936]	valid_0's auc: 0.887447
[1937]	valid_0's auc: 0.887462
[1938]	valid_0's auc: 0.887455
[1939]	valid_0's auc: 0.887463
[1940]	valid_0's auc: 0.887492
[1941]	valid_0's auc: 0.887495
[1942]	valid_0's auc: 0.887502
[1943]	valid_0's auc: 0.887505
[1944]	valid_0's auc: 0.887512
[1945]	valid_0's auc: 0.887526
[1946]	valid_0's auc: 0.887545
[1947]	valid_0's auc: 0.887544
[1948]	valid_0's auc: 0.887554
[1949]	valid_0's auc: 0.887566
[1950]	valid_0's auc: 0.887587
[1951]	valid_0's auc: 0.887606
[1952]	valid_0's auc: 0.88762
[1953]	valid_0's auc: 0.887624
[1954]	valid_0's auc: 0.887628
[1955]	valid_0's auc: 0.887616
[1956]	valid_0's auc: 0.887645
[1957]	valid_0's auc: 0.887658
[1958]	valid_0's auc: 0.887663
[1959]	valid_0's auc: 0.887686
[1960]	valid_0's auc: 0.887677
[1961]	valid_0's auc: 0.887701
[1962]	valid_0's auc: 0.887708
[1963]	valid_0's auc: 0.887738
[1964]	valid_0's auc: 0.887739
[1965]	valid_0's auc: 0.88775
[1966]	valid_0's auc: 0.887757
[1967]	valid_0's auc: 0.887744
[1968]	valid_0's auc: 0.887771
[1969]	valid_0's auc: 0.8878
[1970]	valid_0's auc: 0.887825
[1971]	valid_0's auc: 0.887834
[1972]	valid_0's auc: 0.887833
[1973]	valid_0's auc: 0.887848
[1974]	valid_0's auc: 0.887869
[1975]	valid_0's auc: 0.887887
[1976]	valid_0's auc: 0.887911
[1977]	valid_0's auc: 0.887935
[1978]	valid_0's auc: 0.887931
[1979]	valid_0's auc: 0.887933
[1980]	valid_0's auc: 0.887947
[1981]	valid_0's auc: 0.887984
[1982]	valid_0's auc: 0.888013
[1983]	valid_0's auc: 0.888029
[1984]	valid_0's auc: 0.88804
[1985]	valid_0's auc: 0.88805
[1986]	valid_0's auc: 0.888061
[1987]	valid_0's auc: 0.888067
[1988]	valid_0's auc: 0.888074
[1989]	valid_0's auc: 0.888093
[1990]	valid_0's auc: 0.888103
[1991]	valid_0's auc: 0.888124
[1992]	valid_0's auc: 0.888122
[1993]	valid_0's auc: 0.888129
[1994]	valid_0's auc: 0.888156
[1995]	valid_0's auc: 0.888164
[1996]	valid_0's auc: 0.888162
[1997]	valid_0's auc: 0.888191
[1998]	valid_0's auc: 0.888204
[1999]	valid_0's auc: 0.888202
[2000]	valid_0's auc: 0.888211
[2001]	valid_0's auc: 0.888228
[2002]	valid_0's auc: 0.888249
[2003]	valid_0's auc: 0.888267
[2004]	valid_0's auc: 0.888276
[2005]	valid_0's auc: 0.888281
[2006]	valid_0's auc: 0.888276
[2007]	valid_0's auc: 0.888291
[2008]	valid_0's auc: 0.888288
[2009]	valid_0's auc: 0.888299
[2010]	valid_0's auc: 0.888303
[2011]	valid_0's auc: 0.888321
[2012]	valid_0's auc: 0.888348
[2013]	valid_0's auc: 0.888333
[2014]	valid_0's auc: 0.888351
[2015]	valid_0's auc: 0.888371
[2016]	valid_0's auc: 0.888396
[2017]	valid_0's auc: 0.888415
[2018]	valid_0's auc: 0.888423
[2019]	valid_0's auc: 0.888423
[2020]	valid_0's auc: 0.888432
[2021]	valid_0's auc: 0.888433
[2022]	valid_0's auc: 0.888446
[2023]	valid_0's auc: 0.888465
[2024]	valid_0's auc: 0.888474
[2025]	valid_0's auc: 0.888493
[2026]	valid_0's auc: 0.888501
[2027]	valid_0's auc: 0.888518
[2028]	valid_0's auc: 0.888508
[2029]	valid_0's auc: 0.888509
[2030]	valid_0's auc: 0.888526
[2031]	valid_0's auc: 0.888514
[2032]	valid_0's auc: 0.888515
[2033]	valid_0's auc: 0.888505
[2034]	valid_0's auc: 0.888529
[2035]	valid_0's auc: 0.88853
[2036]	valid_0's auc: 0.888542
[2037]	valid_0's auc: 0.888558
[2038]	valid_0's auc: 0.888579
[2039]	valid_0's auc: 0.888593
[2040]	valid_0's auc: 0.888595
[2041]	valid_0's auc: 0.888607
[2042]	valid_0's auc: 0.888601
[2043]	valid_0's auc: 0.888577
[2044]	valid_0's auc: 0.888591
[2045]	valid_0's auc: 0.888601
[2046]	valid_0's auc: 0.888607
[2047]	valid_0's auc: 0.888626
[2048]	valid_0's auc: 0.888631
[2049]	valid_0's auc: 0.888654
[2050]	valid_0's auc: 0.888664
[2051]	valid_0's auc: 0.888668
[2052]	valid_0's auc: 0.888657
[2053]	valid_0's auc: 0.888666
[2054]	valid_0's auc: 0.888654
[2055]	valid_0's auc: 0.888669
[2056]	valid_0's auc: 0.888668
[2057]	valid_0's auc: 0.888678
[2058]	valid_0's auc: 0.888698
[2059]	valid_0's auc: 0.888694
[2060]	valid_0's auc: 0.888702
[2061]	valid_0's auc: 0.888706
[2062]	valid_0's auc: 0.888728
[2063]	valid_0's auc: 0.88873
[2064]	valid_0's auc: 0.888745
[2065]	valid_0's auc: 0.888739
[2066]	valid_0's auc: 0.888761
[2067]	valid_0's auc: 0.888763
[2068]	valid_0's auc: 0.88876
[2069]	valid_0's auc: 0.88878
[2070]	valid_0's auc: 0.888776
[2071]	valid_0's auc: 0.88878
[2072]	valid_0's auc: 0.888789
[2073]	valid_0's auc: 0.888792
[2074]	valid_0's auc: 0.888794
[2075]	valid_0's auc: 0.888801
[2076]	valid_0's auc: 0.888808
[2077]	valid_0's auc: 0.888827
[2078]	valid_0's auc: 0.888836
[2079]	valid_0's auc: 0.888845
[2080]	valid_0's auc: 0.888855
[2081]	valid_0's auc: 0.888862
[2082]	valid_0's auc: 0.888859
[2083]	valid_0's auc: 0.888871
[2084]	valid_0's auc: 0.888903
[2085]	valid_0's auc: 0.888923
[2086]	valid_0's auc: 0.888931
[2087]	valid_0's auc: 0.888952
[2088]	valid_0's auc: 0.888966
[2089]	valid_0's auc: 0.888992
[2090]	valid_0's auc: 0.889007
[2091]	valid_0's auc: 0.889024
[2092]	valid_0's auc: 0.889038
[2093]	valid_0's auc: 0.889034
[2094]	valid_0's auc: 0.889046
[2095]	valid_0's auc: 0.889043
[2096]	valid_0's auc: 0.889047
[2097]	valid_0's auc: 0.88905
[2098]	valid_0's auc: 0.889062
[2099]	valid_0's auc: 0.889065
[2100]	valid_0's auc: 0.889072
[2101]	valid_0's auc: 0.889087
[2102]	valid_0's auc: 0.889088
[2103]	valid_0's auc: 0.889088
[2104]	valid_0's auc: 0.889104
[2105]	valid_0's auc: 0.889096
[2106]	valid_0's auc: 0.889113
[2107]	valid_0's auc: 0.889107
[2108]	valid_0's auc: 0.889119
[2109]	valid_0's auc: 0.889139
[2110]	valid_0's auc: 0.889156
[2111]	valid_0's auc: 0.889177
[2112]	valid_0's auc: 0.889177
[2113]	valid_0's auc: 0.889193
[2114]	valid_0's auc: 0.889224
[2115]	valid_0's auc: 0.889244
[2116]	valid_0's auc: 0.88925
[2117]	valid_0's auc: 0.889257
[2118]	valid_0's auc: 0.889257
[2119]	valid_0's auc: 0.889256
[2120]	valid_0's auc: 0.889266
[2121]	valid_0's auc: 0.889271
[2122]	valid_0's auc: 0.889277
[2123]	valid_0's auc: 0.889294
[2124]	valid_0's auc: 0.889293
[2125]	valid_0's auc: 0.889298
[2126]	valid_0's auc: 0.88931
[2127]	valid_0's auc: 0.889312
[2128]	valid_0's auc: 0.88933
[2129]	valid_0's auc: 0.889326
[2130]	valid_0's auc: 0.889333
[2131]	valid_0's auc: 0.889336
[2132]	valid_0's auc: 0.889346
[2133]	valid_0's auc: 0.889353
[2134]	valid_0's auc: 0.889358
[2135]	valid_0's auc: 0.889374
[2136]	valid_0's auc: 0.889397
[2137]	valid_0's auc: 0.889384
[2138]	valid_0's auc: 0.889375
[2139]	valid_0's auc: 0.889397
[2140]	valid_0's auc: 0.889412
[2141]	valid_0's auc: 0.889415
[2142]	valid_0's auc: 0.889404
[2143]	valid_0's auc: 0.889429
[2144]	valid_0's auc: 0.889445
[2145]	valid_0's auc: 0.88946
[2146]	valid_0's auc: 0.889459
[2147]	valid_0's auc: 0.88947
[2148]	valid_0's auc: 0.889503
[2149]	valid_0's auc: 0.889522
[2150]	valid_0's auc: 0.889517
[2151]	valid_0's auc: 0.88953
[2152]	valid_0's auc: 0.889547
[2153]	valid_0's auc: 0.889585
[2154]	valid_0's auc: 0.889596
[2155]	valid_0's auc: 0.889593
[2156]	valid_0's auc: 0.889618
[2157]	valid_0's auc: 0.889626
[2158]	valid_0's auc: 0.889628
[2159]	valid_0's auc: 0.889638
[2160]	valid_0's auc: 0.88965
[2161]	valid_0's auc: 0.889645
[2162]	valid_0's auc: 0.88965
[2163]	valid_0's auc: 0.889666
[2164]	valid_0's auc: 0.889678
[2165]	valid_0's auc: 0.889685
[2166]	valid_0's auc: 0.889683
[2167]	valid_0's auc: 0.889673
[2168]	valid_0's auc: 0.889685
[2169]	valid_0's auc: 0.889685
[2170]	valid_0's auc: 0.889682
[2171]	valid_0's auc: 0.889682
[2172]	valid_0's auc: 0.889688
[2173]	valid_0's auc: 0.88969
[2174]	valid_0's auc: 0.889689
[2175]	valid_0's auc: 0.889705
[2176]	valid_0's auc: 0.88973
[2177]	valid_0's auc: 0.889718
[2178]	valid_0's auc: 0.889728
[2179]	valid_0's auc: 0.889716
[2180]	valid_0's auc: 0.889731
[2181]	valid_0's auc: 0.889735
[2182]	valid_0's auc: 0.88974
[2183]	valid_0's auc: 0.889753
[2184]	valid_0's auc: 0.889766
[2185]	valid_0's auc: 0.889783
[2186]	valid_0's auc: 0.889782
[2187]	valid_0's auc: 0.889806
[2188]	valid_0's auc: 0.889813
[2189]	valid_0's auc: 0.889823
[2190]	valid_0's auc: 0.889838
[2191]	valid_0's auc: 0.889839
[2192]	valid_0's auc: 0.889831
[2193]	valid_0's auc: 0.889835
[2194]	valid_0's auc: 0.889836
[2195]	valid_0's auc: 0.889827
[2196]	valid_0's auc: 0.889842
[2197]	valid_0's auc: 0.88985
[2198]	valid_0's auc: 0.889848
[2199]	valid_0's auc: 0.889851
[2200]	valid_0's auc: 0.889877
[2201]	valid_0's auc: 0.889887
[2202]	valid_0's auc: 0.889889
[2203]	valid_0's auc: 0.889892
[2204]	valid_0's auc: 0.889908
[2205]	valid_0's auc: 0.889935
[2206]	valid_0's auc: 0.889937
[2207]	valid_0's auc: 0.889934
[2208]	valid_0's auc: 0.889952
[2209]	valid_0's auc: 0.889962
[2210]	valid_0's auc: 0.889964
[2211]	valid_0's auc: 0.889973
[2212]	valid_0's auc: 0.889995
[2213]	valid_0's auc: 0.890011
[2214]	valid_0's auc: 0.890024
[2215]	valid_0's auc: 0.890044
[2216]	valid_0's auc: 0.890055
[2217]	valid_0's auc: 0.890066
[2218]	valid_0's auc: 0.890086
[2219]	valid_0's auc: 0.890111
[2220]	valid_0's auc: 0.890121
[2221]	valid_0's auc: 0.890129
[2222]	valid_0's auc: 0.890133
[2223]	valid_0's auc: 0.890142
[2224]	valid_0's auc: 0.890151
[2225]	valid_0's auc: 0.890174
[2226]	valid_0's auc: 0.890202
[2227]	valid_0's auc: 0.890196
[2228]	valid_0's auc: 0.890196
[2229]	valid_0's auc: 0.89019
[2230]	valid_0's auc: 0.890196
[2231]	valid_0's auc: 0.890198
[2232]	valid_0's auc: 0.890195
[2233]	valid_0's auc: 0.890219
[2234]	valid_0's auc: 0.890236
[2235]	valid_0's auc: 0.890243
[2236]	valid_0's auc: 0.890248
[2237]	valid_0's auc: 0.890254
[2238]	valid_0's auc: 0.890262
[2239]	valid_0's auc: 0.890258
[2240]	valid_0's auc: 0.890285
[2241]	valid_0's auc: 0.890283
[2242]	valid_0's auc: 0.890286
[2243]	valid_0's auc: 0.890297
[2244]	valid_0's auc: 0.890301
[2245]	valid_0's auc: 0.890312
[2246]	valid_0's auc: 0.890329
[2247]	valid_0's auc: 0.890338
[2248]	valid_0's auc: 0.890343
[2249]	valid_0's auc: 0.890371
[2250]	valid_0's auc: 0.890374
[2251]	valid_0's auc: 0.890374
[2252]	valid_0's auc: 0.890375
[2253]	valid_0's auc: 0.890387
[2254]	valid_0's auc: 0.890395
[2255]	valid_0's auc: 0.890394
[2256]	valid_0's auc: 0.890393
[2257]	valid_0's auc: 0.890403
[2258]	valid_0's auc: 0.890425
[2259]	valid_0's auc: 0.890448
[2260]	valid_0's auc: 0.890454
[2261]	valid_0's auc: 0.890473
[2262]	valid_0's auc: 0.890501
[2263]	valid_0's auc: 0.890498
[2264]	valid_0's auc: 0.890508
[2265]	valid_0's auc: 0.890524
[2266]	valid_0's auc: 0.890542
[2267]	valid_0's auc: 0.89054
[2268]	valid_0's auc: 0.890542
[2269]	valid_0's auc: 0.890551
[2270]	valid_0's auc: 0.890549
[2271]	valid_0's auc: 0.89057
[2272]	valid_0's auc: 0.890585
[2273]	valid_0's auc: 0.890601
[2274]	valid_0's auc: 0.890606
[2275]	valid_0's auc: 0.890619
[2276]	valid_0's auc: 0.890624
[2277]	valid_0's auc: 0.890619
[2278]	valid_0's auc: 0.890613
[2279]	valid_0's auc: 0.89062
[2280]	valid_0's auc: 0.890625
[2281]	valid_0's auc: 0.890652
[2282]	valid_0's auc: 0.890658
[2283]	valid_0's auc: 0.890679
[2284]	valid_0's auc: 0.890687
[2285]	valid_0's auc: 0.890694
[2286]	valid_0's auc: 0.890694
[2287]	valid_0's auc: 0.890694
[2288]	valid_0's auc: 0.890707
[2289]	valid_0's auc: 0.890727
[2290]	valid_0's auc: 0.890719
[2291]	valid_0's auc: 0.890747
[2292]	valid_0's auc: 0.890727
[2293]	valid_0's auc: 0.89073
[2294]	valid_0's auc: 0.890726
[2295]	valid_0's auc: 0.890746
[2296]	valid_0's auc: 0.890762
[2297]	valid_0's auc: 0.89078
[2298]	valid_0's auc: 0.890803
[2299]	valid_0's auc: 0.890805
[2300]	valid_0's auc: 0.890817
[2301]	valid_0's auc: 0.890828
[2302]	valid_0's auc: 0.890846
[2303]	valid_0's auc: 0.890857
[2304]	valid_0's auc: 0.890862
[2305]	valid_0's auc: 0.89088
[2306]	valid_0's auc: 0.890895
[2307]	valid_0's auc: 0.890901
[2308]	valid_0's auc: 0.890897
[2309]	valid_0's auc: 0.890904
[2310]	valid_0's auc: 0.890924
[2311]	valid_0's auc: 0.890941
[2312]	valid_0's auc: 0.890951
[2313]	valid_0's auc: 0.890962
[2314]	valid_0's auc: 0.890983
[2315]	valid_0's auc: 0.891
[2316]	valid_0's auc: 0.890981
[2317]	valid_0's auc: 0.890998
[2318]	valid_0's auc: 0.891012
[2319]	valid_0's auc: 0.891018
[2320]	valid_0's auc: 0.891037
[2321]	valid_0's auc: 0.891034
[2322]	valid_0's auc: 0.891053
[2323]	valid_0's auc: 0.891067
[2324]	valid_0's auc: 0.891087
[2325]	valid_0's auc: 0.89109
[2326]	valid_0's auc: 0.891095
[2327]	valid_0's auc: 0.891102
[2328]	valid_0's auc: 0.891116
[2329]	valid_0's auc: 0.891144
[2330]	valid_0's auc: 0.891158
[2331]	valid_0's auc: 0.891159
[2332]	valid_0's auc: 0.891165
[2333]	valid_0's auc: 0.891182
[2334]	valid_0's auc: 0.891191
[2335]	valid_0's auc: 0.891215
[2336]	valid_0's auc: 0.891205
[2337]	valid_0's auc: 0.891202
[2338]	valid_0's auc: 0.891211
[2339]	valid_0's auc: 0.891215
[2340]	valid_0's auc: 0.891225
[2341]	valid_0's auc: 0.891241
[2342]	valid_0's auc: 0.89127
[2343]	valid_0's auc: 0.891297
[2344]	valid_0's auc: 0.891303
[2345]	valid_0's auc: 0.89131
[2346]	valid_0's auc: 0.891306
[2347]	valid_0's auc: 0.891326
[2348]	valid_0's auc: 0.891331
[2349]	valid_0's auc: 0.891331
[2350]	valid_0's auc: 0.891352
[2351]	valid_0's auc: 0.891336
[2352]	valid_0's auc: 0.891344
[2353]	valid_0's auc: 0.891328
[2354]	valid_0's auc: 0.891318
[2355]	valid_0's auc: 0.891326
[2356]	valid_0's auc: 0.891321
[2357]	valid_0's auc: 0.891316
[2358]	valid_0's auc: 0.891343
[2359]	valid_0's auc: 0.891353
[2360]	valid_0's auc: 0.891356
[2361]	valid_0's auc: 0.891358
[2362]	valid_0's auc: 0.891346
[2363]	valid_0's auc: 0.891348
[2364]	valid_0's auc: 0.891357
[2365]	valid_0's auc: 0.89137
[2366]	valid_0's auc: 0.891367
[2367]	valid_0's auc: 0.891388
[2368]	valid_0's auc: 0.891384
[2369]	valid_0's auc: 0.891389
[2370]	valid_0's auc: 0.891402
[2371]	valid_0's auc: 0.891386
[2372]	valid_0's auc: 0.891391
[2373]	valid_0's auc: 0.891414
[2374]	valid_0's auc: 0.891425
[2375]	valid_0's auc: 0.89144
[2376]	valid_0's auc: 0.891447
[2377]	valid_0's auc: 0.891456
[2378]	valid_0's auc: 0.891449
[2379]	valid_0's auc: 0.891476
[2380]	valid_0's auc: 0.891487
[2381]	valid_0's auc: 0.891494
[2382]	valid_0's auc: 0.891519
[2383]	valid_0's auc: 0.891519
[2384]	valid_0's auc: 0.891522
[2385]	valid_0's auc: 0.891524
[2386]	valid_0's auc: 0.89152
[2387]	valid_0's auc: 0.891559
[2388]	valid_0's auc: 0.891571
[2389]	valid_0's auc: 0.891574
[2390]	valid_0's auc: 0.891594
[2391]	valid_0's auc: 0.891604
[2392]	valid_0's auc: 0.891588
[2393]	valid_0's auc: 0.891607
[2394]	valid_0's auc: 0.891609
[2395]	valid_0's auc: 0.8916
[2396]	valid_0's auc: 0.891591
[2397]	valid_0's auc: 0.891606
[2398]	valid_0's auc: 0.891598
[2399]	valid_0's auc: 0.891616
[2400]	valid_0's auc: 0.891628
[2401]	valid_0's auc: 0.891647
[2402]	valid_0's auc: 0.891652
[2403]	valid_0's auc: 0.891661
[2404]	valid_0's auc: 0.891648
[2405]	valid_0's auc: 0.891641
[2406]	valid_0's auc: 0.891666
[2407]	valid_0's auc: 0.891675
[2408]	valid_0's auc: 0.891674
[2409]	valid_0's auc: 0.891672
[2410]	valid_0's auc: 0.891677
[2411]	valid_0's auc: 0.891689
[2412]	valid_0's auc: 0.891688
[2413]	valid_0's auc: 0.891702
[2414]	valid_0's auc: 0.891723
[2415]	valid_0's auc: 0.891735
[2416]	valid_0's auc: 0.891744
[2417]	valid_0's auc: 0.891761
[2418]	valid_0's auc: 0.89176
[2419]	valid_0's auc: 0.891755
[2420]	valid_0's auc: 0.891762
[2421]	valid_0's auc: 0.891759
[2422]	valid_0's auc: 0.891778
[2423]	valid_0's auc: 0.891765
[2424]	valid_0's auc: 0.891779
[2425]	valid_0's auc: 0.891789
[2426]	valid_0's auc: 0.891795
[2427]	valid_0's auc: 0.891805
[2428]	valid_0's auc: 0.891806
[2429]	valid_0's auc: 0.891813
[2430]	valid_0's auc: 0.891822
[2431]	valid_0's auc: 0.891829
[2432]	valid_0's auc: 0.891829
[2433]	valid_0's auc: 0.89183
[2434]	valid_0's auc: 0.891841
[2435]	valid_0's auc: 0.891841
[2436]	valid_0's auc: 0.891848
[2437]	valid_0's auc: 0.891855
[2438]	valid_0's auc: 0.891863
[2439]	valid_0's auc: 0.891873
[2440]	valid_0's auc: 0.891869
[2441]	valid_0's auc: 0.891873
[2442]	valid_0's auc: 0.891874
[2443]	valid_0's auc: 0.891895
[2444]	valid_0's auc: 0.891905
[2445]	valid_0's auc: 0.891913
[2446]	valid_0's auc: 0.891924
[2447]	valid_0's auc: 0.891953
[2448]	valid_0's auc: 0.891947
[2449]	valid_0's auc: 0.891955
[2450]	valid_0's auc: 0.891948
[2451]	valid_0's auc: 0.891967
[2452]	valid_0's auc: 0.891966
[2453]	valid_0's auc: 0.891967
[2454]	valid_0's auc: 0.89198
[2455]	valid_0's auc: 0.891988
[2456]	valid_0's auc: 0.891993
[2457]	valid_0's auc: 0.892006
[2458]	valid_0's auc: 0.892029
[2459]	valid_0's auc: 0.892047
[2460]	valid_0's auc: 0.892051
[2461]	valid_0's auc: 0.892058
[2462]	valid_0's auc: 0.892062
[2463]	valid_0's auc: 0.892083
[2464]	valid_0's auc: 0.892086
[2465]	valid_0's auc: 0.8921
[2466]	valid_0's auc: 0.892109
[2467]	valid_0's auc: 0.892113
[2468]	valid_0's auc: 0.89212
[2469]	valid_0's auc: 0.892112
[2470]	valid_0's auc: 0.892115
[2471]	valid_0's auc: 0.892121
[2472]	valid_0's auc: 0.892141
[2473]	valid_0's auc: 0.892179
[2474]	valid_0's auc: 0.892191
[2475]	valid_0's auc: 0.892188
[2476]	valid_0's auc: 0.892208
[2477]	valid_0's auc: 0.892224
[2478]	valid_0's auc: 0.892225
[2479]	valid_0's auc: 0.892233
[2480]	valid_0's auc: 0.892241
[2481]	valid_0's auc: 0.89225
[2482]	valid_0's auc: 0.892251
[2483]	valid_0's auc: 0.892254
[2484]	valid_0's auc: 0.892259
[2485]	valid_0's auc: 0.892265
[2486]	valid_0's auc: 0.892282
[2487]	valid_0's auc: 0.892277
[2488]	valid_0's auc: 0.892285
[2489]	valid_0's auc: 0.8923
[2490]	valid_0's auc: 0.892301
[2491]	valid_0's auc: 0.892313
[2492]	valid_0's auc: 0.892329
[2493]	valid_0's auc: 0.89233
[2494]	valid_0's auc: 0.892338
[2495]	valid_0's auc: 0.892346
[2496]	valid_0's auc: 0.892347
[2497]	valid_0's auc: 0.89235
[2498]	valid_0's auc: 0.892347
[2499]	valid_0's auc: 0.892348
[2500]	valid_0's auc: 0.892345
[2501]	valid_0's auc: 0.89234
[2502]	valid_0's auc: 0.892355
[2503]	valid_0's auc: 0.892367
[2504]	valid_0's auc: 0.892372
[2505]	valid_0's auc: 0.892397
[2506]	valid_0's auc: 0.8924
[2507]	valid_0's auc: 0.892402
[2508]	valid_0's auc: 0.892411
[2509]	valid_0's auc: 0.892438
[2510]	valid_0's auc: 0.892447
[2511]	valid_0's auc: 0.892441
[2512]	valid_0's auc: 0.892443
[2513]	valid_0's auc: 0.892454
[2514]	valid_0's auc: 0.892473
[2515]	valid_0's auc: 0.892489
[2516]	valid_0's auc: 0.8925
[2517]	valid_0's auc: 0.892518
[2518]	valid_0's auc: 0.892542
[2519]	valid_0's auc: 0.892556
[2520]	valid_0's auc: 0.892565
[2521]	valid_0's auc: 0.892578
[2522]	valid_0's auc: 0.892585
[2523]	valid_0's auc: 0.89259
[2524]	valid_0's auc: 0.892604
[2525]	valid_0's auc: 0.892609
[2526]	valid_0's auc: 0.892627
[2527]	valid_0's auc: 0.892632
[2528]	valid_0's auc: 0.892635
[2529]	valid_0's auc: 0.892639
[2530]	valid_0's auc: 0.892653
[2531]	valid_0's auc: 0.892648
[2532]	valid_0's auc: 0.892663
[2533]	valid_0's auc: 0.892665
[2534]	valid_0's auc: 0.892665
[2535]	valid_0's auc: 0.892652
[2536]	valid_0's auc: 0.892673
[2537]	valid_0's auc: 0.892679
[2538]	valid_0's auc: 0.89269
[2539]	valid_0's auc: 0.892689
[2540]	valid_0's auc: 0.892674
[2541]	valid_0's auc: 0.892693
[2542]	valid_0's auc: 0.892701
[2543]	valid_0's auc: 0.892706
[2544]	valid_0's auc: 0.892715
[2545]	valid_0's auc: 0.892711
[2546]	valid_0's auc: 0.892721
[2547]	valid_0's auc: 0.892726
[2548]	valid_0's auc: 0.892743
[2549]	valid_0's auc: 0.892749
[2550]	valid_0's auc: 0.892749
[2551]	valid_0's auc: 0.892764
[2552]	valid_0's auc: 0.892777
[2553]	valid_0's auc: 0.892802
[2554]	valid_0's auc: 0.892808
[2555]	valid_0's auc: 0.892819
[2556]	valid_0's auc: 0.892828
[2557]	valid_0's auc: 0.892821
[2558]	valid_0's auc: 0.892814
[2559]	valid_0's auc: 0.892826
[2560]	valid_0's auc: 0.892839
[2561]	valid_0's auc: 0.892841
[2562]	valid_0's auc: 0.892862
[2563]	valid_0's auc: 0.892867
[2564]	valid_0's auc: 0.892858
[2565]	valid_0's auc: 0.892855
[2566]	valid_0's auc: 0.892859
[2567]	valid_0's auc: 0.892869
[2568]	valid_0's auc: 0.89288
[2569]	valid_0's auc: 0.892887
[2570]	valid_0's auc: 0.892903
[2571]	valid_0's auc: 0.892919
[2572]	valid_0's auc: 0.892938
[2573]	valid_0's auc: 0.892944
[2574]	valid_0's auc: 0.892962
[2575]	valid_0's auc: 0.892953
[2576]	valid_0's auc: 0.89296
[2577]	valid_0's auc: 0.892974
[2578]	valid_0's auc: 0.892977
[2579]	valid_0's auc: 0.892989
[2580]	valid_0's auc: 0.893012
[2581]	valid_0's auc: 0.893013
[2582]	valid_0's auc: 0.893022
[2583]	valid_0's auc: 0.893022
[2584]	valid_0's auc: 0.893032
[2585]	valid_0's auc: 0.893047
[2586]	valid_0's auc: 0.89307
[2587]	valid_0's auc: 0.893074
[2588]	valid_0's auc: 0.893068
[2589]	valid_0's auc: 0.893085
[2590]	valid_0's auc: 0.893089
[2591]	valid_0's auc: 0.8931
[2592]	valid_0's auc: 0.8931
[2593]	valid_0's auc: 0.893115
[2594]	valid_0's auc: 0.893127
[2595]	valid_0's auc: 0.893139
[2596]	valid_0's auc: 0.893141
[2597]	valid_0's auc: 0.89315
[2598]	valid_0's auc: 0.893156
[2599]	valid_0's auc: 0.893175
[2600]	valid_0's auc: 0.893188
[2601]	valid_0's auc: 0.893185
[2602]	valid_0's auc: 0.89319
[2603]	valid_0's auc: 0.893201
[2604]	valid_0's auc: 0.893215
[2605]	valid_0's auc: 0.893218
[2606]	valid_0's auc: 0.89323
[2607]	valid_0's auc: 0.893238
[2608]	valid_0's auc: 0.893235
[2609]	valid_0's auc: 0.893236
[2610]	valid_0's auc: 0.893242
[2611]	valid_0's auc: 0.893248
[2612]	valid_0's auc: 0.893274
[2613]	valid_0's auc: 0.893265
[2614]	valid_0's auc: 0.893274
[2615]	valid_0's auc: 0.893287
[2616]	valid_0's auc: 0.893288
[2617]	valid_0's auc: 0.893289
[2618]	valid_0's auc: 0.893278
[2619]	valid_0's auc: 0.893275
[2620]	valid_0's auc: 0.893278
[2621]	valid_0's auc: 0.893261
[2622]	valid_0's auc: 0.893276
[2623]	valid_0's auc: 0.893275
[2624]	valid_0's auc: 0.893283
[2625]	valid_0's auc: 0.893291
[2626]	valid_0's auc: 0.893291
[2627]	valid_0's auc: 0.893297
[2628]	valid_0's auc: 0.893311
[2629]	valid_0's auc: 0.893331
[2630]	valid_0's auc: 0.89334
[2631]	valid_0's auc: 0.893354
[2632]	valid_0's auc: 0.89337
[2633]	valid_0's auc: 0.893386
[2634]	valid_0's auc: 0.893402
[2635]	valid_0's auc: 0.893406
[2636]	valid_0's auc: 0.893418
[2637]	valid_0's auc: 0.893419
[2638]	valid_0's auc: 0.893419
[2639]	valid_0's auc: 0.893419
[2640]	valid_0's auc: 0.893437
[2641]	valid_0's auc: 0.89344
[2642]	valid_0's auc: 0.893441
[2643]	valid_0's auc: 0.893453
[2644]	valid_0's auc: 0.893458
[2645]	valid_0's auc: 0.893467
[2646]	valid_0's auc: 0.893471
[2647]	valid_0's auc: 0.89348
[2648]	valid_0's auc: 0.893492
[2649]	valid_0's auc: 0.893496
[2650]	valid_0's auc: 0.893495
[2651]	valid_0's auc: 0.893503
[2652]	valid_0's auc: 0.893508
[2653]	valid_0's auc: 0.893502
[2654]	valid_0's auc: 0.893497
[2655]	valid_0's auc: 0.893508
[2656]	valid_0's auc: 0.893517
[2657]	valid_0's auc: 0.893522
[2658]	valid_0's auc: 0.893532
[2659]	valid_0's auc: 0.893518
[2660]	valid_0's auc: 0.893509
[2661]	valid_0's auc: 0.893512
[2662]	valid_0's auc: 0.89351
[2663]	valid_0's auc: 0.893504
[2664]	valid_0's auc: 0.893532
[2665]	valid_0's auc: 0.893532
[2666]	valid_0's auc: 0.893553
[2667]	valid_0's auc: 0.893554
[2668]	valid_0's auc: 0.89355
[2669]	valid_0's auc: 0.89357
[2670]	valid_0's auc: 0.893569
[2671]	valid_0's auc: 0.893571
[2672]	valid_0's auc: 0.893572
[2673]	valid_0's auc: 0.893588
[2674]	valid_0's auc: 0.893592
[2675]	valid_0's auc: 0.893618
[2676]	valid_0's auc: 0.893613
[2677]	valid_0's auc: 0.893612
[2678]	valid_0's auc: 0.893614
[2679]	valid_0's auc: 0.893618
[2680]	valid_0's auc: 0.893611
[2681]	valid_0's auc: 0.893616
[2682]	valid_0's auc: 0.893636
[2683]	valid_0's auc: 0.89362
[2684]	valid_0's auc: 0.893629
[2685]	valid_0's auc: 0.89363
[2686]	valid_0's auc: 0.893636
[2687]	valid_0's auc: 0.893645
[2688]	valid_0's auc: 0.893657
[2689]	valid_0's auc: 0.893668
[2690]	valid_0's auc: 0.893672
[2691]	valid_0's auc: 0.89369
[2692]	valid_0's auc: 0.893685
[2693]	valid_0's auc: 0.893688
[2694]	valid_0's auc: 0.893696
[2695]	valid_0's auc: 0.893707
[2696]	valid_0's auc: 0.893705
[2697]	valid_0's auc: 0.893698
[2698]	valid_0's auc: 0.8937
[2699]	valid_0's auc: 0.893708
[2700]	valid_0's auc: 0.89372
[2701]	valid_0's auc: 0.893722
[2702]	valid_0's auc: 0.893737
[2703]	valid_0's auc: 0.893744
[2704]	valid_0's auc: 0.893751
[2705]	valid_0's auc: 0.893759
[2706]	valid_0's auc: 0.893765
[2707]	valid_0's auc: 0.893764
[2708]	valid_0's auc: 0.893767
[2709]	valid_0's auc: 0.893776
[2710]	valid_0's auc: 0.893781
[2711]	valid_0's auc: 0.893785
[2712]	valid_0's auc: 0.893791
[2713]	valid_0's auc: 0.893802
[2714]	valid_0's auc: 0.893802
[2715]	valid_0's auc: 0.893821
[2716]	valid_0's auc: 0.89383
[2717]	valid_0's auc: 0.89384
[2718]	valid_0's auc: 0.893849
[2719]	valid_0's auc: 0.893863
[2720]	valid_0's auc: 0.893859
[2721]	valid_0's auc: 0.893878
[2722]	valid_0's auc: 0.893883
[2723]	valid_0's auc: 0.893872
[2724]	valid_0's auc: 0.893868
[2725]	valid_0's auc: 0.893873
[2726]	valid_0's auc: 0.89388
[2727]	valid_0's auc: 0.893886
[2728]	valid_0's auc: 0.893911
[2729]	valid_0's auc: 0.893923
[2730]	valid_0's auc: 0.89393
[2731]	valid_0's auc: 0.893938
[2732]	valid_0's auc: 0.89394
[2733]	valid_0's auc: 0.893955
[2734]	valid_0's auc: 0.893959
[2735]	valid_0's auc: 0.893966
[2736]	valid_0's auc: 0.893977
[2737]	valid_0's auc: 0.893984
[2738]	valid_0's auc: 0.893981
[2739]	valid_0's auc: 0.893981
[2740]	valid_0's auc: 0.893971
[2741]	valid_0's auc: 0.893981
[2742]	valid_0's auc: 0.893987
[2743]	valid_0's auc: 0.894002
[2744]	valid_0's auc: 0.893998
[2745]	valid_0's auc: 0.893996
[2746]	valid_0's auc: 0.893998
[2747]	valid_0's auc: 0.894016
[2748]	valid_0's auc: 0.894022
[2749]	valid_0's auc: 0.894027
[2750]	valid_0's auc: 0.894035
[2751]	valid_0's auc: 0.89404
[2752]	valid_0's auc: 0.894043
[2753]	valid_0's auc: 0.894055
[2754]	valid_0's auc: 0.894067
[2755]	valid_0's auc: 0.894082
[2756]	valid_0's auc: 0.894072
[2757]	valid_0's auc: 0.894082
[2758]	valid_0's auc: 0.894101
[2759]	valid_0's auc: 0.894111
[2760]	valid_0's auc: 0.894127
[2761]	valid_0's auc: 0.89415
[2762]	valid_0's auc: 0.894173
[2763]	valid_0's auc: 0.894166
[2764]	valid_0's auc: 0.894178
[2765]	valid_0's auc: 0.89418
[2766]	valid_0's auc: 0.894171
[2767]	valid_0's auc: 0.894183
[2768]	valid_0's auc: 0.894184
[2769]	valid_0's auc: 0.894191
[2770]	valid_0's auc: 0.894185
[2771]	valid_0's auc: 0.894208
[2772]	valid_0's auc: 0.894202
[2773]	valid_0's auc: 0.894209
[2774]	valid_0's auc: 0.894211
[2775]	valid_0's auc: 0.894224
[2776]	valid_0's auc: 0.89422
[2777]	valid_0's auc: 0.894222
[2778]	valid_0's auc: 0.894235
[2779]	valid_0's auc: 0.894241
[2780]	valid_0's auc: 0.894242
[2781]	valid_0's auc: 0.89426
[2782]	valid_0's auc: 0.894264
[2783]	valid_0's auc: 0.894277
[2784]	valid_0's auc: 0.894287
[2785]	valid_0's auc: 0.894281
[2786]	valid_0's auc: 0.894281
[2787]	valid_0's auc: 0.894291
[2788]	valid_0's auc: 0.894292
[2789]	valid_0's auc: 0.894287
[2790]	valid_0's auc: 0.894298
[2791]	valid_0's auc: 0.894295
[2792]	valid_0's auc: 0.894305
[2793]	valid_0's auc: 0.894321
[2794]	valid_0's auc: 0.894338
[2795]	valid_0's auc: 0.89435
[2796]	valid_0's auc: 0.894346
[2797]	valid_0's auc: 0.894352
[2798]	valid_0's auc: 0.894358
[2799]	valid_0's auc: 0.894347
[2800]	valid_0's auc: 0.894348
[2801]	valid_0's auc: 0.894353
[2802]	valid_0's auc: 0.894361
[2803]	valid_0's auc: 0.89437
[2804]	valid_0's auc: 0.89438
[2805]	valid_0's auc: 0.894391
[2806]	valid_0's auc: 0.894397
[2807]	valid_0's auc: 0.89441
[2808]	valid_0's auc: 0.894428
[2809]	valid_0's auc: 0.894422
[2810]	valid_0's auc: 0.894434
[2811]	valid_0's auc: 0.894447
[2812]	valid_0's auc: 0.894448
[2813]	valid_0's auc: 0.894442
[2814]	valid_0's auc: 0.894446
[2815]	valid_0's auc: 0.89444
[2816]	valid_0's auc: 0.894446
[2817]	valid_0's auc: 0.894452
[2818]	valid_0's auc: 0.894453
[2819]	valid_0's auc: 0.894462
[2820]	valid_0's auc: 0.894453
[2821]	valid_0's auc: 0.894461
[2822]	valid_0's auc: 0.89446
[2823]	valid_0's auc: 0.894468
[2824]	valid_0's auc: 0.894465
[2825]	valid_0's auc: 0.894462
[2826]	valid_0's auc: 0.894457
[2827]	valid_0's auc: 0.89447
[2828]	valid_0's auc: 0.894472
[2829]	valid_0's auc: 0.894482
[2830]	valid_0's auc: 0.894489
[2831]	valid_0's auc: 0.894483
[2832]	valid_0's auc: 0.894489
[2833]	valid_0's auc: 0.8945
[2834]	valid_0's auc: 0.894505
[2835]	valid_0's auc: 0.894517
[2836]	valid_0's auc: 0.894506
[2837]	valid_0's auc: 0.894514
[2838]	valid_0's auc: 0.894532
[2839]	valid_0's auc: 0.894533
[2840]	valid_0's auc: 0.894533
[2841]	valid_0's auc: 0.894544
[2842]	valid_0's auc: 0.894559
[2843]	valid_0's auc: 0.894575
[2844]	valid_0's auc: 0.894586
[2845]	valid_0's auc: 0.894586
[2846]	valid_0's auc: 0.894583
[2847]	valid_0's auc: 0.894583
[2848]	valid_0's auc: 0.894581
[2849]	valid_0's auc: 0.894586
[2850]	valid_0's auc: 0.894604
[2851]	valid_0's auc: 0.894616
[2852]	valid_0's auc: 0.894621
[2853]	valid_0's auc: 0.894621
[2854]	valid_0's auc: 0.894625
[2855]	valid_0's auc: 0.894612
[2856]	valid_0's auc: 0.894594
[2857]	valid_0's auc: 0.8946
[2858]	valid_0's auc: 0.894606
[2859]	valid_0's auc: 0.894611
[2860]	valid_0's auc: 0.894592
[2861]	valid_0's auc: 0.894613
[2862]	valid_0's auc: 0.894622
[2863]	valid_0's auc: 0.894632
[2864]	valid_0's auc: 0.894642
[2865]	valid_0's auc: 0.894645
[2866]	valid_0's auc: 0.894651
[2867]	valid_0's auc: 0.894654
[2868]	valid_0's auc: 0.894663
[2869]	valid_0's auc: 0.894683
[2870]	valid_0's auc: 0.894694
[2871]	valid_0's auc: 0.894696
[2872]	valid_0's auc: 0.894699
[2873]	valid_0's auc: 0.894687
[2874]	valid_0's auc: 0.894676
[2875]	valid_0's auc: 0.894682
[2876]	valid_0's auc: 0.894681
[2877]	valid_0's auc: 0.89468
[2878]	valid_0's auc: 0.894688
[2879]	valid_0's auc: 0.8947
[2880]	valid_0's auc: 0.8947
[2881]	valid_0's auc: 0.894698
[2882]	valid_0's auc: 0.894699
[2883]	valid_0's auc: 0.894717
[2884]	valid_0's auc: 0.894718
[2885]	valid_0's auc: 0.894723
[2886]	valid_0's auc: 0.894708
[2887]	valid_0's auc: 0.894705
[2888]	valid_0's auc: 0.894722
[2889]	valid_0's auc: 0.894717
[2890]	valid_0's auc: 0.894722
[2891]	valid_0's auc: 0.894737
[2892]	valid_0's auc: 0.894744
[2893]	valid_0's auc: 0.894749
[2894]	valid_0's auc: 0.894763
[2895]	valid_0's auc: 0.894772
[2896]	valid_0's auc: 0.894783
[2897]	valid_0's auc: 0.894791
[2898]	valid_0's auc: 0.894794
[2899]	valid_0's auc: 0.894795
[2900]	valid_0's auc: 0.894804
[2901]	valid_0's auc: 0.894821
[2902]	valid_0's auc: 0.894828
[2903]	valid_0's auc: 0.894821
[2904]	valid_0's auc: 0.894834
[2905]	valid_0's auc: 0.894834
[2906]	valid_0's auc: 0.894833
[2907]	valid_0's auc: 0.894835
[2908]	valid_0's auc: 0.894839
[2909]	valid_0's auc: 0.894835
[2910]	valid_0's auc: 0.894829
[2911]	valid_0's auc: 0.894827
[2912]	valid_0's auc: 0.894836
[2913]	valid_0's auc: 0.89484
[2914]	valid_0's auc: 0.894836
[2915]	valid_0's auc: 0.894843
[2916]	valid_0's auc: 0.894862
[2917]	valid_0's auc: 0.894884
[2918]	valid_0's auc: 0.894885
[2919]	valid_0's auc: 0.894892
[2920]	valid_0's auc: 0.894909
[2921]	valid_0's auc: 0.894918
[2922]	valid_0's auc: 0.894934
[2923]	valid_0's auc: 0.894938
[2924]	valid_0's auc: 0.894942
[2925]	valid_0's auc: 0.894945
[2926]	valid_0's auc: 0.894949
[2927]	valid_0's auc: 0.894938
[2928]	valid_0's auc: 0.894942
[2929]	valid_0's auc: 0.894961
[2930]	valid_0's auc: 0.894955
[2931]	valid_0's auc: 0.89497
[2932]	valid_0's auc: 0.894977
[2933]	valid_0's auc: 0.89498
[2934]	valid_0's auc: 0.894988
[2935]	valid_0's auc: 0.894976
[2936]	valid_0's auc: 0.894984
[2937]	valid_0's auc: 0.894994
[2938]	valid_0's auc: 0.895005
[2939]	valid_0's auc: 0.895002
[2940]	valid_0's auc: 0.895014
[2941]	valid_0's auc: 0.895027
[2942]	valid_0's auc: 0.895032
[2943]	valid_0's auc: 0.895046
[2944]	valid_0's auc: 0.895076
[2945]	valid_0's auc: 0.895096
[2946]	valid_0's auc: 0.895102
[2947]	valid_0's auc: 0.895114
[2948]	valid_0's auc: 0.895124
[2949]	valid_0's auc: 0.895133
[2950]	valid_0's auc: 0.895153
[2951]	valid_0's auc: 0.895165
[2952]	valid_0's auc: 0.895173
[2953]	valid_0's auc: 0.895174
[2954]	valid_0's auc: 0.895167
[2955]	valid_0's auc: 0.895166
[2956]	valid_0's auc: 0.895172
[2957]	valid_0's auc: 0.895153
[2958]	valid_0's auc: 0.895164
[2959]	valid_0's auc: 0.895172
[2960]	valid_0's auc: 0.895171
[2961]	valid_0's auc: 0.895183
[2962]	valid_0's auc: 0.895183
[2963]	valid_0's auc: 0.895189
[2964]	valid_0's auc: 0.895186
[2965]	valid_0's auc: 0.895182
[2966]	valid_0's auc: 0.895187
[2967]	valid_0's auc: 0.895187
[2968]	valid_0's auc: 0.895203
[2969]	valid_0's auc: 0.895209
[2970]	valid_0's auc: 0.89521
[2971]	valid_0's auc: 0.89521
[2972]	valid_0's auc: 0.895219
[2973]	valid_0's auc: 0.895229
[2974]	valid_0's auc: 0.895238
[2975]	valid_0's auc: 0.895246
[2976]	valid_0's auc: 0.895246
[2977]	valid_0's auc: 0.895244
[2978]	valid_0's auc: 0.895257
[2979]	valid_0's auc: 0.895271
[2980]	valid_0's auc: 0.895267
[2981]	valid_0's auc: 0.89528
[2982]	valid_0's auc: 0.895287
[2983]	valid_0's auc: 0.895286
[2984]	valid_0's auc: 0.895291
[2985]	valid_0's auc: 0.895288
[2986]	valid_0's auc: 0.895303
[2987]	valid_0's auc: 0.895301
[2988]	valid_0's auc: 0.895303
[2989]	valid_0's auc: 0.895301
[2990]	valid_0's auc: 0.895309
[2991]	valid_0's auc: 0.895311
[2992]	valid_0's auc: 0.895302
[2993]	valid_0's auc: 0.895304
[2994]	valid_0's auc: 0.895309
[2995]	valid_0's auc: 0.895327
[2996]	valid_0's auc: 0.895344
[2997]	valid_0's auc: 0.895354
[2998]	valid_0's auc: 0.895359
[2999]	valid_0's auc: 0.895383
[3000]	valid_0's auc: 0.895392
[3001]	valid_0's auc: 0.895394
[3002]	valid_0's auc: 0.895394
[3003]	valid_0's auc: 0.8954
[3004]	valid_0's auc: 0.895399
[3005]	valid_0's auc: 0.895405
[3006]	valid_0's auc: 0.895406
[3007]	valid_0's auc: 0.89542
[3008]	valid_0's auc: 0.895428
[3009]	valid_0's auc: 0.895431
[3010]	valid_0's auc: 0.895433
[3011]	valid_0's auc: 0.89543
[3012]	valid_0's auc: 0.895429
[3013]	valid_0's auc: 0.895434
[3014]	valid_0's auc: 0.895439
[3015]	valid_0's auc: 0.895455
[3016]	valid_0's auc: 0.895453
[3017]	valid_0's auc: 0.895451
[3018]	valid_0's auc: 0.895459
[3019]	valid_0's auc: 0.895461
[3020]	valid_0's auc: 0.895461
[3021]	valid_0's auc: 0.895464
[3022]	valid_0's auc: 0.895455
[3023]	valid_0's auc: 0.895459
[3024]	valid_0's auc: 0.895459
[3025]	valid_0's auc: 0.895464
[3026]	valid_0's auc: 0.895465
[3027]	valid_0's auc: 0.89547
[3028]	valid_0's auc: 0.895476
[3029]	valid_0's auc: 0.895485
[3030]	valid_0's auc: 0.895493
[3031]	valid_0's auc: 0.895499
[3032]	valid_0's auc: 0.895491
[3033]	valid_0's auc: 0.895486
[3034]	valid_0's auc: 0.895495
[3035]	valid_0's auc: 0.895498
[3036]	valid_0's auc: 0.895493
[3037]	valid_0's auc: 0.895505
[3038]	valid_0's auc: 0.895519
[3039]	valid_0's auc: 0.895528
[3040]	valid_0's auc: 0.895524
[3041]	valid_0's auc: 0.895518
[3042]	valid_0's auc: 0.895523
[3043]	valid_0's auc: 0.895518
[3044]	valid_0's auc: 0.895528
[3045]	valid_0's auc: 0.895529
[3046]	valid_0's auc: 0.895527
[3047]	valid_0's auc: 0.895524
[3048]	valid_0's auc: 0.895528
[3049]	valid_0's auc: 0.895534
[3050]	valid_0's auc: 0.895539
[3051]	valid_0's auc: 0.895538
[3052]	valid_0's auc: 0.895541
[3053]	valid_0's auc: 0.895544
[3054]	valid_0's auc: 0.89554
[3055]	valid_0's auc: 0.895536
[3056]	valid_0's auc: 0.895541
[3057]	valid_0's auc: 0.895558
[3058]	valid_0's auc: 0.89557
[3059]	valid_0's auc: 0.895563
[3060]	valid_0's auc: 0.895583
[3061]	valid_0's auc: 0.895598
[3062]	valid_0's auc: 0.895603
[3063]	valid_0's auc: 0.8956
[3064]	valid_0's auc: 0.895622
[3065]	valid_0's auc: 0.895644
[3066]	valid_0's auc: 0.895657
[3067]	valid_0's auc: 0.895664
[3068]	valid_0's auc: 0.895675
[3069]	valid_0's auc: 0.895675
[3070]	valid_0's auc: 0.895676
[3071]	valid_0's auc: 0.895687
[3072]	valid_0's auc: 0.89569
[3073]	valid_0's auc: 0.895693
[3074]	valid_0's auc: 0.895694
[3075]	valid_0's auc: 0.895691
[3076]	valid_0's auc: 0.8957
[3077]	valid_0's auc: 0.895703
[3078]	valid_0's auc: 0.895714
[3079]	valid_0's auc: 0.89572
[3080]	valid_0's auc: 0.895731
[3081]	valid_0's auc: 0.895738
[3082]	valid_0's auc: 0.895738
[3083]	valid_0's auc: 0.895741
[3084]	valid_0's auc: 0.895731
[3085]	valid_0's auc: 0.895741
[3086]	valid_0's auc: 0.895744
[3087]	valid_0's auc: 0.895752
[3088]	valid_0's auc: 0.895749
[3089]	valid_0's auc: 0.895764
[3090]	valid_0's auc: 0.895776
[3091]	valid_0's auc: 0.895783
[3092]	valid_0's auc: 0.895791
[3093]	valid_0's auc: 0.89579
[3094]	valid_0's auc: 0.895799
[3095]	valid_0's auc: 0.895803
[3096]	valid_0's auc: 0.895798
[3097]	valid_0's auc: 0.895804
[3098]	valid_0's auc: 0.895802
[3099]	valid_0's auc: 0.895796
[3100]	valid_0's auc: 0.895801
[3101]	valid_0's auc: 0.895801
[3102]	valid_0's auc: 0.895806
[3103]	valid_0's auc: 0.895802
[3104]	valid_0's auc: 0.8958
[3105]	valid_0's auc: 0.895819
[3106]	valid_0's auc: 0.895827
[3107]	valid_0's auc: 0.895845
[3108]	valid_0's auc: 0.895851
[3109]	valid_0's auc: 0.895849
[3110]	valid_0's auc: 0.895854
[3111]	valid_0's auc: 0.895857
[3112]	valid_0's auc: 0.895866
[3113]	valid_0's auc: 0.895874
[3114]	valid_0's auc: 0.895867
[3115]	valid_0's auc: 0.895882
[3116]	valid_0's auc: 0.895893
[3117]	valid_0's auc: 0.895895
[3118]	valid_0's auc: 0.895904
[3119]	valid_0's auc: 0.89592
[3120]	valid_0's auc: 0.895911
[3121]	valid_0's auc: 0.895912
[3122]	valid_0's auc: 0.895901
[3123]	valid_0's auc: 0.895899
[3124]	valid_0's auc: 0.895897
[3125]	valid_0's auc: 0.895896
[3126]	valid_0's auc: 0.895915
[3127]	valid_0's auc: 0.89593
[3128]	valid_0's auc: 0.895936
[3129]	valid_0's auc: 0.89595
[3130]	valid_0's auc: 0.895963
[3131]	valid_0's auc: 0.895972
[3132]	valid_0's auc: 0.89597
[3133]	valid_0's auc: 0.895979
[3134]	valid_0's auc: 0.895995
[3135]	valid_0's auc: 0.896
[3136]	valid_0's auc: 0.896029
[3137]	valid_0's auc: 0.896038
[3138]	valid_0's auc: 0.896035
[3139]	valid_0's auc: 0.896037
[3140]	valid_0's auc: 0.896043
[3141]	valid_0's auc: 0.89606
[3142]	valid_0's auc: 0.896078
[3143]	valid_0's auc: 0.89609
[3144]	valid_0's auc: 0.896087
[3145]	valid_0's auc: 0.896095
[3146]	valid_0's auc: 0.896108
[3147]	valid_0's auc: 0.896103
[3148]	valid_0's auc: 0.896095
[3149]	valid_0's auc: 0.896084
[3150]	valid_0's auc: 0.896077
[3151]	valid_0's auc: 0.896075
[3152]	valid_0's auc: 0.896076
[3153]	valid_0's auc: 0.896072
[3154]	valid_0's auc: 0.896083
[3155]	valid_0's auc: 0.896083
[3156]	valid_0's auc: 0.896076
[3157]	valid_0's auc: 0.896082
[3158]	valid_0's auc: 0.896089
[3159]	valid_0's auc: 0.896099
[3160]	valid_0's auc: 0.896113
[3161]	valid_0's auc: 0.896114
[3162]	valid_0's auc: 0.896116
[3163]	valid_0's auc: 0.89613
[3164]	valid_0's auc: 0.896135
[3165]	valid_0's auc: 0.896139
[3166]	valid_0's auc: 0.896149
[3167]	valid_0's auc: 0.896141
[3168]	valid_0's auc: 0.896158
[3169]	valid_0's auc: 0.896169
[3170]	valid_0's auc: 0.89619
[3171]	valid_0's auc: 0.896189
[3172]	valid_0's auc: 0.896191
[3173]	valid_0's auc: 0.896195
[3174]	valid_0's auc: 0.896189
[3175]	valid_0's auc: 0.896182
[3176]	valid_0's auc: 0.89617
[3177]	valid_0's auc: 0.896169
[3178]	valid_0's auc: 0.896179
[3179]	valid_0's auc: 0.896178
[3180]	valid_0's auc: 0.896186
[3181]	valid_0's auc: 0.896186
[3182]	valid_0's auc: 0.8962
[3183]	valid_0's auc: 0.896215
[3184]	valid_0's auc: 0.896216
[3185]	valid_0's auc: 0.896233
[3186]	valid_0's auc: 0.896229
[3187]	valid_0's auc: 0.896228
[3188]	valid_0's auc: 0.896242
[3189]	valid_0's auc: 0.896255
[3190]	valid_0's auc: 0.896254
[3191]	valid_0's auc: 0.896259
[3192]	valid_0's auc: 0.896259
[3193]	valid_0's auc: 0.896264
[3194]	valid_0's auc: 0.896262
[3195]	valid_0's auc: 0.896268
[3196]	valid_0's auc: 0.896276
[3197]	valid_0's auc: 0.896269
[3198]	valid_0's auc: 0.89628
[3199]	valid_0's auc: 0.896278
[3200]	valid_0's auc: 0.896276
[3201]	valid_0's auc: 0.896282
[3202]	valid_0's auc: 0.896286
[3203]	valid_0's auc: 0.89629
[3204]	valid_0's auc: 0.8963
[3205]	valid_0's auc: 0.896307
[3206]	valid_0's auc: 0.89631
[3207]	valid_0's auc: 0.896314
[3208]	valid_0's auc: 0.896321
[3209]	valid_0's auc: 0.896327
[3210]	valid_0's auc: 0.896333
[3211]	valid_0's auc: 0.896346
[3212]	valid_0's auc: 0.896355
[3213]	valid_0's auc: 0.89636
[3214]	valid_0's auc: 0.896368
[3215]	valid_0's auc: 0.896372
[3216]	valid_0's auc: 0.896371
[3217]	valid_0's auc: 0.896385
[3218]	valid_0's auc: 0.89639
[3219]	valid_0's auc: 0.896389
[3220]	valid_0's auc: 0.896401
[3221]	valid_0's auc: 0.896399
[3222]	valid_0's auc: 0.896413
[3223]	valid_0's auc: 0.896409
[3224]	valid_0's auc: 0.896395
[3225]	valid_0's auc: 0.896411
[3226]	valid_0's auc: 0.896411
[3227]	valid_0's auc: 0.896422
[3228]	valid_0's auc: 0.896431
[3229]	valid_0's auc: 0.896435
[3230]	valid_0's auc: 0.896447
[3231]	valid_0's auc: 0.896453
[3232]	valid_0's auc: 0.896448
[3233]	valid_0's auc: 0.896454
[3234]	valid_0's auc: 0.896465
[3235]	valid_0's auc: 0.896465
[3236]	valid_0's auc: 0.896456
[3237]	valid_0's auc: 0.896458
[3238]	valid_0's auc: 0.896481
[3239]	valid_0's auc: 0.896477
[3240]	valid_0's auc: 0.896495
[3241]	valid_0's auc: 0.896506
[3242]	valid_0's auc: 0.896516
[3243]	valid_0's auc: 0.896511
[3244]	valid_0's auc: 0.896509
[3245]	valid_0's auc: 0.8965
[3246]	valid_0's auc: 0.896516
[3247]	valid_0's auc: 0.896519
[3248]	valid_0's auc: 0.896525
[3249]	valid_0's auc: 0.896534
[3250]	valid_0's auc: 0.896546
[3251]	valid_0's auc: 0.896551
[3252]	valid_0's auc: 0.896553
[3253]	valid_0's auc: 0.896564
[3254]	valid_0's auc: 0.896568
[3255]	valid_0's auc: 0.896564
[3256]	valid_0's auc: 0.896563
[3257]	valid_0's auc: 0.896564
[3258]	valid_0's auc: 0.896568
[3259]	valid_0's auc: 0.89657
[3260]	valid_0's auc: 0.896576
[3261]	valid_0's auc: 0.896584
[3262]	valid_0's auc: 0.89658
[3263]	valid_0's auc: 0.896585
[3264]	valid_0's auc: 0.896586
[3265]	valid_0's auc: 0.896588
[3266]	valid_0's auc: 0.896592
[3267]	valid_0's auc: 0.896589
[3268]	valid_0's auc: 0.896597
[3269]	valid_0's auc: 0.896594
[3270]	valid_0's auc: 0.896596
[3271]	valid_0's auc: 0.896602
[3272]	valid_0's auc: 0.896601
[3273]	valid_0's auc: 0.896608
[3274]	valid_0's auc: 0.896613
[3275]	valid_0's auc: 0.896609
[3276]	valid_0's auc: 0.896618
[3277]	valid_0's auc: 0.896618
[3278]	valid_0's auc: 0.896613
[3279]	valid_0's auc: 0.896618
[3280]	valid_0's auc: 0.896623
[3281]	valid_0's auc: 0.896615
[3282]	valid_0's auc: 0.896604
[3283]	valid_0's auc: 0.896621
[3284]	valid_0's auc: 0.896626
[3285]	valid_0's auc: 0.896629
[3286]	valid_0's auc: 0.896636
[3287]	valid_0's auc: 0.896639
[3288]	valid_0's auc: 0.896651
[3289]	valid_0's auc: 0.896662
[3290]	valid_0's auc: 0.896664
[3291]	valid_0's auc: 0.896675
[3292]	valid_0's auc: 0.896668
[3293]	valid_0's auc: 0.896676
[3294]	valid_0's auc: 0.896677
[3295]	valid_0's auc: 0.896672
[3296]	valid_0's auc: 0.896682
[3297]	valid_0's auc: 0.896682
[3298]	valid_0's auc: 0.896672
[3299]	valid_0's auc: 0.896682
[3300]	valid_0's auc: 0.896678
[3301]	valid_0's auc: 0.896674
[3302]	valid_0's auc: 0.896689
[3303]	valid_0's auc: 0.896706
[3304]	valid_0's auc: 0.896709
[3305]	valid_0's auc: 0.896708
[3306]	valid_0's auc: 0.896722
[3307]	valid_0's auc: 0.896719
[3308]	valid_0's auc: 0.896724
[3309]	valid_0's auc: 0.896714
[3310]	valid_0's auc: 0.89671
[3311]	valid_0's auc: 0.896711
[3312]	valid_0's auc: 0.896713
[3313]	valid_0's auc: 0.896732
[3314]	valid_0's auc: 0.896747
[3315]	valid_0's auc: 0.896753
[3316]	valid_0's auc: 0.896752
[3317]	valid_0's auc: 0.896753
[3318]	valid_0's auc: 0.896756
[3319]	valid_0's auc: 0.896753
[3320]	valid_0's auc: 0.896757
[3321]	valid_0's auc: 0.896759
[3322]	valid_0's auc: 0.896767
[3323]	valid_0's auc: 0.896766
[3324]	valid_0's auc: 0.896773
[3325]	valid_0's auc: 0.89678
[3326]	valid_0's auc: 0.896782
[3327]	valid_0's auc: 0.896795
[3328]	valid_0's auc: 0.896807
[3329]	valid_0's auc: 0.896809
[3330]	valid_0's auc: 0.896809
[3331]	valid_0's auc: 0.896801
[3332]	valid_0's auc: 0.896801
[3333]	valid_0's auc: 0.896804
[3334]	valid_0's auc: 0.896818
[3335]	valid_0's auc: 0.896812
[3336]	valid_0's auc: 0.896821
[3337]	valid_0's auc: 0.896825
[3338]	valid_0's auc: 0.89683
[3339]	valid_0's auc: 0.896836
[3340]	valid_0's auc: 0.896835
[3341]	valid_0's auc: 0.896836
[3342]	valid_0's auc: 0.896833
[3343]	valid_0's auc: 0.896838
[3344]	valid_0's auc: 0.896849
[3345]	valid_0's auc: 0.896853
[3346]	valid_0's auc: 0.896852
[3347]	valid_0's auc: 0.896848
[3348]	valid_0's auc: 0.896853
[3349]	valid_0's auc: 0.896849
[3350]	valid_0's auc: 0.896846
[3351]	valid_0's auc: 0.896854
[3352]	valid_0's auc: 0.896852
[3353]	valid_0's auc: 0.896858
[3354]	valid_0's auc: 0.89686
[3355]	valid_0's auc: 0.896862
[3356]	valid_0's auc: 0.896867
[3357]	valid_0's auc: 0.896869
[3358]	valid_0's auc: 0.896875
[3359]	valid_0's auc: 0.896869
[3360]	valid_0's auc: 0.896875
[3361]	valid_0's auc: 0.896876
[3362]	valid_0's auc: 0.89687
[3363]	valid_0's auc: 0.896883
[3364]	valid_0's auc: 0.896881
[3365]	valid_0's auc: 0.896875
[3366]	valid_0's auc: 0.896891
[3367]	valid_0's auc: 0.896877
[3368]	valid_0's auc: 0.896886
[3369]	valid_0's auc: 0.896884
[3370]	valid_0's auc: 0.896875
[3371]	valid_0's auc: 0.896888
[3372]	valid_0's auc: 0.896883
[3373]	valid_0's auc: 0.896896
[3374]	valid_0's auc: 0.896888
[3375]	valid_0's auc: 0.896892
[3376]	valid_0's auc: 0.896898
[3377]	valid_0's auc: 0.896892
[3378]	valid_0's auc: 0.896897
[3379]	valid_0's auc: 0.896911
[3380]	valid_0's auc: 0.896931
[3381]	valid_0's auc: 0.896936
[3382]	valid_0's auc: 0.896937
[3383]	valid_0's auc: 0.896941
[3384]	valid_0's auc: 0.896944
[3385]	valid_0's auc: 0.896947
[3386]	valid_0's auc: 0.896961
[3387]	valid_0's auc: 0.896956
[3388]	valid_0's auc: 0.896961
[3389]	valid_0's auc: 0.896975
[3390]	valid_0's auc: 0.896978
[3391]	valid_0's auc: 0.896975
[3392]	valid_0's auc: 0.896975
[3393]	valid_0's auc: 0.896988
[3394]	valid_0's auc: 0.896993
[3395]	valid_0's auc: 0.897006
[3396]	valid_0's auc: 0.897007
[3397]	valid_0's auc: 0.897014
[3398]	valid_0's auc: 0.897012
[3399]	valid_0's auc: 0.897019
[3400]	valid_0's auc: 0.897027
[3401]	valid_0's auc: 0.897026
[3402]	valid_0's auc: 0.897032
[3403]	valid_0's auc: 0.89702
[3404]	valid_0's auc: 0.89703
[3405]	valid_0's auc: 0.897031
[3406]	valid_0's auc: 0.89703
[3407]	valid_0's auc: 0.897035
[3408]	valid_0's auc: 0.897036
[3409]	valid_0's auc: 0.897045
[3410]	valid_0's auc: 0.897053
[3411]	valid_0's auc: 0.897052
[3412]	valid_0's auc: 0.897063
[3413]	valid_0's auc: 0.897069
[3414]	valid_0's auc: 0.89708
[3415]	valid_0's auc: 0.897086
[3416]	valid_0's auc: 0.897084
[3417]	valid_0's auc: 0.897085
[3418]	valid_0's auc: 0.89709
[3419]	valid_0's auc: 0.897085
[3420]	valid_0's auc: 0.897095
[3421]	valid_0's auc: 0.897106
[3422]	valid_0's auc: 0.897108
[3423]	valid_0's auc: 0.897109
[3424]	valid_0's auc: 0.897104
[3425]	valid_0's auc: 0.897109
[3426]	valid_0's auc: 0.897102
[3427]	valid_0's auc: 0.89711
[3428]	valid_0's auc: 0.897109
[3429]	valid_0's auc: 0.897103
[3430]	valid_0's auc: 0.897116
[3431]	valid_0's auc: 0.897124
[3432]	valid_0's auc: 0.897128
[3433]	valid_0's auc: 0.897118
[3434]	valid_0's auc: 0.897125
[3435]	valid_0's auc: 0.897142
[3436]	valid_0's auc: 0.897146
[3437]	valid_0's auc: 0.897156
[3438]	valid_0's auc: 0.897167
[3439]	valid_0's auc: 0.897184
[3440]	valid_0's auc: 0.897185
[3441]	valid_0's auc: 0.897188
[3442]	valid_0's auc: 0.897194
[3443]	valid_0's auc: 0.897204
[3444]	valid_0's auc: 0.897202
[3445]	valid_0's auc: 0.897194
[3446]	valid_0's auc: 0.897196
[3447]	valid_0's auc: 0.897209
[3448]	valid_0's auc: 0.897217
[3449]	valid_0's auc: 0.897233
[3450]	valid_0's auc: 0.897235
[3451]	valid_0's auc: 0.897242
[3452]	valid_0's auc: 0.897252
[3453]	valid_0's auc: 0.897257
[3454]	valid_0's auc: 0.89726
[3455]	valid_0's auc: 0.897262
[3456]	valid_0's auc: 0.897253
[3457]	valid_0's auc: 0.897251
[3458]	valid_0's auc: 0.897259
[3459]	valid_0's auc: 0.897256
[3460]	valid_0's auc: 0.897254
[3461]	valid_0's auc: 0.897255
[3462]	valid_0's auc: 0.897276
[3463]	valid_0's auc: 0.897284
[3464]	valid_0's auc: 0.897296
[3465]	valid_0's auc: 0.897298
[3466]	valid_0's auc: 0.897292
[3467]	valid_0's auc: 0.897293
[3468]	valid_0's auc: 0.897298
[3469]	valid_0's auc: 0.897295
[3470]	valid_0's auc: 0.897302
[3471]	valid_0's auc: 0.897306
[3472]	valid_0's auc: 0.897307
[3473]	valid_0's auc: 0.897314
[3474]	valid_0's auc: 0.897311
[3475]	valid_0's auc: 0.897306
[3476]	valid_0's auc: 0.897306
[3477]	valid_0's auc: 0.897314
[3478]	valid_0's auc: 0.897314
[3479]	valid_0's auc: 0.897324
[3480]	valid_0's auc: 0.897315
[3481]	valid_0's auc: 0.897316
[3482]	valid_0's auc: 0.897313
[3483]	valid_0's auc: 0.897302
[3484]	valid_0's auc: 0.897307
[3485]	valid_0's auc: 0.897317
[3486]	valid_0's auc: 0.897322
[3487]	valid_0's auc: 0.897336
[3488]	valid_0's auc: 0.897332
[3489]	valid_0's auc: 0.897336
[3490]	valid_0's auc: 0.897338
[3491]	valid_0's auc: 0.897361
[3492]	valid_0's auc: 0.89736
[3493]	valid_0's auc: 0.897363
[3494]	valid_0's auc: 0.897355
[3495]	valid_0's auc: 0.897354
[3496]	valid_0's auc: 0.897347
[3497]	valid_0's auc: 0.897355
[3498]	valid_0's auc: 0.897364
[3499]	valid_0's auc: 0.89736
[3500]	valid_0's auc: 0.897366
[3501]	valid_0's auc: 0.897373
[3502]	valid_0's auc: 0.897389
[3503]	valid_0's auc: 0.897386
[3504]	valid_0's auc: 0.897384
[3505]	valid_0's auc: 0.897393
[3506]	valid_0's auc: 0.897397
[3507]	valid_0's auc: 0.8974
[3508]	valid_0's auc: 0.897404
[3509]	valid_0's auc: 0.897407
[3510]	valid_0's auc: 0.897409
[3511]	valid_0's auc: 0.897409
[3512]	valid_0's auc: 0.89742
[3513]	valid_0's auc: 0.897431
[3514]	valid_0's auc: 0.89744
[3515]	valid_0's auc: 0.897442
[3516]	valid_0's auc: 0.897438
[3517]	valid_0's auc: 0.89744
[3518]	valid_0's auc: 0.897452
[3519]	valid_0's auc: 0.897455
[3520]	valid_0's auc: 0.89747
[3521]	valid_0's auc: 0.897476
[3522]	valid_0's auc: 0.89748
[3523]	valid_0's auc: 0.89748
[3524]	valid_0's auc: 0.897486
[3525]	valid_0's auc: 0.897493
[3526]	valid_0's auc: 0.897504
[3527]	valid_0's auc: 0.897508
[3528]	valid_0's auc: 0.897513
[3529]	valid_0's auc: 0.897519
[3530]	valid_0's auc: 0.897523
[3531]	valid_0's auc: 0.897534
[3532]	valid_0's auc: 0.897538
[3533]	valid_0's auc: 0.897549
[3534]	valid_0's auc: 0.897552
[3535]	valid_0's auc: 0.897547
[3536]	valid_0's auc: 0.897547
[3537]	valid_0's auc: 0.897544
[3538]	valid_0's auc: 0.897535
[3539]	valid_0's auc: 0.897531
[3540]	valid_0's auc: 0.89754
[3541]	valid_0's auc: 0.897538
[3542]	valid_0's auc: 0.897539
[3543]	valid_0's auc: 0.897527
[3544]	valid_0's auc: 0.897529
[3545]	valid_0's auc: 0.897529
[3546]	valid_0's auc: 0.897549
[3547]	valid_0's auc: 0.897559
[3548]	valid_0's auc: 0.897576
[3549]	valid_0's auc: 0.897589
[3550]	valid_0's auc: 0.897585
[3551]	valid_0's auc: 0.897585
[3552]	valid_0's auc: 0.8976
[3553]	valid_0's auc: 0.897609
[3554]	valid_0's auc: 0.897603
[3555]	valid_0's auc: 0.897602
[3556]	valid_0's auc: 0.897605
[3557]	valid_0's auc: 0.89761
[3558]	valid_0's auc: 0.897604
[3559]	valid_0's auc: 0.897613
[3560]	valid_0's auc: 0.897621
[3561]	valid_0's auc: 0.897616
[3562]	valid_0's auc: 0.897621
[3563]	valid_0's auc: 0.897623
[3564]	valid_0's auc: 0.897627
[3565]	valid_0's auc: 0.897627
[3566]	valid_0's auc: 0.897631
[3567]	valid_0's auc: 0.897635
[3568]	valid_0's auc: 0.897641
[3569]	valid_0's auc: 0.897646
[3570]	valid_0's auc: 0.897651
[3571]	valid_0's auc: 0.897651
[3572]	valid_0's auc: 0.897644
[3573]	valid_0's auc: 0.897649
[3574]	valid_0's auc: 0.897653
[3575]	valid_0's auc: 0.897662
[3576]	valid_0's auc: 0.897665
[3577]	valid_0's auc: 0.897666
[3578]	valid_0's auc: 0.897675
[3579]	valid_0's auc: 0.897673
[3580]	valid_0's auc: 0.89767
[3581]	valid_0's auc: 0.897668
[3582]	valid_0's auc: 0.897676
[3583]	valid_0's auc: 0.897673
[3584]	valid_0's auc: 0.897683
[3585]	valid_0's auc: 0.897684
[3586]	valid_0's auc: 0.897676
[3587]	valid_0's auc: 0.897685
[3588]	valid_0's auc: 0.897689
[3589]	valid_0's auc: 0.8977
[3590]	valid_0's auc: 0.897698
[3591]	valid_0's auc: 0.897703
[3592]	valid_0's auc: 0.897711
[3593]	valid_0's auc: 0.897705
[3594]	valid_0's auc: 0.897724
[3595]	valid_0's auc: 0.897734
[3596]	valid_0's auc: 0.897743
[3597]	valid_0's auc: 0.89774
[3598]	valid_0's auc: 0.897756
[3599]	valid_0's auc: 0.897759
[3600]	valid_0's auc: 0.897772
[3601]	valid_0's auc: 0.897778
[3602]	valid_0's auc: 0.897772
[3603]	valid_0's auc: 0.89777
[3604]	valid_0's auc: 0.897763
[3605]	valid_0's auc: 0.897782
[3606]	valid_0's auc: 0.897786
[3607]	valid_0's auc: 0.897784
[3608]	valid_0's auc: 0.897788
[3609]	valid_0's auc: 0.89779
[3610]	valid_0's auc: 0.897792
[3611]	valid_0's auc: 0.897793
[3612]	valid_0's auc: 0.8978
[3613]	valid_0's auc: 0.897808
[3614]	valid_0's auc: 0.897825
[3615]	valid_0's auc: 0.897827
[3616]	valid_0's auc: 0.897829
[3617]	valid_0's auc: 0.897828
[3618]	valid_0's auc: 0.897831
[3619]	valid_0's auc: 0.897839
[3620]	valid_0's auc: 0.897838
[3621]	valid_0's auc: 0.897836
[3622]	valid_0's auc: 0.897838
[3623]	valid_0's auc: 0.897842
[3624]	valid_0's auc: 0.897856
[3625]	valid_0's auc: 0.897852
[3626]	valid_0's auc: 0.897854
[3627]	valid_0's auc: 0.897852
[3628]	valid_0's auc: 0.897863
[3629]	valid_0's auc: 0.89786
[3630]	valid_0's auc: 0.897868
[3631]	valid_0's auc: 0.897868
[3632]	valid_0's auc: 0.897874
[3633]	valid_0's auc: 0.897883
[3634]	valid_0's auc: 0.897883
[3635]	valid_0's auc: 0.897891
[3636]	valid_0's auc: 0.897902
[3637]	valid_0's auc: 0.897918
[3638]	valid_0's auc: 0.897925
[3639]	valid_0's auc: 0.897927
[3640]	valid_0's auc: 0.897936
[3641]	valid_0's auc: 0.897942
[3642]	valid_0's auc: 0.897954
[3643]	valid_0's auc: 0.897945
[3644]	valid_0's auc: 0.897948
[3645]	valid_0's auc: 0.897948
[3646]	valid_0's auc: 0.89796
[3647]	valid_0's auc: 0.897964
[3648]	valid_0's auc: 0.897967
[3649]	valid_0's auc: 0.897975
[3650]	valid_0's auc: 0.897978
[3651]	valid_0's auc: 0.897975
[3652]	valid_0's auc: 0.897982
[3653]	valid_0's auc: 0.897978
[3654]	valid_0's auc: 0.89798
[3655]	valid_0's auc: 0.897977
[3656]	valid_0's auc: 0.897969
[3657]	valid_0's auc: 0.897973
[3658]	valid_0's auc: 0.897976
[3659]	valid_0's auc: 0.897974
[3660]	valid_0's auc: 0.897971
[3661]	valid_0's auc: 0.897972
[3662]	valid_0's auc: 0.89798
[3663]	valid_0's auc: 0.897978
[3664]	valid_0's auc: 0.89798
[3665]	valid_0's auc: 0.897981
[3666]	valid_0's auc: 0.897975
[3667]	valid_0's auc: 0.897969
[3668]	valid_0's auc: 0.897967
[3669]	valid_0's auc: 0.897967
[3670]	valid_0's auc: 0.897961
[3671]	valid_0's auc: 0.897967
[3672]	valid_0's auc: 0.897979
[3673]	valid_0's auc: 0.897977
[3674]	valid_0's auc: 0.897973
[3675]	valid_0's auc: 0.897977
[3676]	valid_0's auc: 0.897981
[3677]	valid_0's auc: 0.897982
[3678]	valid_0's auc: 0.897998
[3679]	valid_0's auc: 0.897995
[3680]	valid_0's auc: 0.897996
[3681]	valid_0's auc: 0.897998
[3682]	valid_0's auc: 0.897993
[3683]	valid_0's auc: 0.897993
[3684]	valid_0's auc: 0.897995
[3685]	valid_0's auc: 0.897994
[3686]	valid_0's auc: 0.897993
[3687]	valid_0's auc: 0.897988
[3688]	valid_0's auc: 0.897988
[3689]	valid_0's auc: 0.897991
[3690]	valid_0's auc: 0.898006
[3691]	valid_0's auc: 0.898013
[3692]	valid_0's auc: 0.898025
[3693]	valid_0's auc: 0.898026
[3694]	valid_0's auc: 0.898033
[3695]	valid_0's auc: 0.898036
[3696]	valid_0's auc: 0.898035
[3697]	valid_0's auc: 0.89804
[3698]	valid_0's auc: 0.898047
[3699]	valid_0's auc: 0.89805
[3700]	valid_0's auc: 0.89805
[3701]	valid_0's auc: 0.898055
[3702]	valid_0's auc: 0.89806
[3703]	valid_0's auc: 0.898072
[3704]	valid_0's auc: 0.898067
[3705]	valid_0's auc: 0.898068
[3706]	valid_0's auc: 0.89807
[3707]	valid_0's auc: 0.898087
[3708]	valid_0's auc: 0.898087
[3709]	valid_0's auc: 0.898089
[3710]	valid_0's auc: 0.898095
[3711]	valid_0's auc: 0.898103
[3712]	valid_0's auc: 0.898097
[3713]	valid_0's auc: 0.898107
[3714]	valid_0's auc: 0.89811
[3715]	valid_0's auc: 0.898109
[3716]	valid_0's auc: 0.898117
[3717]	valid_0's auc: 0.898125
[3718]	valid_0's auc: 0.898132
[3719]	valid_0's auc: 0.898142
[3720]	valid_0's auc: 0.89815
[3721]	valid_0's auc: 0.898145
[3722]	valid_0's auc: 0.898137
[3723]	valid_0's auc: 0.898141
[3724]	valid_0's auc: 0.898147
[3725]	valid_0's auc: 0.898148
[3726]	valid_0's auc: 0.898157
[3727]	valid_0's auc: 0.898164
[3728]	valid_0's auc: 0.898161
[3729]	valid_0's auc: 0.898166
[3730]	valid_0's auc: 0.898163
[3731]	valid_0's auc: 0.898171
[3732]	valid_0's auc: 0.898172
[3733]	valid_0's auc: 0.898176
[3734]	valid_0's auc: 0.898184
[3735]	valid_0's auc: 0.89819
[3736]	valid_0's auc: 0.898198
[3737]	valid_0's auc: 0.898202
[3738]	valid_0's auc: 0.898204
[3739]	valid_0's auc: 0.8982
[3740]	valid_0's auc: 0.898203
[3741]	valid_0's auc: 0.89821
[3742]	valid_0's auc: 0.898213
[3743]	valid_0's auc: 0.898217
[3744]	valid_0's auc: 0.898221
[3745]	valid_0's auc: 0.898227
[3746]	valid_0's auc: 0.898231
[3747]	valid_0's auc: 0.898233
[3748]	valid_0's auc: 0.898242
[3749]	valid_0's auc: 0.898253
[3750]	valid_0's auc: 0.898259
[3751]	valid_0's auc: 0.898252
[3752]	valid_0's auc: 0.89825
[3753]	valid_0's auc: 0.898249
[3754]	valid_0's auc: 0.898256
[3755]	valid_0's auc: 0.898254
[3756]	valid_0's auc: 0.89826
[3757]	valid_0's auc: 0.898251
[3758]	valid_0's auc: 0.898258
[3759]	valid_0's auc: 0.89825
[3760]	valid_0's auc: 0.898243
[3761]	valid_0's auc: 0.89824
[3762]	valid_0's auc: 0.898252
[3763]	valid_0's auc: 0.898253
[3764]	valid_0's auc: 0.898259
[3765]	valid_0's auc: 0.898259
[3766]	valid_0's auc: 0.898267
[3767]	valid_0's auc: 0.898281
[3768]	valid_0's auc: 0.898296
[3769]	valid_0's auc: 0.89831
[3770]	valid_0's auc: 0.898315
[3771]	valid_0's auc: 0.898316
[3772]	valid_0's auc: 0.898319
[3773]	valid_0's auc: 0.89832
[3774]	valid_0's auc: 0.898328
[3775]	valid_0's auc: 0.898322
[3776]	valid_0's auc: 0.898314
[3777]	valid_0's auc: 0.898315
[3778]	valid_0's auc: 0.898334
[3779]	valid_0's auc: 0.898341
[3780]	valid_0's auc: 0.898352
[3781]	valid_0's auc: 0.898359
[3782]	valid_0's auc: 0.898355
[3783]	valid_0's auc: 0.898366
[3784]	valid_0's auc: 0.898363
[3785]	valid_0's auc: 0.898378
[3786]	valid_0's auc: 0.898376
[3787]	valid_0's auc: 0.898381
[3788]	valid_0's auc: 0.898389
[3789]	valid_0's auc: 0.898398
[3790]	valid_0's auc: 0.898395
[3791]	valid_0's auc: 0.89838
[3792]	valid_0's auc: 0.898387
[3793]	valid_0's auc: 0.898397
[3794]	valid_0's auc: 0.898395
[3795]	valid_0's auc: 0.898392
[3796]	valid_0's auc: 0.898397
[3797]	valid_0's auc: 0.898393
[3798]	valid_0's auc: 0.898412
[3799]	valid_0's auc: 0.898421
[3800]	valid_0's auc: 0.898413
[3801]	valid_0's auc: 0.898417
[3802]	valid_0's auc: 0.898412
[3803]	valid_0's auc: 0.898413
[3804]	valid_0's auc: 0.898411
[3805]	valid_0's auc: 0.898418
[3806]	valid_0's auc: 0.898419
[3807]	valid_0's auc: 0.898423
[3808]	valid_0's auc: 0.89843
[3809]	valid_0's auc: 0.898435
[3810]	valid_0's auc: 0.898438
[3811]	valid_0's auc: 0.898451
[3812]	valid_0's auc: 0.898446
[3813]	valid_0's auc: 0.898446
[3814]	valid_0's auc: 0.898447
[3815]	valid_0's auc: 0.898451
[3816]	valid_0's auc: 0.898455
[3817]	valid_0's auc: 0.898452
[3818]	valid_0's auc: 0.898457
[3819]	valid_0's auc: 0.898462
[3820]	valid_0's auc: 0.898463
[3821]	valid_0's auc: 0.898468
[3822]	valid_0's auc: 0.898478
[3823]	valid_0's auc: 0.898482
[3824]	valid_0's auc: 0.898487
[3825]	valid_0's auc: 0.898489
[3826]	valid_0's auc: 0.898493
[3827]	valid_0's auc: 0.898503
[3828]	valid_0's auc: 0.898503
[3829]	valid_0's auc: 0.898514
[3830]	valid_0's auc: 0.898528
[3831]	valid_0's auc: 0.898523
[3832]	valid_0's auc: 0.898534
[3833]	valid_0's auc: 0.898542
[3834]	valid_0's auc: 0.898539
[3835]	valid_0's auc: 0.898545
[3836]	valid_0's auc: 0.898552
[3837]	valid_0's auc: 0.898544
[3838]	valid_0's auc: 0.898552
[3839]	valid_0's auc: 0.898549
[3840]	valid_0's auc: 0.898549
[3841]	valid_0's auc: 0.898547
[3842]	valid_0's auc: 0.898555
[3843]	valid_0's auc: 0.898562
[3844]	valid_0's auc: 0.898561
[3845]	valid_0's auc: 0.898571
[3846]	valid_0's auc: 0.898575
[3847]	valid_0's auc: 0.898585
[3848]	valid_0's auc: 0.898587
[3849]	valid_0's auc: 0.898587
[3850]	valid_0's auc: 0.89859
[3851]	valid_0's auc: 0.898591
[3852]	valid_0's auc: 0.898591
[3853]	valid_0's auc: 0.898592
[3854]	valid_0's auc: 0.898607
[3855]	valid_0's auc: 0.898621
[3856]	valid_0's auc: 0.898612
[3857]	valid_0's auc: 0.898613
[3858]	valid_0's auc: 0.898618
[3859]	valid_0's auc: 0.898616
[3860]	valid_0's auc: 0.89863
[3861]	valid_0's auc: 0.898626
[3862]	valid_0's auc: 0.898639
[3863]	valid_0's auc: 0.89865
[3864]	valid_0's auc: 0.898651
[3865]	valid_0's auc: 0.898648
[3866]	valid_0's auc: 0.898662
[3867]	valid_0's auc: 0.898662
[3868]	valid_0's auc: 0.898668
[3869]	valid_0's auc: 0.898666
[3870]	valid_0's auc: 0.898671
[3871]	valid_0's auc: 0.89867
[3872]	valid_0's auc: 0.898666
[3873]	valid_0's auc: 0.89867
[3874]	valid_0's auc: 0.898666
[3875]	valid_0's auc: 0.898671
[3876]	valid_0's auc: 0.898667
[3877]	valid_0's auc: 0.898669
[3878]	valid_0's auc: 0.898676
[3879]	valid_0's auc: 0.898692
[3880]	valid_0's auc: 0.898685
[3881]	valid_0's auc: 0.898696
[3882]	valid_0's auc: 0.898693
[3883]	valid_0's auc: 0.898698
[3884]	valid_0's auc: 0.8987
[3885]	valid_0's auc: 0.898721
[3886]	valid_0's auc: 0.898726
[3887]	valid_0's auc: 0.898735
[3888]	valid_0's auc: 0.898739
[3889]	valid_0's auc: 0.898739
[3890]	valid_0's auc: 0.898742
[3891]	valid_0's auc: 0.898744
[3892]	valid_0's auc: 0.898743
[3893]	valid_0's auc: 0.89875
[3894]	valid_0's auc: 0.898744
[3895]	valid_0's auc: 0.898742
[3896]	valid_0's auc: 0.89874
[3897]	valid_0's auc: 0.898752
[3898]	valid_0's auc: 0.898747
[3899]	valid_0's auc: 0.898751
[3900]	valid_0's auc: 0.898752
[3901]	valid_0's auc: 0.898751
[3902]	valid_0's auc: 0.898754
[3903]	valid_0's auc: 0.898758
[3904]	valid_0's auc: 0.898765
[3905]	valid_0's auc: 0.898766
[3906]	valid_0's auc: 0.898762
[3907]	valid_0's auc: 0.898776
[3908]	valid_0's auc: 0.89878
[3909]	valid_0's auc: 0.898788
[3910]	valid_0's auc: 0.898798
[3911]	valid_0's auc: 0.898801
[3912]	valid_0's auc: 0.898809
[3913]	valid_0's auc: 0.89881
[3914]	valid_0's auc: 0.898806
[3915]	valid_0's auc: 0.898794
[3916]	valid_0's auc: 0.898801
[3917]	valid_0's auc: 0.89881
[3918]	valid_0's auc: 0.898817
[3919]	valid_0's auc: 0.898824
[3920]	valid_0's auc: 0.898827
[3921]	valid_0's auc: 0.898828
[3922]	valid_0's auc: 0.898836
[3923]	valid_0's auc: 0.89883
[3924]	valid_0's auc: 0.898835
[3925]	valid_0's auc: 0.898837
[3926]	valid_0's auc: 0.898849
[3927]	valid_0's auc: 0.898842
[3928]	valid_0's auc: 0.898848
[3929]	valid_0's auc: 0.898859
[3930]	valid_0's auc: 0.898869
[3931]	valid_0's auc: 0.898861
[3932]	valid_0's auc: 0.898856
[3933]	valid_0's auc: 0.898862
[3934]	valid_0's auc: 0.898875
[3935]	valid_0's auc: 0.898879
[3936]	valid_0's auc: 0.898884
[3937]	valid_0's auc: 0.898886
[3938]	valid_0's auc: 0.898885
[3939]	valid_0's auc: 0.898891
[3940]	valid_0's auc: 0.898882
[3941]	valid_0's auc: 0.898883
[3942]	valid_0's auc: 0.898894
[3943]	valid_0's auc: 0.89889
[3944]	valid_0's auc: 0.898893
[3945]	valid_0's auc: 0.8989
[3946]	valid_0's auc: 0.898906
[3947]	valid_0's auc: 0.898905
[3948]	valid_0's auc: 0.898902
[3949]	valid_0's auc: 0.898901
[3950]	valid_0's auc: 0.898893
[3951]	valid_0's auc: 0.898901
[3952]	valid_0's auc: 0.898902
[3953]	valid_0's auc: 0.898895
[3954]	valid_0's auc: 0.898902
[3955]	valid_0's auc: 0.898905
[3956]	valid_0's auc: 0.898915
[3957]	valid_0's auc: 0.898916
[3958]	valid_0's auc: 0.898917
[3959]	valid_0's auc: 0.898913
[3960]	valid_0's auc: 0.898918
[3961]	valid_0's auc: 0.898917
[3962]	valid_0's auc: 0.89892
[3963]	valid_0's auc: 0.898919
[3964]	valid_0's auc: 0.898921
[3965]	valid_0's auc: 0.898921
[3966]	valid_0's auc: 0.898931
[3967]	valid_0's auc: 0.898929
[3968]	valid_0's auc: 0.898923
[3969]	valid_0's auc: 0.898928
[3970]	valid_0's auc: 0.898929
[3971]	valid_0's auc: 0.898943
[3972]	valid_0's auc: 0.898945
[3973]	valid_0's auc: 0.898944
[3974]	valid_0's auc: 0.898954
[3975]	valid_0's auc: 0.89895
[3976]	valid_0's auc: 0.898964
[3977]	valid_0's auc: 0.898969
[3978]	valid_0's auc: 0.898972
[3979]	valid_0's auc: 0.898972
[3980]	valid_0's auc: 0.898981
[3981]	valid_0's auc: 0.898983
[3982]	valid_0's auc: 0.898992
[3983]	valid_0's auc: 0.899009
[3984]	valid_0's auc: 0.899019
[3985]	valid_0's auc: 0.899031
[3986]	valid_0's auc: 0.899033
[3987]	valid_0's auc: 0.899033
[3988]	valid_0's auc: 0.899034
[3989]	valid_0's auc: 0.899042
[3990]	valid_0's auc: 0.899041
[3991]	valid_0's auc: 0.899038
[3992]	valid_0's auc: 0.899049
[3993]	valid_0's auc: 0.899053
[3994]	valid_0's auc: 0.899065
[3995]	valid_0's auc: 0.899066
[3996]	valid_0's auc: 0.899071
[3997]	valid_0's auc: 0.899081
[3998]	valid_0's auc: 0.899075
[3999]	valid_0's auc: 0.899087
[4000]	valid_0's auc: 0.899088
[4001]	valid_0's auc: 0.899092
[4002]	valid_0's auc: 0.899091
[4003]	valid_0's auc: 0.8991
[4004]	valid_0's auc: 0.899087
[4005]	valid_0's auc: 0.899095
[4006]	valid_0's auc: 0.899095
[4007]	valid_0's auc: 0.8991
[4008]	valid_0's auc: 0.899089
[4009]	valid_0's auc: 0.899085
[4010]	valid_0's auc: 0.899083
[4011]	valid_0's auc: 0.899079
[4012]	valid_0's auc: 0.899077
[4013]	valid_0's auc: 0.899081
[4014]	valid_0's auc: 0.899079
[4015]	valid_0's auc: 0.899075
[4016]	valid_0's auc: 0.899085
[4017]	valid_0's auc: 0.899088
[4018]	valid_0's auc: 0.899101
[4019]	valid_0's auc: 0.899102
[4020]	valid_0's auc: 0.89909
[4021]	valid_0's auc: 0.899088
[4022]	valid_0's auc: 0.899092
[4023]	valid_0's auc: 0.899097
[4024]	valid_0's auc: 0.899092
[4025]	valid_0's auc: 0.899108
[4026]	valid_0's auc: 0.899116
[4027]	valid_0's auc: 0.899118
[4028]	valid_0's auc: 0.899132
[4029]	valid_0's auc: 0.899139
[4030]	valid_0's auc: 0.899138
[4031]	valid_0's auc: 0.899132
[4032]	valid_0's auc: 0.899134
[4033]	valid_0's auc: 0.899144
[4034]	valid_0's auc: 0.899149
[4035]	valid_0's auc: 0.899143
[4036]	valid_0's auc: 0.899149
[4037]	valid_0's auc: 0.899143
[4038]	valid_0's auc: 0.899155
[4039]	valid_0's auc: 0.899163
[4040]	valid_0's auc: 0.899162
[4041]	valid_0's auc: 0.899163
[4042]	valid_0's auc: 0.899172
[4043]	valid_0's auc: 0.899174
[4044]	valid_0's auc: 0.89918
[4045]	valid_0's auc: 0.899189
[4046]	valid_0's auc: 0.899196
[4047]	valid_0's auc: 0.899192
[4048]	valid_0's auc: 0.899195
[4049]	valid_0's auc: 0.899192
[4050]	valid_0's auc: 0.899213
[4051]	valid_0's auc: 0.899219
[4052]	valid_0's auc: 0.899223
[4053]	valid_0's auc: 0.899232
[4054]	valid_0's auc: 0.899227
[4055]	valid_0's auc: 0.899231
[4056]	valid_0's auc: 0.899235
[4057]	valid_0's auc: 0.899236
[4058]	valid_0's auc: 0.899234
[4059]	valid_0's auc: 0.899238
[4060]	valid_0's auc: 0.899255
[4061]	valid_0's auc: 0.899267
[4062]	valid_0's auc: 0.899272
[4063]	valid_0's auc: 0.899275
[4064]	valid_0's auc: 0.899271
[4065]	valid_0's auc: 0.899271
[4066]	valid_0's auc: 0.899265
[4067]	valid_0's auc: 0.899269
[4068]	valid_0's auc: 0.899273
[4069]	valid_0's auc: 0.899273
[4070]	valid_0's auc: 0.899287
[4071]	valid_0's auc: 0.899291
[4072]	valid_0's auc: 0.899295
[4073]	valid_0's auc: 0.899294
[4074]	valid_0's auc: 0.8993
[4075]	valid_0's auc: 0.899295
[4076]	valid_0's auc: 0.899292
[4077]	valid_0's auc: 0.899305
[4078]	valid_0's auc: 0.899305
[4079]	valid_0's auc: 0.8993
[4080]	valid_0's auc: 0.899298
[4081]	valid_0's auc: 0.899293
[4082]	valid_0's auc: 0.899283
[4083]	valid_0's auc: 0.899289
[4084]	valid_0's auc: 0.899277
[4085]	valid_0's auc: 0.899286
[4086]	valid_0's auc: 0.899286
[4087]	valid_0's auc: 0.899285
[4088]	valid_0's auc: 0.899283
[4089]	valid_0's auc: 0.899274
[4090]	valid_0's auc: 0.899273
[4091]	valid_0's auc: 0.899275
[4092]	valid_0's auc: 0.899268
[4093]	valid_0's auc: 0.899275
[4094]	valid_0's auc: 0.899282
[4095]	valid_0's auc: 0.899284
[4096]	valid_0's auc: 0.899286
[4097]	valid_0's auc: 0.899293
[4098]	valid_0's auc: 0.899292
[4099]	valid_0's auc: 0.899292
[4100]	valid_0's auc: 0.899291
[4101]	valid_0's auc: 0.899297
[4102]	valid_0's auc: 0.899303
[4103]	valid_0's auc: 0.899302
[4104]	valid_0's auc: 0.899313
[4105]	valid_0's auc: 0.899312
[4106]	valid_0's auc: 0.899311
[4107]	valid_0's auc: 0.899305
[4108]	valid_0's auc: 0.899311
[4109]	valid_0's auc: 0.899313
[4110]	valid_0's auc: 0.899317
[4111]	valid_0's auc: 0.899324
[4112]	valid_0's auc: 0.899338
[4113]	valid_0's auc: 0.89935
[4114]	valid_0's auc: 0.899359
[4115]	valid_0's auc: 0.899361
[4116]	valid_0's auc: 0.899362
[4117]	valid_0's auc: 0.899375
[4118]	valid_0's auc: 0.899383
[4119]	valid_0's auc: 0.899385
[4120]	valid_0's auc: 0.899388
[4121]	valid_0's auc: 0.899403
[4122]	valid_0's auc: 0.899422
[4123]	valid_0's auc: 0.899431
[4124]	valid_0's auc: 0.899441
[4125]	valid_0's auc: 0.899446
[4126]	valid_0's auc: 0.899453
[4127]	valid_0's auc: 0.899445
[4128]	valid_0's auc: 0.89945
[4129]	valid_0's auc: 0.899462
[4130]	valid_0's auc: 0.899477
[4131]	valid_0's auc: 0.899491
[4132]	valid_0's auc: 0.899494
[4133]	valid_0's auc: 0.8995
[4134]	valid_0's auc: 0.899501
[4135]	valid_0's auc: 0.899505
[4136]	valid_0's auc: 0.899494
[4137]	valid_0's auc: 0.899486
[4138]	valid_0's auc: 0.899497
[4139]	valid_0's auc: 0.899507
[4140]	valid_0's auc: 0.89951
[4141]	valid_0's auc: 0.899514
[4142]	valid_0's auc: 0.899508
[4143]	valid_0's auc: 0.899503
[4144]	valid_0's auc: 0.899498
[4145]	valid_0's auc: 0.899512
[4146]	valid_0's auc: 0.89951
[4147]	valid_0's auc: 0.899517
[4148]	valid_0's auc: 0.899529
[4149]	valid_0's auc: 0.899536
[4150]	valid_0's auc: 0.899534
[4151]	valid_0's auc: 0.899545
[4152]	valid_0's auc: 0.899554
[4153]	valid_0's auc: 0.899558
[4154]	valid_0's auc: 0.899554
[4155]	valid_0's auc: 0.89956
[4156]	valid_0's auc: 0.899571
[4157]	valid_0's auc: 0.899566
[4158]	valid_0's auc: 0.899562
[4159]	valid_0's auc: 0.899562
[4160]	valid_0's auc: 0.899571
[4161]	valid_0's auc: 0.899582
[4162]	valid_0's auc: 0.89958
[4163]	valid_0's auc: 0.899586
[4164]	valid_0's auc: 0.899598
[4165]	valid_0's auc: 0.899605
[4166]	valid_0's auc: 0.899606
[4167]	valid_0's auc: 0.899613
[4168]	valid_0's auc: 0.899617
[4169]	valid_0's auc: 0.899622
[4170]	valid_0's auc: 0.899629
[4171]	valid_0's auc: 0.899633
[4172]	valid_0's auc: 0.89964
[4173]	valid_0's auc: 0.899639
[4174]	valid_0's auc: 0.89964
[4175]	valid_0's auc: 0.899655
[4176]	valid_0's auc: 0.899665
[4177]	valid_0's auc: 0.899664
[4178]	valid_0's auc: 0.899663
[4179]	valid_0's auc: 0.899662
[4180]	valid_0's auc: 0.899661
[4181]	valid_0's auc: 0.899666
[4182]	valid_0's auc: 0.899673
[4183]	valid_0's auc: 0.899672
[4184]	valid_0's auc: 0.899673
[4185]	valid_0's auc: 0.899667
[4186]	valid_0's auc: 0.899665
[4187]	valid_0's auc: 0.899658
[4188]	valid_0's auc: 0.899659
[4189]	valid_0's auc: 0.899656
[4190]	valid_0's auc: 0.899663
[4191]	valid_0's auc: 0.899675
[4192]	valid_0's auc: 0.899677
[4193]	valid_0's auc: 0.899669
[4194]	valid_0's auc: 0.899667
[4195]	valid_0's auc: 0.899674
[4196]	valid_0's auc: 0.899674
[4197]	valid_0's auc: 0.899678
[4198]	valid_0's auc: 0.899676
[4199]	valid_0's auc: 0.899677
[4200]	valid_0's auc: 0.899678
[4201]	valid_0's auc: 0.89968
[4202]	valid_0's auc: 0.899686
[4203]	valid_0's auc: 0.899674
[4204]	valid_0's auc: 0.89968
[4205]	valid_0's auc: 0.899678
[4206]	valid_0's auc: 0.899679
[4207]	valid_0's auc: 0.899677
[4208]	valid_0's auc: 0.899675
[4209]	valid_0's auc: 0.899684
[4210]	valid_0's auc: 0.899672
[4211]	valid_0's auc: 0.899685
[4212]	valid_0's auc: 0.899686
[4213]	valid_0's auc: 0.899681
[4214]	valid_0's auc: 0.899674
[4215]	valid_0's auc: 0.899675
[4216]	valid_0's auc: 0.899677
[4217]	valid_0's auc: 0.899673
[4218]	valid_0's auc: 0.89967
[4219]	valid_0's auc: 0.899673
[4220]	valid_0's auc: 0.899679
[4221]	valid_0's auc: 0.89968
[4222]	valid_0's auc: 0.899678
[4223]	valid_0's auc: 0.899669
[4224]	valid_0's auc: 0.899676
[4225]	valid_0's auc: 0.899676
[4226]	valid_0's auc: 0.899678
[4227]	valid_0's auc: 0.899686
[4228]	valid_0's auc: 0.899683
[4229]	valid_0's auc: 0.899687
[4230]	valid_0's auc: 0.89969
[4231]	valid_0's auc: 0.899692
[4232]	valid_0's auc: 0.899704
[4233]	valid_0's auc: 0.899708
[4234]	valid_0's auc: 0.899717
[4235]	valid_0's auc: 0.899719
[4236]	valid_0's auc: 0.899723
[4237]	valid_0's auc: 0.89972
[4238]	valid_0's auc: 0.899721
[4239]	valid_0's auc: 0.899725
[4240]	valid_0's auc: 0.899715
[4241]	valid_0's auc: 0.899713
[4242]	valid_0's auc: 0.899718
[4243]	valid_0's auc: 0.899715
[4244]	valid_0's auc: 0.899714
[4245]	valid_0's auc: 0.899716
[4246]	valid_0's auc: 0.899718
[4247]	valid_0's auc: 0.899714
[4248]	valid_0's auc: 0.899712
[4249]	valid_0's auc: 0.899711
[4250]	valid_0's auc: 0.899719
[4251]	valid_0's auc: 0.899715
[4252]	valid_0's auc: 0.899714
[4253]	valid_0's auc: 0.899706
[4254]	valid_0's auc: 0.899699
[4255]	valid_0's auc: 0.899704
[4256]	valid_0's auc: 0.899698
[4257]	valid_0's auc: 0.899709
[4258]	valid_0's auc: 0.899703
[4259]	valid_0's auc: 0.899701
[4260]	valid_0's auc: 0.899708
[4261]	valid_0's auc: 0.899705
[4262]	valid_0's auc: 0.899702
[4263]	valid_0's auc: 0.899697
[4264]	valid_0's auc: 0.899697
[4265]	valid_0's auc: 0.899711
[4266]	valid_0's auc: 0.899709
[4267]	valid_0's auc: 0.899709
[4268]	valid_0's auc: 0.899714
[4269]	valid_0's auc: 0.89972
[4270]	valid_0's auc: 0.899723
[4271]	valid_0's auc: 0.899719
[4272]	valid_0's auc: 0.899717
[4273]	valid_0's auc: 0.899714
[4274]	valid_0's auc: 0.899713
[4275]	valid_0's auc: 0.899713
[4276]	valid_0's auc: 0.899725
[4277]	valid_0's auc: 0.899737
[4278]	valid_0's auc: 0.899746
[4279]	valid_0's auc: 0.89975
[4280]	valid_0's auc: 0.899756
[4281]	valid_0's auc: 0.899752
[4282]	valid_0's auc: 0.899748
[4283]	valid_0's auc: 0.899745
[4284]	valid_0's auc: 0.899746
[4285]	valid_0's auc: 0.899747
[4286]	valid_0's auc: 0.899752
[4287]	valid_0's auc: 0.899743
[4288]	valid_0's auc: 0.899745
[4289]	valid_0's auc: 0.899749
[4290]	valid_0's auc: 0.899761
[4291]	valid_0's auc: 0.899762
[4292]	valid_0's auc: 0.899767
[4293]	valid_0's auc: 0.89977
[4294]	valid_0's auc: 0.899775
[4295]	valid_0's auc: 0.899772
[4296]	valid_0's auc: 0.899767
[4297]	valid_0's auc: 0.899775
[4298]	valid_0's auc: 0.899786
[4299]	valid_0's auc: 0.899789
[4300]	valid_0's auc: 0.899787
[4301]	valid_0's auc: 0.899787
[4302]	valid_0's auc: 0.899774
[4303]	valid_0's auc: 0.899777
[4304]	valid_0's auc: 0.89978
[4305]	valid_0's auc: 0.899771
[4306]	valid_0's auc: 0.899767
[4307]	valid_0's auc: 0.899763
[4308]	valid_0's auc: 0.899761
[4309]	valid_0's auc: 0.899764
[4310]	valid_0's auc: 0.899764
[4311]	valid_0's auc: 0.899763
[4312]	valid_0's auc: 0.899762
[4313]	valid_0's auc: 0.899768
[4314]	valid_0's auc: 0.899767
[4315]	valid_0's auc: 0.899769
[4316]	valid_0's auc: 0.899772
[4317]	valid_0's auc: 0.899776
[4318]	valid_0's auc: 0.899772
[4319]	valid_0's auc: 0.899779
[4320]	valid_0's auc: 0.899783
[4321]	valid_0's auc: 0.899792
[4322]	valid_0's auc: 0.899791
[4323]	valid_0's auc: 0.899792
[4324]	valid_0's auc: 0.899793
[4325]	valid_0's auc: 0.8998
[4326]	valid_0's auc: 0.899791
[4327]	valid_0's auc: 0.899794
[4328]	valid_0's auc: 0.899784
[4329]	valid_0's auc: 0.899771
[4330]	valid_0's auc: 0.899781
[4331]	valid_0's auc: 0.899781
[4332]	valid_0's auc: 0.899791
[4333]	valid_0's auc: 0.899794
[4334]	valid_0's auc: 0.899799
[4335]	valid_0's auc: 0.8998
[4336]	valid_0's auc: 0.899802
[4337]	valid_0's auc: 0.899812
[4338]	valid_0's auc: 0.899804
[4339]	valid_0's auc: 0.899822
[4340]	valid_0's auc: 0.899824
[4341]	valid_0's auc: 0.899824
[4342]	valid_0's auc: 0.899834
[4343]	valid_0's auc: 0.899829
[4344]	valid_0's auc: 0.899828
[4345]	valid_0's auc: 0.899829
[4346]	valid_0's auc: 0.899827
[4347]	valid_0's auc: 0.899824
[4348]	valid_0's auc: 0.899822
[4349]	valid_0's auc: 0.899821
[4350]	valid_0's auc: 0.899819
[4351]	valid_0's auc: 0.899819
[4352]	valid_0's auc: 0.899813
[4353]	valid_0's auc: 0.899814
[4354]	valid_0's auc: 0.899823
[4355]	valid_0's auc: 0.899825
[4356]	valid_0's auc: 0.899827
[4357]	valid_0's auc: 0.899839
[4358]	valid_0's auc: 0.899834
[4359]	valid_0's auc: 0.899826
[4360]	valid_0's auc: 0.89983
[4361]	valid_0's auc: 0.899836
[4362]	valid_0's auc: 0.899842
[4363]	valid_0's auc: 0.899837
[4364]	valid_0's auc: 0.899845
[4365]	valid_0's auc: 0.899854
[4366]	valid_0's auc: 0.899857
[4367]	valid_0's auc: 0.899858
[4368]	valid_0's auc: 0.899857
[4369]	valid_0's auc: 0.899865
[4370]	valid_0's auc: 0.899868
[4371]	valid_0's auc: 0.899866
[4372]	valid_0's auc: 0.899865
[4373]	valid_0's auc: 0.899869
[4374]	valid_0's auc: 0.899874
[4375]	valid_0's auc: 0.899881
[4376]	valid_0's auc: 0.899883
[4377]	valid_0's auc: 0.899875
[4378]	valid_0's auc: 0.899874
[4379]	valid_0's auc: 0.899879
[4380]	valid_0's auc: 0.899878
[4381]	valid_0's auc: 0.899881
[4382]	valid_0's auc: 0.899887
[4383]	valid_0's auc: 0.899883
[4384]	valid_0's auc: 0.899889
[4385]	valid_0's auc: 0.899889
[4386]	valid_0's auc: 0.89989
[4387]	valid_0's auc: 0.899892
[4388]	valid_0's auc: 0.899893
[4389]	valid_0's auc: 0.899892
[4390]	valid_0's auc: 0.89989
[4391]	valid_0's auc: 0.899892
[4392]	valid_0's auc: 0.899895
[4393]	valid_0's auc: 0.899893
[4394]	valid_0's auc: 0.899901
[4395]	valid_0's auc: 0.899892
[4396]	valid_0's auc: 0.899893
[4397]	valid_0's auc: 0.899888
[4398]	valid_0's auc: 0.899884
[4399]	valid_0's auc: 0.899891
[4400]	valid_0's auc: 0.899886
[4401]	valid_0's auc: 0.899884
[4402]	valid_0's auc: 0.899893
[4403]	valid_0's auc: 0.899892
[4404]	valid_0's auc: 0.899896
[4405]	valid_0's auc: 0.899897
[4406]	valid_0's auc: 0.899893
[4407]	valid_0's auc: 0.899886
[4408]	valid_0's auc: 0.899883
[4409]	valid_0's auc: 0.899877
[4410]	valid_0's auc: 0.899874
[4411]	valid_0's auc: 0.899876
[4412]	valid_0's auc: 0.899874
[4413]	valid_0's auc: 0.899874
[4414]	valid_0's auc: 0.899883
[4415]	valid_0's auc: 0.899885
[4416]	valid_0's auc: 0.899888
[4417]	valid_0's auc: 0.89989
[4418]	valid_0's auc: 0.899906
[4419]	valid_0's auc: 0.899899
[4420]	valid_0's auc: 0.89989
[4421]	valid_0's auc: 0.899895
[4422]	valid_0's auc: 0.899901
[4423]	valid_0's auc: 0.899912
[4424]	valid_0's auc: 0.899908
[4425]	valid_0's auc: 0.899896
[4426]	valid_0's auc: 0.8999
[4427]	valid_0's auc: 0.8999
[4428]	valid_0's auc: 0.899898
[4429]	valid_0's auc: 0.899897
[4430]	valid_0's auc: 0.899889
[4431]	valid_0's auc: 0.899886
[4432]	valid_0's auc: 0.899898
[4433]	valid_0's auc: 0.899893
[4434]	valid_0's auc: 0.899904
[4435]	valid_0's auc: 0.899898
[4436]	valid_0's auc: 0.8999
[4437]	valid_0's auc: 0.899898
[4438]	valid_0's auc: 0.899901
[4439]	valid_0's auc: 0.899899
[4440]	valid_0's auc: 0.899894
[4441]	valid_0's auc: 0.899895
[4442]	valid_0's auc: 0.899895
[4443]	valid_0's auc: 0.899899
[4444]	valid_0's auc: 0.899903
[4445]	valid_0's auc: 0.899908
[4446]	valid_0's auc: 0.899906
[4447]	valid_0's auc: 0.899902
[4448]	valid_0's auc: 0.899901
[4449]	valid_0's auc: 0.899892
[4450]	valid_0's auc: 0.899901
[4451]	valid_0's auc: 0.899909
[4452]	valid_0's auc: 0.899913
[4453]	valid_0's auc: 0.899915
[4454]	valid_0's auc: 0.899921
[4455]	valid_0's auc: 0.899914
[4456]	valid_0's auc: 0.899914
[4457]	valid_0's auc: 0.899916
[4458]	valid_0's auc: 0.899922
[4459]	valid_0's auc: 0.899927
[4460]	valid_0's auc: 0.899926
[4461]	valid_0's auc: 0.899928
[4462]	valid_0's auc: 0.89992
[4463]	valid_0's auc: 0.899929
[4464]	valid_0's auc: 0.899921
[4465]	valid_0's auc: 0.899927
[4466]	valid_0's auc: 0.899943
[4467]	valid_0's auc: 0.899941
[4468]	valid_0's auc: 0.899947
[4469]	valid_0's auc: 0.899951
[4470]	valid_0's auc: 0.899947
[4471]	valid_0's auc: 0.899947
[4472]	valid_0's auc: 0.899951
[4473]	valid_0's auc: 0.899958
[4474]	valid_0's auc: 0.899957
[4475]	valid_0's auc: 0.899954
[4476]	valid_0's auc: 0.899969
[4477]	valid_0's auc: 0.899978
[4478]	valid_0's auc: 0.899997
[4479]	valid_0's auc: 0.89999
[4480]	valid_0's auc: 0.899998
[4481]	valid_0's auc: 0.900002
[4482]	valid_0's auc: 0.899997
[4483]	valid_0's auc: 0.899991
[4484]	valid_0's auc: 0.899992
[4485]	valid_0's auc: 0.899993
[4486]	valid_0's auc: 0.89999
[4487]	valid_0's auc: 0.899986
[4488]	valid_0's auc: 0.899982
[4489]	valid_0's auc: 0.899984
[4490]	valid_0's auc: 0.899987
[4491]	valid_0's auc: 0.899992
[4492]	valid_0's auc: 0.900006
[4493]	valid_0's auc: 0.900013
[4494]	valid_0's auc: 0.900017
[4495]	valid_0's auc: 0.900016
[4496]	valid_0's auc: 0.900011
[4497]	valid_0's auc: 0.900011
[4498]	valid_0's auc: 0.900004
[4499]	valid_0's auc: 0.900006
[4500]	valid_0's auc: 0.900006
[4501]	valid_0's auc: 0.900013
[4502]	valid_0's auc: 0.900008
[4503]	valid_0's auc: 0.900008
[4504]	valid_0's auc: 0.900016
[4505]	valid_0's auc: 0.900017
[4506]	valid_0's auc: 0.900018
[4507]	valid_0's auc: 0.900021
[4508]	valid_0's auc: 0.900023
[4509]	valid_0's auc: 0.900018
[4510]	valid_0's auc: 0.900017
[4511]	valid_0's auc: 0.900022
[4512]	valid_0's auc: 0.900028
[4513]	valid_0's auc: 0.900036
[4514]	valid_0's auc: 0.900037
[4515]	valid_0's auc: 0.900032
[4516]	valid_0's auc: 0.900031
[4517]	valid_0's auc: 0.90002
[4518]	valid_0's auc: 0.900027
[4519]	valid_0's auc: 0.900019
[4520]	valid_0's auc: 0.900014
[4521]	valid_0's auc: 0.900015
[4522]	valid_0's auc: 0.900007
[4523]	valid_0's auc: 0.900013
[4524]	valid_0's auc: 0.900018
[4525]	valid_0's auc: 0.900023
[4526]	valid_0's auc: 0.900029
[4527]	valid_0's auc: 0.900038
[4528]	valid_0's auc: 0.900031
[4529]	valid_0's auc: 0.900041
[4530]	valid_0's auc: 0.900051
[4531]	valid_0's auc: 0.90005
[4532]	valid_0's auc: 0.90005
[4533]	valid_0's auc: 0.900054
[4534]	valid_0's auc: 0.900053
[4535]	valid_0's auc: 0.900055
[4536]	valid_0's auc: 0.90006
[4537]	valid_0's auc: 0.900069
[4538]	valid_0's auc: 0.900077
[4539]	valid_0's auc: 0.900091
[4540]	valid_0's auc: 0.900087
[4541]	valid_0's auc: 0.90009
[4542]	valid_0's auc: 0.900089
[4543]	valid_0's auc: 0.900094
[4544]	valid_0's auc: 0.900091
[4545]	valid_0's auc: 0.900091
[4546]	valid_0's auc: 0.900091
[4547]	valid_0's auc: 0.900095
[4548]	valid_0's auc: 0.900094
[4549]	valid_0's auc: 0.900111
[4550]	valid_0's auc: 0.900108
[4551]	valid_0's auc: 0.900107
[4552]	valid_0's auc: 0.900108
[4553]	valid_0's auc: 0.900113
[4554]	valid_0's auc: 0.900115
[4555]	valid_0's auc: 0.900119
[4556]	valid_0's auc: 0.900121
[4557]	valid_0's auc: 0.900131
[4558]	valid_0's auc: 0.900133
[4559]	valid_0's auc: 0.900127
[4560]	valid_0's auc: 0.900125
[4561]	valid_0's auc: 0.90012
[4562]	valid_0's auc: 0.900122
[4563]	valid_0's auc: 0.900114
[4564]	valid_0's auc: 0.900121
[4565]	valid_0's auc: 0.900124
[4566]	valid_0's auc: 0.900123
[4567]	valid_0's auc: 0.900129
[4568]	valid_0's auc: 0.900125
[4569]	valid_0's auc: 0.90013
[4570]	valid_0's auc: 0.90013
[4571]	valid_0's auc: 0.900129
[4572]	valid_0's auc: 0.900127
[4573]	valid_0's auc: 0.900134
[4574]	valid_0's auc: 0.900139
[4575]	valid_0's auc: 0.900133
[4576]	valid_0's auc: 0.90014
[4577]	valid_0's auc: 0.900138
[4578]	valid_0's auc: 0.900142
[4579]	valid_0's auc: 0.900141
[4580]	valid_0's auc: 0.90015
[4581]	valid_0's auc: 0.900147
[4582]	valid_0's auc: 0.900152
[4583]	valid_0's auc: 0.900151
[4584]	valid_0's auc: 0.90016
[4585]	valid_0's auc: 0.900162
[4586]	valid_0's auc: 0.900167
[4587]	valid_0's auc: 0.900166
[4588]	valid_0's auc: 0.900153
[4589]	valid_0's auc: 0.900153
[4590]	valid_0's auc: 0.900156
[4591]	valid_0's auc: 0.900159
[4592]	valid_0's auc: 0.90016
[4593]	valid_0's auc: 0.900167
[4594]	valid_0's auc: 0.900167
[4595]	valid_0's auc: 0.900166
[4596]	valid_0's auc: 0.900171
[4597]	valid_0's auc: 0.900175
[4598]	valid_0's auc: 0.900174
[4599]	valid_0's auc: 0.900169
[4600]	valid_0's auc: 0.900167
[4601]	valid_0's auc: 0.900161
[4602]	valid_0's auc: 0.900163
[4603]	valid_0's auc: 0.900156
[4604]	valid_0's auc: 0.900154
[4605]	valid_0's auc: 0.900155
[4606]	valid_0's auc: 0.900164
[4607]	valid_0's auc: 0.900156
[4608]	valid_0's auc: 0.900163
[4609]	valid_0's auc: 0.900173
[4610]	valid_0's auc: 0.900181
[4611]	valid_0's auc: 0.900178
[4612]	valid_0's auc: 0.900178
[4613]	valid_0's auc: 0.900179
[4614]	valid_0's auc: 0.900176
[4615]	valid_0's auc: 0.900188
[4616]	valid_0's auc: 0.900202
[4617]	valid_0's auc: 0.900203
[4618]	valid_0's auc: 0.900206
[4619]	valid_0's auc: 0.90021
[4620]	valid_0's auc: 0.900218
[4621]	valid_0's auc: 0.900221
[4622]	valid_0's auc: 0.900215
[4623]	valid_0's auc: 0.900217
[4624]	valid_0's auc: 0.90022
[4625]	valid_0's auc: 0.900223
[4626]	valid_0's auc: 0.900227
[4627]	valid_0's auc: 0.90023
[4628]	valid_0's auc: 0.900234
[4629]	valid_0's auc: 0.900232
[4630]	valid_0's auc: 0.900232
[4631]	valid_0's auc: 0.900238
[4632]	valid_0's auc: 0.900246
[4633]	valid_0's auc: 0.900248
[4634]	valid_0's auc: 0.900255
[4635]	valid_0's auc: 0.900262
[4636]	valid_0's auc: 0.900266
[4637]	valid_0's auc: 0.900266
[4638]	valid_0's auc: 0.900267
[4639]	valid_0's auc: 0.900263
[4640]	valid_0's auc: 0.900263
[4641]	valid_0's auc: 0.90026
[4642]	valid_0's auc: 0.900265
[4643]	valid_0's auc: 0.900266
[4644]	valid_0's auc: 0.900268
[4645]	valid_0's auc: 0.900275
[4646]	valid_0's auc: 0.900275
[4647]	valid_0's auc: 0.900278
[4648]	valid_0's auc: 0.900286
[4649]	valid_0's auc: 0.900284
[4650]	valid_0's auc: 0.900297
[4651]	valid_0's auc: 0.900295
[4652]	valid_0's auc: 0.900296
[4653]	valid_0's auc: 0.900295
[4654]	valid_0's auc: 0.900302
[4655]	valid_0's auc: 0.900294
[4656]	valid_0's auc: 0.900295
[4657]	valid_0's auc: 0.900297
[4658]	valid_0's auc: 0.900307
[4659]	valid_0's auc: 0.900305
[4660]	valid_0's auc: 0.900308
[4661]	valid_0's auc: 0.900311
[4662]	valid_0's auc: 0.900317
[4663]	valid_0's auc: 0.900318
[4664]	valid_0's auc: 0.900317
[4665]	valid_0's auc: 0.900326
[4666]	valid_0's auc: 0.900338
[4667]	valid_0's auc: 0.90034
[4668]	valid_0's auc: 0.900342
[4669]	valid_0's auc: 0.900344
[4670]	valid_0's auc: 0.900339
[4671]	valid_0's auc: 0.900341
[4672]	valid_0's auc: 0.900338
[4673]	valid_0's auc: 0.900341
[4674]	valid_0's auc: 0.900349
[4675]	valid_0's auc: 0.900341
[4676]	valid_0's auc: 0.900344
[4677]	valid_0's auc: 0.900348
[4678]	valid_0's auc: 0.900352
[4679]	valid_0's auc: 0.900346
[4680]	valid_0's auc: 0.900349
[4681]	valid_0's auc: 0.900348
[4682]	valid_0's auc: 0.900354
[4683]	valid_0's auc: 0.90035
[4684]	valid_0's auc: 0.900346
[4685]	valid_0's auc: 0.900345
[4686]	valid_0's auc: 0.900344
[4687]	valid_0's auc: 0.900338
[4688]	valid_0's auc: 0.900336
[4689]	valid_0's auc: 0.90033
[4690]	valid_0's auc: 0.900329
[4691]	valid_0's auc: 0.900327
[4692]	valid_0's auc: 0.900327
[4693]	valid_0's auc: 0.900332
[4694]	valid_0's auc: 0.90034
[4695]	valid_0's auc: 0.900338
[4696]	valid_0's auc: 0.900342
[4697]	valid_0's auc: 0.900347
[4698]	valid_0's auc: 0.900349
[4699]	valid_0's auc: 0.90035
[4700]	valid_0's auc: 0.900352
[4701]	valid_0's auc: 0.900358
[4702]	valid_0's auc: 0.900369
[4703]	valid_0's auc: 0.90037
[4704]	valid_0's auc: 0.900376
[4705]	valid_0's auc: 0.900377
[4706]	valid_0's auc: 0.90037
[4707]	valid_0's auc: 0.90037
[4708]	valid_0's auc: 0.900369
[4709]	valid_0's auc: 0.900376
[4710]	valid_0's auc: 0.900384
[4711]	valid_0's auc: 0.900383
[4712]	valid_0's auc: 0.900392
[4713]	valid_0's auc: 0.900389
[4714]	valid_0's auc: 0.900392
[4715]	valid_0's auc: 0.900394
[4716]	valid_0's auc: 0.900389
[4717]	valid_0's auc: 0.900397
[4718]	valid_0's auc: 0.900402
[4719]	valid_0's auc: 0.900407
[4720]	valid_0's auc: 0.900412
[4721]	valid_0's auc: 0.900418
[4722]	valid_0's auc: 0.900418
[4723]	valid_0's auc: 0.900426
[4724]	valid_0's auc: 0.900425
[4725]	valid_0's auc: 0.900421
[4726]	valid_0's auc: 0.900421
[4727]	valid_0's auc: 0.900418
[4728]	valid_0's auc: 0.900414
[4729]	valid_0's auc: 0.90041
[4730]	valid_0's auc: 0.90041
[4731]	valid_0's auc: 0.900418
[4732]	valid_0's auc: 0.90042
[4733]	valid_0's auc: 0.900419
[4734]	valid_0's auc: 0.900414
[4735]	valid_0's auc: 0.900421
[4736]	valid_0's auc: 0.900417
[4737]	valid_0's auc: 0.900423
[4738]	valid_0's auc: 0.900425
[4739]	valid_0's auc: 0.900426
[4740]	valid_0's auc: 0.900433
[4741]	valid_0's auc: 0.900448
[4742]	valid_0's auc: 0.900455
[4743]	valid_0's auc: 0.900458
[4744]	valid_0's auc: 0.900459
[4745]	valid_0's auc: 0.900457
[4746]	valid_0's auc: 0.90047
[4747]	valid_0's auc: 0.90047
[4748]	valid_0's auc: 0.900468
[4749]	valid_0's auc: 0.900476
[4750]	valid_0's auc: 0.900478
[4751]	valid_0's auc: 0.900475
[4752]	valid_0's auc: 0.900468
[4753]	valid_0's auc: 0.900477
[4754]	valid_0's auc: 0.900477
[4755]	valid_0's auc: 0.900476
[4756]	valid_0's auc: 0.900474
[4757]	valid_0's auc: 0.900473
[4758]	valid_0's auc: 0.900477
[4759]	valid_0's auc: 0.900472
[4760]	valid_0's auc: 0.900473
[4761]	valid_0's auc: 0.900472
[4762]	valid_0's auc: 0.900471
[4763]	valid_0's auc: 0.900482
[4764]	valid_0's auc: 0.900475
[4765]	valid_0's auc: 0.900486
[4766]	valid_0's auc: 0.900496
[4767]	valid_0's auc: 0.900498
[4768]	valid_0's auc: 0.900502
[4769]	valid_0's auc: 0.900502
[4770]	valid_0's auc: 0.900512
[4771]	valid_0's auc: 0.900513
[4772]	valid_0's auc: 0.900511
[4773]	valid_0's auc: 0.900506
[4774]	valid_0's auc: 0.900515
[4775]	valid_0's auc: 0.900514
[4776]	valid_0's auc: 0.900519
[4777]	valid_0's auc: 0.90052
[4778]	valid_0's auc: 0.900522
[4779]	valid_0's auc: 0.900531
[4780]	valid_0's auc: 0.900524
[4781]	valid_0's auc: 0.900533
[4782]	valid_0's auc: 0.900532
[4783]	valid_0's auc: 0.900535
[4784]	valid_0's auc: 0.900541
[4785]	valid_0's auc: 0.900543
[4786]	valid_0's auc: 0.900545
[4787]	valid_0's auc: 0.900543
[4788]	valid_0's auc: 0.900542
[4789]	valid_0's auc: 0.900555
[4790]	valid_0's auc: 0.900552
[4791]	valid_0's auc: 0.900549
[4792]	valid_0's auc: 0.900543
[4793]	valid_0's auc: 0.900542
[4794]	valid_0's auc: 0.900548
[4795]	valid_0's auc: 0.900544
[4796]	valid_0's auc: 0.90055
[4797]	valid_0's auc: 0.90056
[4798]	valid_0's auc: 0.900558
[4799]	valid_0's auc: 0.900562
[4800]	valid_0's auc: 0.90057
[4801]	valid_0's auc: 0.900564
[4802]	valid_0's auc: 0.900567
[4803]	valid_0's auc: 0.900556
[4804]	valid_0's auc: 0.900549
[4805]	valid_0's auc: 0.900536
[4806]	valid_0's auc: 0.900536
[4807]	valid_0's auc: 0.900538
[4808]	valid_0's auc: 0.900541
[4809]	valid_0's auc: 0.900538
[4810]	valid_0's auc: 0.900534
[4811]	valid_0's auc: 0.900541
[4812]	valid_0's auc: 0.900537
[4813]	valid_0's auc: 0.90054
[4814]	valid_0's auc: 0.900553
[4815]	valid_0's auc: 0.900555
[4816]	valid_0's auc: 0.90056
[4817]	valid_0's auc: 0.900565
[4818]	valid_0's auc: 0.900556
[4819]	valid_0's auc: 0.900556
[4820]	valid_0's auc: 0.900553
[4821]	valid_0's auc: 0.900555
[4822]	valid_0's auc: 0.900549
[4823]	valid_0's auc: 0.90054
[4824]	valid_0's auc: 0.900542
[4825]	valid_0's auc: 0.900548
[4826]	valid_0's auc: 0.900538
[4827]	valid_0's auc: 0.900535
[4828]	valid_0's auc: 0.900537
[4829]	valid_0's auc: 0.900535
[4830]	valid_0's auc: 0.900531
[4831]	valid_0's auc: 0.900539
[4832]	valid_0's auc: 0.900543
[4833]	valid_0's auc: 0.900546
[4834]	valid_0's auc: 0.900555
[4835]	valid_0's auc: 0.900556
[4836]	valid_0's auc: 0.900574
[4837]	valid_0's auc: 0.900578
[4838]	valid_0's auc: 0.900576
[4839]	valid_0's auc: 0.900575
[4840]	valid_0's auc: 0.900587
[4841]	valid_0's auc: 0.900579
[4842]	valid_0's auc: 0.900583
[4843]	valid_0's auc: 0.900584
[4844]	valid_0's auc: 0.900592
[4845]	valid_0's auc: 0.900599
[4846]	valid_0's auc: 0.900584
[4847]	valid_0's auc: 0.900585
[4848]	valid_0's auc: 0.900595
[4849]	valid_0's auc: 0.900598
[4850]	valid_0's auc: 0.900607
[4851]	valid_0's auc: 0.900605
[4852]	valid_0's auc: 0.900602
[4853]	valid_0's auc: 0.900605
[4854]	valid_0's auc: 0.900608
[4855]	valid_0's auc: 0.900603
[4856]	valid_0's auc: 0.900609
[4857]	valid_0's auc: 0.900614
[4858]	valid_0's auc: 0.900614
[4859]	valid_0's auc: 0.90062
[4860]	valid_0's auc: 0.900615
[4861]	valid_0's auc: 0.900614
[4862]	valid_0's auc: 0.900617
[4863]	valid_0's auc: 0.900612
[4864]	valid_0's auc: 0.900611
[4865]	valid_0's auc: 0.900616
[4866]	valid_0's auc: 0.900608
[4867]	valid_0's auc: 0.900613
[4868]	valid_0's auc: 0.90061
[4869]	valid_0's auc: 0.90061
[4870]	valid_0's auc: 0.900613
[4871]	valid_0's auc: 0.900616
[4872]	valid_0's auc: 0.900626
[4873]	valid_0's auc: 0.900625
[4874]	valid_0's auc: 0.900624
[4875]	valid_0's auc: 0.900626
[4876]	valid_0's auc: 0.900618
[4877]	valid_0's auc: 0.90063
[4878]	valid_0's auc: 0.900635
[4879]	valid_0's auc: 0.900633
[4880]	valid_0's auc: 0.900636
[4881]	valid_0's auc: 0.90064
[4882]	valid_0's auc: 0.900637
[4883]	valid_0's auc: 0.900637
[4884]	valid_0's auc: 0.900644
[4885]	valid_0's auc: 0.900659
[4886]	valid_0's auc: 0.900661
[4887]	valid_0's auc: 0.900656
[4888]	valid_0's auc: 0.900657
[4889]	valid_0's auc: 0.900656
[4890]	valid_0's auc: 0.900655
[4891]	valid_0's auc: 0.900651
[4892]	valid_0's auc: 0.900662
[4893]	valid_0's auc: 0.900667
[4894]	valid_0's auc: 0.900669
[4895]	valid_0's auc: 0.900672
[4896]	valid_0's auc: 0.900664
[4897]	valid_0's auc: 0.90066
[4898]	valid_0's auc: 0.900663
[4899]	valid_0's auc: 0.90066
[4900]	valid_0's auc: 0.900662
[4901]	valid_0's auc: 0.900657
[4902]	valid_0's auc: 0.900654
[4903]	valid_0's auc: 0.900652
[4904]	valid_0's auc: 0.900644
[4905]	valid_0's auc: 0.900644
[4906]	valid_0's auc: 0.900645
[4907]	valid_0's auc: 0.900644
[4908]	valid_0's auc: 0.900648
[4909]	valid_0's auc: 0.900642
[4910]	valid_0's auc: 0.900643
[4911]	valid_0's auc: 0.90064
[4912]	valid_0's auc: 0.900648
[4913]	valid_0's auc: 0.90065
[4914]	valid_0's auc: 0.900644
[4915]	valid_0's auc: 0.900647
[4916]	valid_0's auc: 0.900647
[4917]	valid_0's auc: 0.90065
[4918]	valid_0's auc: 0.900643
[4919]	valid_0's auc: 0.900641
[4920]	valid_0's auc: 0.900649
[4921]	valid_0's auc: 0.900651
[4922]	valid_0's auc: 0.900652
[4923]	valid_0's auc: 0.900652
[4924]	valid_0's auc: 0.900655
[4925]	valid_0's auc: 0.90065
[4926]	valid_0's auc: 0.900652
[4927]	valid_0's auc: 0.900648
[4928]	valid_0's auc: 0.900655
[4929]	valid_0's auc: 0.900659
[4930]	valid_0's auc: 0.900661
[4931]	valid_0's auc: 0.900662
[4932]	valid_0's auc: 0.900669
[4933]	valid_0's auc: 0.900673
[4934]	valid_0's auc: 0.900676
[4935]	valid_0's auc: 0.900678
[4936]	valid_0's auc: 0.900676
[4937]	valid_0's auc: 0.900677
[4938]	valid_0's auc: 0.900678
[4939]	valid_0's auc: 0.90069
[4940]	valid_0's auc: 0.900694
[4941]	valid_0's auc: 0.900701
[4942]	valid_0's auc: 0.900704
[4943]	valid_0's auc: 0.900703
[4944]	valid_0's auc: 0.900704
[4945]	valid_0's auc: 0.900704
[4946]	valid_0's auc: 0.900712
[4947]	valid_0's auc: 0.900715
[4948]	valid_0's auc: 0.900719
[4949]	valid_0's auc: 0.900718
[4950]	valid_0's auc: 0.900717
[4951]	valid_0's auc: 0.900713
[4952]	valid_0's auc: 0.900707
[4953]	valid_0's auc: 0.900702
[4954]	valid_0's auc: 0.900705
[4955]	valid_0's auc: 0.900704
[4956]	valid_0's auc: 0.900712
[4957]	valid_0's auc: 0.900718
[4958]	valid_0's auc: 0.900719
[4959]	valid_0's auc: 0.900713
[4960]	valid_0's auc: 0.900722
[4961]	valid_0's auc: 0.900719
[4962]	valid_0's auc: 0.90072
[4963]	valid_0's auc: 0.900723
[4964]	valid_0's auc: 0.900725
[4965]	valid_0's auc: 0.900728
[4966]	valid_0's auc: 0.900733
[4967]	valid_0's auc: 0.900741
[4968]	valid_0's auc: 0.900745
[4969]	valid_0's auc: 0.900733
[4970]	valid_0's auc: 0.900734
[4971]	valid_0's auc: 0.900727
[4972]	valid_0's auc: 0.900733
[4973]	valid_0's auc: 0.900726
[4974]	valid_0's auc: 0.90073
[4975]	valid_0's auc: 0.900726
[4976]	valid_0's auc: 0.900725
[4977]	valid_0's auc: 0.900729
[4978]	valid_0's auc: 0.900727
[4979]	valid_0's auc: 0.900726
[4980]	valid_0's auc: 0.900724
[4981]	valid_0's auc: 0.900724
[4982]	valid_0's auc: 0.900733
[4983]	valid_0's auc: 0.90073
[4984]	valid_0's auc: 0.900729
[4985]	valid_0's auc: 0.900728
[4986]	valid_0's auc: 0.900729
[4987]	valid_0's auc: 0.900723
[4988]	valid_0's auc: 0.900724
[4989]	valid_0's auc: 0.900723
[4990]	valid_0's auc: 0.900729
[4991]	valid_0's auc: 0.900732
[4992]	valid_0's auc: 0.900728
[4993]	valid_0's auc: 0.900734
[4994]	valid_0's auc: 0.900736
[4995]	valid_0's auc: 0.900731
[4996]	valid_0's auc: 0.90073
[4997]	valid_0's auc: 0.900737
[4998]	valid_0's auc: 0.900736
[4999]	valid_0's auc: 0.900742
[5000]	valid_0's auc: 0.900751
[5001]	valid_0's auc: 0.900753
[5002]	valid_0's auc: 0.900746
[5003]	valid_0's auc: 0.900745
[5004]	valid_0's auc: 0.900745
[5005]	valid_0's auc: 0.900746
[5006]	valid_0's auc: 0.900747
[5007]	valid_0's auc: 0.900754
[5008]	valid_0's auc: 0.900755
[5009]	valid_0's auc: 0.900755
[5010]	valid_0's auc: 0.900751
[5011]	valid_0's auc: 0.900759
[5012]	valid_0's auc: 0.900771
[5013]	valid_0's auc: 0.90078
[5014]	valid_0's auc: 0.900776
[5015]	valid_0's auc: 0.900778
[5016]	valid_0's auc: 0.900781
[5017]	valid_0's auc: 0.900776
[5018]	valid_0's auc: 0.900777
[5019]	valid_0's auc: 0.900778
[5020]	valid_0's auc: 0.90078
[5021]	valid_0's auc: 0.900786
[5022]	valid_0's auc: 0.900784
[5023]	valid_0's auc: 0.900792
[5024]	valid_0's auc: 0.900789
[5025]	valid_0's auc: 0.900799
[5026]	valid_0's auc: 0.900796
[5027]	valid_0's auc: 0.900795
[5028]	valid_0's auc: 0.900794
[5029]	valid_0's auc: 0.90078
[5030]	valid_0's auc: 0.90078
[5031]	valid_0's auc: 0.900779
[5032]	valid_0's auc: 0.90078
[5033]	valid_0's auc: 0.900773
[5034]	valid_0's auc: 0.900772
[5035]	valid_0's auc: 0.900773
[5036]	valid_0's auc: 0.900771
[5037]	valid_0's auc: 0.900773
[5038]	valid_0's auc: 0.900771
[5039]	valid_0's auc: 0.900774
[5040]	valid_0's auc: 0.900773
[5041]	valid_0's auc: 0.90078
[5042]	valid_0's auc: 0.900773
[5043]	valid_0's auc: 0.900778
[5044]	valid_0's auc: 0.900774
[5045]	valid_0's auc: 0.900773
[5046]	valid_0's auc: 0.90077
[5047]	valid_0's auc: 0.900767
[5048]	valid_0's auc: 0.900766
[5049]	valid_0's auc: 0.900775
[5050]	valid_0's auc: 0.900772
[5051]	valid_0's auc: 0.900781
[5052]	valid_0's auc: 0.900781
[5053]	valid_0's auc: 0.900779
[5054]	valid_0's auc: 0.900776
[5055]	valid_0's auc: 0.900781
[5056]	valid_0's auc: 0.90078
[5057]	valid_0's auc: 0.900781
[5058]	valid_0's auc: 0.900778
[5059]	valid_0's auc: 0.90078
[5060]	valid_0's auc: 0.900774
[5061]	valid_0's auc: 0.900777
[5062]	valid_0's auc: 0.900776
[5063]	valid_0's auc: 0.900784
[5064]	valid_0's auc: 0.900782
[5065]	valid_0's auc: 0.900782
[5066]	valid_0's auc: 0.900788
[5067]	valid_0's auc: 0.900782
[5068]	valid_0's auc: 0.90079
[5069]	valid_0's auc: 0.900786
[5070]	valid_0's auc: 0.900786
[5071]	valid_0's auc: 0.900789
[5072]	valid_0's auc: 0.900796
[5073]	valid_0's auc: 0.900792
[5074]	valid_0's auc: 0.900792
[5075]	valid_0's auc: 0.900794
Early stopping, best iteration is:
[5025]	valid_0's auc: 0.900799
Done Training.
#+end_example

#+BEGIN_SRC python :session :results file
# feature importances
# print('Feature importances:', list(gbm.feature_importance()))
from matplotlib import pyplot as plt

lgb.plot_importance(gbm, max_num_features=20)
plt.savefig('feature_importance.png')
plt.close 
'feature_importance.png'
#+END_SRC

#+RESULTS:
[[file:feature_importance.png]]
** Frequency features

To try get our model to notice the importance of repeated values we can try adding new features where every dataframe entry is replaced by its count in train, or its count in each column of train (we can ignore an overall normalization factor as we'll probably rescale things anyway). One issue: we need to be able to do this on unseen data, which may not include the exact same numbers that are seen in train. Perhaps we can tell new entries to be mapped to the closest entry that has been seen before in train. We could also try binning.

#+BEGIN_SRC python :session :results output
class FrequencyFeatures:
    """
    Tools to create and apply a lookup table that maps dataframe entries to the frequency of those entries in the training data. Frequencies are normalized so that maximum frequency = 1.

    Attributes:
        lookup (pandas dataframe): Dataframe were indices are entries, values are corresponding frequencies.
        decimals (int): Number of decimal places of desired precision when rounding and binning for 'fit' and 'transform'
    """

    def __init__(self):
        self.lookup = None
        self.decimals = None

    def bin_as_int(self, dataframe, decimals):
        """
        Rounds dataframe entries to desired level of precision (as a form of binning), multiplies by appropriate power of 10 and converts to ints to avoid problems with floating point errors later.

        Args:
            dataframe (pandas dataframe): Dataframe to be binned as int.
            decimals (int): Number of decimal places of desired precision when rounding.

        Returns:
            pandas dataframe of ints
        """
        # multiply 'dataframe' by 10 to the power 'decimals'
        binned_dataframe = dataframe*(10**decimals)

        # round away all decimal places and convert to int
        binned_dataframe = binned_dataframe.round(decimals=0).astype(int)

        return binned_dataframe

    def fit(self, train_dataframe, decimals):
        """
        Creates frequency lookup dataframe from training data and stores it as a class variable, so that it can be used to transform any dataframe. Frequencies are normalized so that max frequency = 1.

        Args:
            train_dataframe (pandas dataframe): Dataframe to use to create lookup table.
            decimals (int): Number of decimal places of desired precision when rounding.

        Returns:
            None
        """
        # store decimals as class variable to be used later in 'transform'
        self.decimals = decimals

        # bin train_dataframe into bins of int type
        binned_dataframe = self.bin_as_int(dataframe=train_dataframe, decimals=self.decimals)

        # flatten 'binned_dataframe' into a single series, then count values and sort by values
        lookup = binned_dataframe.melt()['value'].value_counts().sort_index()

        # normalize by maximum
        lookup = lookup/lookup.max()

        # cut off very small values by rounding, then taking nonzero entries. Reduces size of lookup dataframe
        # choose convention of 4 decimal places, because this is level of precision of raw data in this problem
        lookup = lookup.round(decimals=4)
        lookup = lookup[lookup != 0]

        # linearly interpolate any missing rows, so that we can handle any unseen data
        # create new index with no missing values from first index to last
        filled_index = pd.RangeIndex(start=lookup.index[0], stop=lookup.index[-1] + 1)

        # fill in all missing indices with NaN
        lookup = lookup.reindex(index=filled_index)

        # replace NaNs with linear interpolated values
        lookup = lookup.interpolate()

        # store as a class variable
        self.lookup = lookup

        return None

    def transform(self, dataframe):
        """
        Creates a dataframe from 'dataframe' where entries have been mapped to frequencies according to 'self.lookup'.

        Args:
            dataframe (pandas dataframe): Dataframe to be transformed.

        Returns:
            Dataframe transformed according to 'self.lookup'.
        """
        dataframe = self.bin_as_int(dataframe=dataframe, decimals=self.decimals)

        # transform lol according to lookup, then replace any missing values with zero (these will be on the tails of the lookup distribution)
        # applying transformation column-wise using .apply() and .map() is a performance improvement vs. other methods such as using .replace()
        dataframe = dataframe.apply(lambda x: x.map(self.lookup)).fillna(value=0)

        return dataframe
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session :results output
train_copy = train.copy()
validation_copy = validation.copy()

# fit and transform
frequency_features = FrequencyFeatures()

# debating between round data to one or two decimal places. we'll start imprecise for performance, then go from there
# number of decimal places we'll round everything to
decimals = 0

frequency_features.fit(train_dataframe=train_copy, decimals=decimals)

train_copy = frequency_features.transform(dataframe=train_copy)
validation_copy = frequency_features.transform(dataframe=validation_copy)

# # we should scale the original features before including
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

train_scaled = scaler.fit_transform(train)
train_scaled = pd.DataFrame(train_scaled)

print(train_scaled.head(), train_scaled.describe(), sep='\n')

validation_scaled = scaler.transform(validation)
validation_scaled = pd.DataFrame(validation_scaled)

print(train_scaled.shape, validation_scaled.shape, train_copy.shape, validation_copy.shape)

# Include the original features
train_copy = pd.concat([train_scaled.reset_index(drop=True), train_copy.reset_index(drop=True)], axis=1)
validation_copy = pd.concat([validation_scaled.reset_index(drop=True), validation_copy.reset_index(drop=True)], axis=1)

# train_copy = pd.concat([train.reset_index(drop=True), train_copy.reset_index(drop=True)], axis=1)
# validation_copy = pd.concat([validation.reset_index(drop=True), validation_copy.reset_index(drop=True)], axis=1)

# convert to LightGBM format
# train_copy = lgb.Dataset(train_copy, train_labels)
# validation_copy = lgb.Dataset(validation_copy,
#                               validation_labels,
#                               reference=train_copy)
#+END_SRC

#+RESULTS:
#+begin_example
0         1         2         3    ...       196       197       198       199
0  0.679202  0.606589  0.286890  0.425838  ...  0.598541  0.380640  0.307283  0.695027
1  0.682793  0.390667  0.550108  0.561583  ...  0.436516  0.611818  0.490809  0.560944
2  0.454195  0.275065  0.724157  0.705938  ...  0.316621  0.339824  0.433124  0.748007
3  0.384626  0.671790  0.366270  0.508554  ...  0.808417  0.671039  0.575056  0.608577
4  0.621095  0.605432  0.356024  0.565718  ...  0.295312  0.508894  0.197697  0.448978

[5 rows x 200 columns]
                 0              1    ...            198            199
count  128000.000000  128000.000000  ...  128000.000000  128000.000000
mean        0.511663       0.521669  ...       0.484130       0.535135
std         0.154518       0.161576  ...       0.152122       0.157123
min         0.000000       0.000000  ...       0.000000       0.000000
25%         0.398816       0.397781  ...       0.380954       0.416170
50%         0.503654       0.522315  ...       0.487338       0.542872
75%         0.617193       0.640902  ...       0.594925       0.658002
max         1.000000       1.000000  ...       1.000000       1.000000

[8 rows x 200 columns]
(128000, 200) (32000, 200) (128000, 200) (32000, 200)
#+end_example
 
Plot binned relative frequency distribution (normalized by diving by max)

#+BEGIN_SRC python :session :results file
frequency_features.lookup.plot()
plt.savefig('frequencies.png')
plt.close()
'frequencies.png'
#+END_SRC

#+RESULTS:
[[file:frequencies.png]]


#+BEGIN_SRC python :session :results output
# Train the model
random_state = 42

# 13 leaves .10 features works well for just frequency features
# 4 leaves, .50 features works well for frequency + original features
params = {
    # default num_trees=100
    'num_trees': 20000,
    'objective': 'binary',
    'metric': 'auc',
    'num_leaves': 4,
    'learning_rate': 0.02,
    'boost_from_average': 'false',
    # Percentage of features to be used for each tree
    'feature_fraction': 0.50,
    'min_data_in_leaf': 80,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.4,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 7,
    'bagging_seed' : random_state,
    'seed': random_state
}

print('Starting training...')

# train
frequencies_model = lgb.train(params,
                          train_copy,
                          valid_sets=validation_copy,
                          early_stopping_rounds=50)

print('Done Training.')
#+END_SRC


Plot feature importance. LightGBM still doesn't seem to care about any new features

#+BEGIN_SRC python :session :results file
# feature importances
# print('Feature importances:', list(gbm.feature_importance()))

lgb.plot_importance(frequencies_model, max_num_features=40)
plt.savefig('feature_importance.png')
plt.close 
'feature_importance.png'
#+END_SRC

#+RESULTS:
[[file:feature_importance.png]]


Predictions:

#+BEGIN_SRC python :session :results output
# predictions with original features
predictions_original = original_model.predict(test, num_iteration=original_model.best_iteration)

# transform the test data so that the one_hot model can make predictions
test_copy = test.copy()
test_copy = frequency_features.transform(test_copy)

# scale
# test_scaled = scaler.transform(test)
# test_scaled = pd.DataFrame(test_scaled)

# Include the original features
test_copy = pd.concat([test.reset_index(drop=True), test_copy.reset_index(drop=True)], axis=1)

# predictions with frequencies model 
predictions_frequencies = frequencies_model.predict(test_copy, num_iteration=frequencies_model.best_iteration)
#+END_SRC

#+RESULTS:

Ensembling predictions. Still struggling to see any improvement from ensembling.

#+BEGIN_SRC python :session :results output
predictions = (predictions_original + predictions_frequencies)/2

print(roc_auc_score(test_labels, predictions_original))
print(roc_auc_score(test_labels, predictions_frequencies))
print(roc_auc_score(test_labels, predictions))
#+END_SRC

#+RESULTS:
: 0.8928207149484219
: 0.8893392523042765
: 0.8917075794821049

** NN with Frequency features

Use train_copy and validation_copy from previous section

#+BEGIN_SRC python :session :results output
import tensorflow as tf
from tensorflow import keras
#+END_SRC

#+RESULTS:
: 2020-02-20 23:12:13.582173: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

#+BEGIN_SRC python :session :results output
# Tensorflow model needs numpy arrays as inputs:
train_copy = np.array(train_copy)
validation_copy = np.array(validation_copy)

train_labels_tf = np.array(train_labels)
validation_labels_tf = np.array(validation_labels)
#+END_SRC

#+RESULTS:
: 2020-02-20 01:09:12.629846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

Results: 
val_auc 0.8631, 50 epochs, 512 architecture, no early stop, lr 0.001
val_auc 0.8623, 50 epochs, 16 architecture, no early stop, lr 0.01
val_auc 0.8645, 100 epochs, 16 architecture, no early stop, lr 0.01
val_auc 0.8655, loss 0.4707, 100 epochs, 256 architecture, no early stop
val_auc 0.8652, loss 0.4719, 100 epochs, 128 architecture, no early stop, 4 layers
val_auc 0.8652, loss 0.4741, 100 epochs, 128 architecture, no early stop, 5 layers
val_auc 0.8650, loss 0.4709, 100 epochs, 128 architecture, no early stop, 5 layers
val_auc 0.8647, loss 0.4749, 100 epochs, 128 architecture, no early stop, 6 layers
val_auc 0.8654, loss 0.4717, 100 epochs, 128 architecture, no early stop, 3 layers

#+BEGIN_SRC python :session :results output
# clear keras session so that we can rerun without errors
tf.keras.backend.clear_session()

METRICS = [
    keras.metrics.TruePositives(name='tp'),
    keras.metrics.FalsePositives(name='fp'),
    keras.metrics.TrueNegatives(name='tn'),
    keras.metrics.FalseNegatives(name='fn'),
    keras.metrics.BinaryAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='auc'),
]

N_TRAIN = train_copy.shape[0]
BATCH_SIZE = 2048
EPOCHS = 500 
STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE

# we've been struggling with overfitting, so take some ideas from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit

# learning rate decay
# initial learning rate 
initial_rate = 0.001

# 'decay_factor' = x means learning rate decays to 1/2 of the 'initial_rate' after x epochs, 1/3 after 2x epochs, etc.
decay_factor = 500

lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
    initial_rate,
    decay_steps=STEPS_PER_EPOCH*decay_factor,
    decay_rate=1,
    staircase=False)

def get_optimizer():
    return tf.keras.optimizers.Adam(lr_schedule)

# model architecture
def make_model(train_features, metrics=METRICS):
    model = keras.Sequential([
        keras.layers.Dense(128,
                           kernel_regularizer=keras.regularizers.l2(0.0001),
                           activation='elu',
                           input_shape=(train_features.shape[-1],)),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(128, kernel_regularizer=keras.regularizers.l2(0.0001),
                     activation='elu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(128, kernel_regularizer=keras.regularizers.l2(0.0001),
                     activation='elu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(128, kernel_regularizer=keras.regularizers.l2(0.0001),
                     activation='elu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(1, activation='sigmoid')
        ])

    model.compile(
        optimizer=get_optimizer(),
        loss=keras.losses.BinaryCrossentropy(),
        metrics=metrics)
    return model

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc',
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)

# initialize the model
model = make_model(train_copy)

# Let's try with class weights
num_pos = np.count_nonzero(train_labels_tf)
num_neg = N_TRAIN - num_pos
class_weight = {0: N_TRAIN/(2.0*num_neg) , 1: N_TRAIN/(2.0*num_pos)}

# train
# Features and labels input as numpy arrays
model_history = model.fit(
    train_copy,
    train_labels_tf,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    # callbacks=[early_stopping],
    validation_data=(validation_copy, validation_labels_tf))
    # class_weight=class_weight)
#+END_SRC


Define function to plot metrics:

#+BEGIN_SRC python :session :results output
def plot_metrics(history):
    metrics =  ['loss', 'auc', 'precision', 'recall']
    plt.rcParams['figure.figsize'] = (12, 10)
    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
    plt.figure(figsize=(6,4))

    for n, metric in enumerate(metrics):
        name = metric.replace("_"," ").capitalize()
        plt.subplot(2,2,n+1)
        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')
        plt.plot(history.epoch, history.history['val_'+metric], color=colors[0], linestyle="--", label='Val')
        plt.xlabel('Epoch')
        plt.ylabel(name)
        if metric == 'loss':
            plt.ylim([0, plt.ylim()[1]])
        elif metric == 'auc':
            plt.ylim([0.8,1])
        else:
            plt.ylim([0,1])

    plt.legend()
    plt.savefig('metrics.png')
    plt.close()
#+END_SRC

#+RESULTS:

Plot metrics:

#+BEGIN_SRC python :session :results file
# Display metrics plot
plot_metrics(model_history)
'metrics.png'
#+END_SRC

#+RESULTS:
[[file:metrics.png]]

We can run the baseline lightgbm from the previous subheading. Predictions:

#+BEGIN_SRC python :session :results output
# predictions with original features
predictions_original = original_model.predict(test, num_iteration=original_model.best_iteration)

# transform the test data so that the frequency + original model can make predictions
test_copy = test.copy()
test_copy

# Tensorflow wants numpy arrays
test_copy = np.array(test_copy)

# predictions with one-hot model (note the array shape of the tensorflow predictions)
predictions_nn = model.predict(test_copy)[:,0]
#+END_SRC

Ensembling predictions

#+BEGIN_SRC python :session :results output
predictions = np.multiply(predictions_original, predictions_nn)

print(roc_auc_score(test_labels, predictions))
#+END_SRC

#+RESULTS:
** Class balancing

We'll do special train/test/val splits here so that val is smaller than train after augmenting train.

#+BEGIN_SRC python :results output :tangle lol
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import lightgbm as lgb
## don't want to import tensorflow unless we need to
# import tensorflow as tf
# from tensorflow import keras
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session :results output
train = pd.read_csv('train.csv')

# The ID_code column contains no information, so we remove it
train.drop('ID_code', axis=1, inplace=True)

train, validation = train_test_split(train, test_size=0.2)

train_labels = train.pop('target')
validation_labels = validation.pop('target')
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session :results output
def change_class_balance(train_dataframe, train_labels, oversample_rate, majority_class_ratio):
    """
    Rebalance classes in the training data by oversampling classes with label 1 (duplicating, with replacement) and undersampling classes with label 0 (without replacement).

    Args:
        train_dataframe (pandas dataframe): Training data.
        train_labels (pandas dataframe): Training data labels.
        oversample_rate (float): Percentage of class 1 examples to duplicate.
        majority_class_ratio (float): Multiplies class 1 size to give class 0 size.

    Returns:
        Dataframe with balanced classes, Dataframe with corresponding labels
    """
    train_augmented = train_dataframe.copy()
    train_augmented['target'] = train_labels

    pos_examples = train_augmented.loc[train_augmented['target']==1]
    neg_examples = train_augmented.loc[train_augmented['target']==0]

    # oversample positive examples with replacement, according to 'oversample_rate'
    num_pos = len(pos_examples.index)
    num_additional_pos_examples = int(oversample_rate*num_pos)

    # duplicate examples to be added
    duplicate_pos_examples = pos_examples.sample(n=num_additional_pos_examples, replace=True)

    # combine duplicate positive examples with original positive examples
    pos_examples = pd.concat([pos_examples, duplicate_pos_examples])

    # choose class balance by undersampling the same number of negative examples as augmented positive examples, without replacement
    num_neg = int(majority_class_ratio*len(pos_examples.index))
    neg_examples = neg_examples.sample(n=num_neg, replace=False)

    # combine positive and negative examples, shuffle, reindex, split off targets
    train_augmented = pd.concat([pos_examples, neg_examples])
    train_augmented = train_augmented.sample(frac=1, replace=False).reset_index(drop=True)

    train_labels_augmented = train_augmented.pop('target')

    return train_augmented, train_labels_augmented

# prepare for tensorflow in previous section
train_copy = train.copy()
validation_copy = validation.copy()

train_labels_tf = train_labels.copy()
validation_labels_tf = validation_labels.copy()

majority_class_ratio = 7.0
oversample_rate = 0.25

# train_copy, train_labels_tf = change_class_balance(train_dataframe=train_copy,
#                                                    train_labels=train_labels_tf,
#                                                    oversample_rate=oversample_rate,
#                                                    majority_class_ratio=majority_class_ratio)

print(train_copy.shape, validation_copy.shape)
#+END_SRC

#+RESULTS:
: (160000, 200) (40000, 200)

Run this cell if we want to include frequency features (need to run class in previous subtree).

#+BEGIN_SRC python :session :results output
# # we should scale the original features before including

scaler = MinMaxScaler()

# need copies of the already preprocessed data
train_scaled = train_copy.copy()
validation_scaled = validation_copy.copy()

# fit and transform, convert to dataframe
train_scaled = scaler.fit_transform(train_scaled)
train_scaled = pd.DataFrame(train_scaled).reset_index(drop=True)

validation_scaled = scaler.transform(validation_scaled)
validation_scaled = pd.DataFrame(validation_scaled).reset_index(drop=True)

# fit and transform
frequency_features = FrequencyFeatures()

# seem to get best results round to 0 decimal places
# number of decimal places we'll round everything to
decimals = 4

# fit on raw train data, not data that has been preprocessed in other ways
frequency_features.fit(train_dataframe=train, decimals=decimals)

train_copy = frequency_features.transform(dataframe=train_copy).reset_index(drop=True)
validation_copy = frequency_features.transform(dataframe=validation_copy).reset_index(drop=True)

# Include the original features (we reset the index of everything earlier)
train_copy = pd.concat([train_scaled, train_copy], axis=1)
validation_copy = pd.concat([validation_scaled, validation_copy], axis=1)

print(train_copy.shape, validation_copy.shape)
#+END_SRC

#+RESULTS:
: (160000, 400) (40000, 400)

Train NN

#+BEGIN_SRC python :session :results output
import tensorflow as tf
from tensorflow import keras
#+END_SRC

#+RESULTS:
: 2020-02-21 20:13:31.563163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

#+BEGIN_SRC python :session :results output
# tensorflow needs numpy arrays
train_copy = np.array(train_copy)
validation_copy = np.array(validation_copy)

train_labels_tf = np.array(train_labels_tf)
validation_labels_tf = np.array(validation_labels_tf)
#+END_SRC

#+RESULTS:

Reshape so that new features are learned to be related to old features (goal: input shape

#+BEGIN_SRC python :session :results output
def pair_features(array):
    """
    Pairs each original feature value with the frequency feature value made from it.
    
    Args:
        array (ndarray): Numpy array of shape (number of data points, 400)

    Returns:
        Reshaped numpy array of shap (number of data points, 200, 2), where the first column of the old array is paired with the 200th column of the old array, 2nd column paired with 201st column, etc.
    """
    print("Initial shape ", array.shape)

    # first 200 features form row 1, last 200 features form row 2, for each data point
    array = np.reshape(array, (-1, 2, 200))

    # transpose only axis 2 and 3 so that each data point is transposed
    array = np.transpose(array, (0,2,1))

    print("Final shape ", array.shape)

    return array

train_copy = pair_features(train_copy)
validation_copy = pair_features(validation_copy)
#+END_SRC

#+RESULTS:
: Initial shape  (160000, 400)
: Final shape  (160000, 200, 2)
: Initial shape  (40000, 400)
: Final shape  (40000, 200, 2)
 

Results:
oversample_rate 0.25, majority_class_ratio 7.0
auc 0.8678, precis 0.5984, recall 0.3963
tp 1645, fp 1104

no over or undersampling
auc 0.8707, precis 0.6978, recall 0.3 
tp 1180, fp 511

#+BEGIN_SRC python :session :results output
# clear keras session so that we can rerun without errors
tf.keras.backend.clear_session()

METRICS = [
    keras.metrics.TruePositives(name='tp'),
    keras.metrics.FalsePositives(name='fp'),
    keras.metrics.TrueNegatives(name='tn'),
    keras.metrics.FalseNegatives(name='fn'),
    keras.metrics.BinaryAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='auc'),
]

N_TRAIN = train_copy.shape[0]
BATCH_SIZE = 64 
EPOCHS = 100 
STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE

# we've been struggling with overfitting, so take some ideas from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit

# learning rate decay
# initial learning rate 
initial_rate = 0.01

# 'decay_factor' = x means learning rate decays to 1/2 of the 'initial_rate' after x epochs, 1/3 after 2x epochs, etc.
decay_factor = 10

lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
    initial_rate,
    decay_steps=STEPS_PER_EPOCH*decay_factor,
    decay_rate=1,
    staircase=False)

def get_optimizer():
    return tf.keras.optimizers.Adam(lr_schedule, clipnorm=1.0)

# model architecture
def make_model(train_features, metrics=METRICS):
    model = keras.Sequential([
        # Note input shape (200, 2) when we pair features with frequency features
        keras.layers.Dense(64,
                           kernel_regularizer=keras.regularizers.l2(0.0001),
                           activation='elu',
                           input_shape=(train_features.shape[1], train_features.shape[2])),
        keras.layers.Dense(32, kernel_regularizer=keras.regularizers.l2(0.0001),
                     activation='elu'),
        # keras.layers.Dense(128, kernel_regularizer=keras.regularizers.l2(0.0001),
        #              activation='elu'),
        # keras.layers.Dropout(0.5),
        # keras.layers.Dense(128, kernel_regularizer=keras.regularizers.l2(0.0001),
        #              activation='elu'),
        # keras.layers.Dropout(0.5),
        # keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.0001),
        #              activation='elu'),
        # keras.layers.Dropout(0.4),
        # keras.layers.Dense(32, kernel_regularizer=keras.regularizers.l2(0.0001),
        #              activation='elu'),
        # keras.layers.Dropout(0.3),
        # keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.0001),
        #              activation='elu'),
        # keras.layers.Dropout(0.2),
        # keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.0001),
        #              activation='elu'),
        # keras.layers.Dropout(0.2),
        # keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.0001),
        #              activation='elu'),
        # keras.layers.Dropout(0.2),
        keras.layers.Flatten(),
        keras.layers.Dense(1, activation='sigmoid')
        ])

    model.compile(
        optimizer=get_optimizer(),
        loss=keras.losses.BinaryCrossentropy(),
        metrics=metrics)
    return model

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc',
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)

# initialize the model
model = make_model(train_copy)

# Let's try with class weights
# num_pos = np.count_nonzero(train_labels_tf)
# num_neg = N_TRAIN - num_pos
# class_weight = {0: N_TRAIN/(2.0*num_neg) , 1: N_TRAIN/(2.0*num_pos)}

# train
# Features and labels input as numpy arrays
model_history = model.fit(
    train_copy,
    train_labels_tf,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    # callbacks=[early_stopping],
    validation_data=(validation_copy, validation_labels_tf))
    # class_weight=class_weight)
#+END_SRC

Define function to plot metrics:

#+BEGIN_SRC python :session :results output
def plot_metrics(history):
    metrics =  ['loss', 'auc', 'precision', 'recall']
    plt.rcParams['figure.figsize'] = (12, 10)
    plt.figure(figsize=(6,4))

    for n, metric in enumerate(metrics):
        name = metric.replace("_"," ").capitalize()
        plt.subplot(2,2,n+1)
        plt.plot(history.epoch,  history.history[metric], color='b', label='Train')
        plt.plot(history.epoch, history.history['val_'+metric], color='g', linestyle="--", label='Val')
        plt.xlabel('Epoch')
        plt.ylabel(name)
        if metric == 'loss':
            plt.ylim([0, plt.ylim()[1]])
        elif metric == 'auc':
            plt.ylim([0.8,1])
        else:
            plt.ylim([0,1])

    plt.legend()
    plt.savefig('metrics.png')
    plt.close()
#+END_SRC

#+RESULTS:

Plot metrics:

#+BEGIN_SRC python :session :results file
# Display metrics plot
plot_metrics(model_history)
'metrics.png'
#+END_SRC

#+RESULTS:
[[file:metrics.png]]

We can run the baseline lightgbm from the previous subheading. 
Train the model:

#+BEGIN_SRC python :session :results output
# create dataset for lightgbm
lgb_train = lgb.Dataset(train_copy, train_labels_tf)
lgb_eval = lgb.Dataset(validation_copy,
                       validation_labels_tf,
                       reference=lgb_train)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session :results output
# Train the model
random_state = 42

# old info, don't know if true for augmented data:
# 13 leaves .10 features works well for just frequency features
# 4 leaves, .50 features works well for frequency + original features
params = {
    # default num_trees=100
    'num_trees': 20000,
    'objective': 'binary',
    'metric': 'auc',
    'num_leaves': 13,
    'learning_rate': 0.05,
    'boost_from_average': 'false',
    # Percentage of features to be used for each tree
    'feature_fraction': 0.70,
    'min_data_in_leaf': 80,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.4,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 7,
    'bagging_seed' : random_state,
    'seed': random_state
}

print('Starting training...')

# train
frequencies_model = lgb.train(params,
                          lgb_train,
                          valid_sets=lgb_eval,
                          early_stopping_rounds=50)

print('Done Training.')
#+END_SRC

Ensemble predictions:

#+BEGIN_SRC python :session :results output
# predictions with original features
predictions_original = original_model.predict(test, num_iteration=original_model.best_iteration)

# transform the test data so that the frequency + original model can make predictions
test_copy = test.copy()
test_copy

# Tensorflow wants numpy arrays
test_copy = np.array(test_copy)

# predictions with one-hot model (note the array shape of the tensorflow predictions)
predictions_nn = model.predict(test_copy)[:,0]
#+END_SRC

Ensembling predictions

#+BEGIN_SRC python :session :results output
predictions = np.multiply(predictions_original, predictions_nn)

print(roc_auc_score(test_labels, predictions))

* DONE Things I will do before calling project end
CLOSED: [2020-03-26 Thu 17:39]
** DONE Latest NN with augmentation vs. with no augmentation
CLOSED: [2020-03-26 Thu 17:40]
** DONE Latest NN + stat features
CLOSED: [2020-03-26 Thu 17:40]
** DONE Plot NN ROC, LightGBM ROC, Light GBM precision, recall, loss, confusion matrix
CLOSED: [2020-03-26 Thu 17:40]
** DONE NN to combine model predictions on validation of NN + Lightgbm, validated against test. Possible feature engineering: max, min, multiply, different weighted means, monomial combinations
CLOSED: [2020-03-26 Thu 17:40]
* Kaggle submissions 
** LightGBM baseline

#+BEGIN_SRC python :session :results output
import pandas as pd
from sklearn.model_selection import train_test_split
import lightgbm as lgb

train = pd.read_csv('train.csv')

# The ID_code column contains no information, so we remove it
train.drop('ID_code', axis=1, inplace=True)

train, validation = train_test_split(train, test_size=0.2)

train_labels = train.pop('target')
validation_labels = validation.pop('target')
#+END_SRC

#+RESULTS:
: Python 3.8.1 (default, Jan 21 2020, 20:27:39) 
: [GCC 9.2.0] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: >>> python.el: native completion setup loaded

Train the model

#+BEGIN_SRC python :session :results output
# create dataset for lightgbm
lgb_train = lgb.Dataset(train, train_labels)
lgb_eval = lgb.Dataset(validation,
                       validation_labels,
                       reference=lgb_train)

random_state = 42

params = {
    # default num_trees=100
    'num_trees': 10000,
    'objective': 'binary',
    'metric': 'auc',
    'num_leaves': 4,
    'learning_rate': 0.02,
    'boost_from_average': 'false',
    # Percentage of features to be used for each tree
    'feature_fraction': 0.10,
    'min_data_in_leaf': 80,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.4,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 7,
    'bagging_seed' : random_state,
    'seed': random_state
}

print('Starting training...')

# train
gbm = lgb.train(params,
                lgb_train,
                valid_sets=lgb_eval,
                early_stopping_rounds=50)

print('Done Training.')
#+END_SRC

#+RESULTS:
#+begin_example
Starting training...
/home/jonathan/.pyenv/versions/tensorflow_env/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_trees` in params. Will use it instead of argument
  warnings.warn("Found `{}` in params. Will use it instead of argument".format(alias))
[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true.
This may cause significantly different results comparing to the previous versions of LightGBM.
Try to set boost_from_average=false, if your old models produce bad results
[LightGBM] [Info] Number of positive: 16087, number of negative: 143913
[LightGBM] [Info] Total Bins 51000
[LightGBM] [Info] Number of data: 160000, number of used features: 200
[1]	valid_0's auc: 0.560895
Training until validation scores don't improve for 50 rounds
[2]	valid_0's auc: 0.608177
[3]	valid_0's auc: 0.64031
[4]	valid_0's auc: 0.663038
[5]	valid_0's auc: 0.674875
[6]	valid_0's auc: 0.6866
[7]	valid_0's auc: 0.700027
[8]	valid_0's auc: 0.708123
[9]	valid_0's auc: 0.71292
[10]	valid_0's auc: 0.723843
[11]	valid_0's auc: 0.728221
[12]	valid_0's auc: 0.734828
[13]	valid_0's auc: 0.738261
[14]	valid_0's auc: 0.733998
[15]	valid_0's auc: 0.733443
[16]	valid_0's auc: 0.739398
[17]	valid_0's auc: 0.740679
[18]	valid_0's auc: 0.741655
[19]	valid_0's auc: 0.743321
[20]	valid_0's auc: 0.74318
[21]	valid_0's auc: 0.74234
[22]	valid_0's auc: 0.749551
[23]	valid_0's auc: 0.750411
[24]	valid_0's auc: 0.749871
[25]	valid_0's auc: 0.753734
[26]	valid_0's auc: 0.751221
[27]	valid_0's auc: 0.751594
[28]	valid_0's auc: 0.752831
[29]	valid_0's auc: 0.753962
[30]	valid_0's auc: 0.755394
[31]	valid_0's auc: 0.758938
[32]	valid_0's auc: 0.759328
[33]	valid_0's auc: 0.763101
[34]	valid_0's auc: 0.762905
[35]	valid_0's auc: 0.761792
[36]	valid_0's auc: 0.760855
[37]	valid_0's auc: 0.761879
[38]	valid_0's auc: 0.764892
[39]	valid_0's auc: 0.766515
[40]	valid_0's auc: 0.764225
[41]	valid_0's auc: 0.76397
[42]	valid_0's auc: 0.76616
[43]	valid_0's auc: 0.763574
[44]	valid_0's auc: 0.762329
[45]	valid_0's auc: 0.764259
[46]	valid_0's auc: 0.76666
[47]	valid_0's auc: 0.767655
[48]	valid_0's auc: 0.769084
[49]	valid_0's auc: 0.770202
[50]	valid_0's auc: 0.769536
[51]	valid_0's auc: 0.767721
[52]	valid_0's auc: 0.769381
[53]	valid_0's auc: 0.768109
[54]	valid_0's auc: 0.769157
[55]	valid_0's auc: 0.768078
[56]	valid_0's auc: 0.766274
[57]	valid_0's auc: 0.766645
[58]	valid_0's auc: 0.765348
[59]	valid_0's auc: 0.767623
[60]	valid_0's auc: 0.767452
[61]	valid_0's auc: 0.769032
[62]	valid_0's auc: 0.769745
[63]	valid_0's auc: 0.771556
[64]	valid_0's auc: 0.771753
[65]	valid_0's auc: 0.773074
[66]	valid_0's auc: 0.774675
[67]	valid_0's auc: 0.773874
[68]	valid_0's auc: 0.774099
[69]	valid_0's auc: 0.774991
[70]	valid_0's auc: 0.775617
[71]	valid_0's auc: 0.776118
[72]	valid_0's auc: 0.777216
[73]	valid_0's auc: 0.778714
[74]	valid_0's auc: 0.780105
[75]	valid_0's auc: 0.780284
[76]	valid_0's auc: 0.781414
[77]	valid_0's auc: 0.781895
[78]	valid_0's auc: 0.783046
[79]	valid_0's auc: 0.780934
[80]	valid_0's auc: 0.781875
[81]	valid_0's auc: 0.780205
[82]	valid_0's auc: 0.779513
[83]	valid_0's auc: 0.780092
[84]	valid_0's auc: 0.780725
[85]	valid_0's auc: 0.781368
[86]	valid_0's auc: 0.781362
[87]	valid_0's auc: 0.781483
[88]	valid_0's auc: 0.782086
[89]	valid_0's auc: 0.78367
[90]	valid_0's auc: 0.783879
[91]	valid_0's auc: 0.784958
[92]	valid_0's auc: 0.784924
[93]	valid_0's auc: 0.782592
[94]	valid_0's auc: 0.783248
[95]	valid_0's auc: 0.782185
[96]	valid_0's auc: 0.780274
[97]	valid_0's auc: 0.781061
[98]	valid_0's auc: 0.782295
[99]	valid_0's auc: 0.783098
[100]	valid_0's auc: 0.783475
[101]	valid_0's auc: 0.783081
[102]	valid_0's auc: 0.781291
[103]	valid_0's auc: 0.782868
[104]	valid_0's auc: 0.783415
[105]	valid_0's auc: 0.783657
[106]	valid_0's auc: 0.782608
[107]	valid_0's auc: 0.783486
[108]	valid_0's auc: 0.78387
[109]	valid_0's auc: 0.78432
[110]	valid_0's auc: 0.784143
[111]	valid_0's auc: 0.784317
[112]	valid_0's auc: 0.784725
[113]	valid_0's auc: 0.78556
[114]	valid_0's auc: 0.785842
[115]	valid_0's auc: 0.786084
[116]	valid_0's auc: 0.786523
[117]	valid_0's auc: 0.787077
[118]	valid_0's auc: 0.787459
[119]	valid_0's auc: 0.787324
[120]	valid_0's auc: 0.787635
[121]	valid_0's auc: 0.787647
[122]	valid_0's auc: 0.786972
[123]	valid_0's auc: 0.787901
[124]	valid_0's auc: 0.787933
[125]	valid_0's auc: 0.788686
[126]	valid_0's auc: 0.787851
[127]	valid_0's auc: 0.786935
[128]	valid_0's auc: 0.78713
[129]	valid_0's auc: 0.787323
[130]	valid_0's auc: 0.78822
[131]	valid_0's auc: 0.787849
[132]	valid_0's auc: 0.788024
[133]	valid_0's auc: 0.787829
[134]	valid_0's auc: 0.788902
[135]	valid_0's auc: 0.788896
[136]	valid_0's auc: 0.789137
[137]	valid_0's auc: 0.788892
[138]	valid_0's auc: 0.789212
[139]	valid_0's auc: 0.790273
[140]	valid_0's auc: 0.790727
[141]	valid_0's auc: 0.79024
[142]	valid_0's auc: 0.790419
[143]	valid_0's auc: 0.790894
[144]	valid_0's auc: 0.791759
[145]	valid_0's auc: 0.792539
[146]	valid_0's auc: 0.792891
[147]	valid_0's auc: 0.793554
[148]	valid_0's auc: 0.794373
[149]	valid_0's auc: 0.794862
[150]	valid_0's auc: 0.795238
[151]	valid_0's auc: 0.795859
[152]	valid_0's auc: 0.796235
[153]	valid_0's auc: 0.795932
[154]	valid_0's auc: 0.796417
[155]	valid_0's auc: 0.797403
[156]	valid_0's auc: 0.79712
[157]	valid_0's auc: 0.797727
[158]	valid_0's auc: 0.798275
[159]	valid_0's auc: 0.798659
[160]	valid_0's auc: 0.798523
[161]	valid_0's auc: 0.798196
[162]	valid_0's auc: 0.798675
[163]	valid_0's auc: 0.797936
[164]	valid_0's auc: 0.797743
[165]	valid_0's auc: 0.798124
[166]	valid_0's auc: 0.798832
[167]	valid_0's auc: 0.79882
[168]	valid_0's auc: 0.798582
[169]	valid_0's auc: 0.798708
[170]	valid_0's auc: 0.799304
[171]	valid_0's auc: 0.799857
[172]	valid_0's auc: 0.800457
[173]	valid_0's auc: 0.800597
[174]	valid_0's auc: 0.800896
[175]	valid_0's auc: 0.801291
[176]	valid_0's auc: 0.801822
[177]	valid_0's auc: 0.802459
[178]	valid_0's auc: 0.80296
[179]	valid_0's auc: 0.802426
[180]	valid_0's auc: 0.802968
[181]	valid_0's auc: 0.803442
[182]	valid_0's auc: 0.803626
[183]	valid_0's auc: 0.803324
[184]	valid_0's auc: 0.803358
[185]	valid_0's auc: 0.802762
[186]	valid_0's auc: 0.802814
[187]	valid_0's auc: 0.803109
[188]	valid_0's auc: 0.802701
[189]	valid_0's auc: 0.802621
[190]	valid_0's auc: 0.803037
[191]	valid_0's auc: 0.803266
[192]	valid_0's auc: 0.803196
[193]	valid_0's auc: 0.803134
[194]	valid_0's auc: 0.803535
[195]	valid_0's auc: 0.803959
[196]	valid_0's auc: 0.803539
[197]	valid_0's auc: 0.80351
[198]	valid_0's auc: 0.804023
[199]	valid_0's auc: 0.80441
[200]	valid_0's auc: 0.805182
[201]	valid_0's auc: 0.805286
[202]	valid_0's auc: 0.80585
[203]	valid_0's auc: 0.805256
[204]	valid_0's auc: 0.805557
[205]	valid_0's auc: 0.806033
[206]	valid_0's auc: 0.806741
[207]	valid_0's auc: 0.807026
[208]	valid_0's auc: 0.806999
[209]	valid_0's auc: 0.807522
[210]	valid_0's auc: 0.807631
[211]	valid_0's auc: 0.807928
[212]	valid_0's auc: 0.807936
[213]	valid_0's auc: 0.808331
[214]	valid_0's auc: 0.808613
[215]	valid_0's auc: 0.808825
[216]	valid_0's auc: 0.808513
[217]	valid_0's auc: 0.808807
[218]	valid_0's auc: 0.808883
[219]	valid_0's auc: 0.809089
[220]	valid_0's auc: 0.809327
[221]	valid_0's auc: 0.809567
[222]	valid_0's auc: 0.809361
[223]	valid_0's auc: 0.809506
[224]	valid_0's auc: 0.809894
[225]	valid_0's auc: 0.810066
[226]	valid_0's auc: 0.810681
[227]	valid_0's auc: 0.809998
[228]	valid_0's auc: 0.810072
[229]	valid_0's auc: 0.810309
[230]	valid_0's auc: 0.810443
[231]	valid_0's auc: 0.810255
[232]	valid_0's auc: 0.809954
[233]	valid_0's auc: 0.809926
[234]	valid_0's auc: 0.810483
[235]	valid_0's auc: 0.810516
[236]	valid_0's auc: 0.81072
[237]	valid_0's auc: 0.81083
[238]	valid_0's auc: 0.810972
[239]	valid_0's auc: 0.811182
[240]	valid_0's auc: 0.811833
[241]	valid_0's auc: 0.811974
[242]	valid_0's auc: 0.811939
[243]	valid_0's auc: 0.811806
[244]	valid_0's auc: 0.812
[245]	valid_0's auc: 0.812038
[246]	valid_0's auc: 0.812492
[247]	valid_0's auc: 0.812679
[248]	valid_0's auc: 0.812689
[249]	valid_0's auc: 0.812913
[250]	valid_0's auc: 0.812998
[251]	valid_0's auc: 0.813454
[252]	valid_0's auc: 0.813615
[253]	valid_0's auc: 0.813314
[254]	valid_0's auc: 0.813606
[255]	valid_0's auc: 0.813794
[256]	valid_0's auc: 0.814084
[257]	valid_0's auc: 0.814613
[258]	valid_0's auc: 0.814893
[259]	valid_0's auc: 0.815184
[260]	valid_0's auc: 0.815263
[261]	valid_0's auc: 0.81548
[262]	valid_0's auc: 0.815574
[263]	valid_0's auc: 0.816092
[264]	valid_0's auc: 0.816457
[265]	valid_0's auc: 0.816328
[266]	valid_0's auc: 0.81644
[267]	valid_0's auc: 0.816812
[268]	valid_0's auc: 0.816691
[269]	valid_0's auc: 0.816831
[270]	valid_0's auc: 0.817144
[271]	valid_0's auc: 0.816961
[272]	valid_0's auc: 0.817138
[273]	valid_0's auc: 0.817331
[274]	valid_0's auc: 0.817551
[275]	valid_0's auc: 0.817378
[276]	valid_0's auc: 0.817502
[277]	valid_0's auc: 0.817461
[278]	valid_0's auc: 0.817746
[279]	valid_0's auc: 0.818003
[280]	valid_0's auc: 0.81791
[281]	valid_0's auc: 0.818007
[282]	valid_0's auc: 0.818148
[283]	valid_0's auc: 0.818353
[284]	valid_0's auc: 0.818313
[285]	valid_0's auc: 0.818413
[286]	valid_0's auc: 0.818662
[287]	valid_0's auc: 0.818621
[288]	valid_0's auc: 0.818882
[289]	valid_0's auc: 0.819139
[290]	valid_0's auc: 0.819418
[291]	valid_0's auc: 0.81981
[292]	valid_0's auc: 0.819956
[293]	valid_0's auc: 0.820028
[294]	valid_0's auc: 0.820331
[295]	valid_0's auc: 0.820518
[296]	valid_0's auc: 0.820553
[297]	valid_0's auc: 0.820279
[298]	valid_0's auc: 0.820522
[299]	valid_0's auc: 0.820272
[300]	valid_0's auc: 0.820535
[301]	valid_0's auc: 0.820641
[302]	valid_0's auc: 0.820699
[303]	valid_0's auc: 0.820891
[304]	valid_0's auc: 0.821193
[305]	valid_0's auc: 0.821403
[306]	valid_0's auc: 0.821601
[307]	valid_0's auc: 0.821631
[308]	valid_0's auc: 0.82165
[309]	valid_0's auc: 0.821956
[310]	valid_0's auc: 0.822316
[311]	valid_0's auc: 0.822064
[312]	valid_0's auc: 0.822495
[313]	valid_0's auc: 0.82262
[314]	valid_0's auc: 0.823027
[315]	valid_0's auc: 0.822955
[316]	valid_0's auc: 0.823023
[317]	valid_0's auc: 0.823174
[318]	valid_0's auc: 0.823253
[319]	valid_0's auc: 0.822964
[320]	valid_0's auc: 0.823177
[321]	valid_0's auc: 0.823043
[322]	valid_0's auc: 0.823261
[323]	valid_0's auc: 0.823238
[324]	valid_0's auc: 0.823257
[325]	valid_0's auc: 0.823405
[326]	valid_0's auc: 0.823284
[327]	valid_0's auc: 0.823333
[328]	valid_0's auc: 0.823098
[329]	valid_0's auc: 0.823331
[330]	valid_0's auc: 0.823411
[331]	valid_0's auc: 0.823317
[332]	valid_0's auc: 0.823601
[333]	valid_0's auc: 0.823462
[334]	valid_0's auc: 0.823128
[335]	valid_0's auc: 0.823047
[336]	valid_0's auc: 0.823334
[337]	valid_0's auc: 0.823487
[338]	valid_0's auc: 0.823186
[339]	valid_0's auc: 0.823413
[340]	valid_0's auc: 0.823349
[341]	valid_0's auc: 0.823153
[342]	valid_0's auc: 0.823264
[343]	valid_0's auc: 0.823287
[344]	valid_0's auc: 0.823498
[345]	valid_0's auc: 0.823722
[346]	valid_0's auc: 0.823814
[347]	valid_0's auc: 0.823869
[348]	valid_0's auc: 0.823929
[349]	valid_0's auc: 0.823969
[350]	valid_0's auc: 0.823996
[351]	valid_0's auc: 0.824148
[352]	valid_0's auc: 0.824132
[353]	valid_0's auc: 0.824312
[354]	valid_0's auc: 0.824491
[355]	valid_0's auc: 0.824584
[356]	valid_0's auc: 0.824658
[357]	valid_0's auc: 0.824909
[358]	valid_0's auc: 0.8248
[359]	valid_0's auc: 0.824864
[360]	valid_0's auc: 0.825167
[361]	valid_0's auc: 0.825091
[362]	valid_0's auc: 0.825105
[363]	valid_0's auc: 0.825265
[364]	valid_0's auc: 0.825319
[365]	valid_0's auc: 0.825385
[366]	valid_0's auc: 0.825423
[367]	valid_0's auc: 0.8257
[368]	valid_0's auc: 0.82588
[369]	valid_0's auc: 0.825925
[370]	valid_0's auc: 0.826164
[371]	valid_0's auc: 0.826286
[372]	valid_0's auc: 0.826227
[373]	valid_0's auc: 0.82619
[374]	valid_0's auc: 0.826392
[375]	valid_0's auc: 0.826494
[376]	valid_0's auc: 0.826586
[377]	valid_0's auc: 0.827016
[378]	valid_0's auc: 0.826723
[379]	valid_0's auc: 0.826943
[380]	valid_0's auc: 0.827141
[381]	valid_0's auc: 0.827279
[382]	valid_0's auc: 0.827335
[383]	valid_0's auc: 0.827401
[384]	valid_0's auc: 0.827508
[385]	valid_0's auc: 0.827261
[386]	valid_0's auc: 0.827188
[387]	valid_0's auc: 0.82717
[388]	valid_0's auc: 0.827275
[389]	valid_0's auc: 0.82744
[390]	valid_0's auc: 0.827632
[391]	valid_0's auc: 0.827468
[392]	valid_0's auc: 0.827432
[393]	valid_0's auc: 0.827433
[394]	valid_0's auc: 0.827627
[395]	valid_0's auc: 0.827686
[396]	valid_0's auc: 0.82785
[397]	valid_0's auc: 0.828001
[398]	valid_0's auc: 0.827884
[399]	valid_0's auc: 0.828075
[400]	valid_0's auc: 0.827862
[401]	valid_0's auc: 0.828097
[402]	valid_0's auc: 0.827929
[403]	valid_0's auc: 0.828188
[404]	valid_0's auc: 0.828415
[405]	valid_0's auc: 0.828396
[406]	valid_0's auc: 0.828465
[407]	valid_0's auc: 0.828617
[408]	valid_0's auc: 0.82877
[409]	valid_0's auc: 0.828919
[410]	valid_0's auc: 0.829093
[411]	valid_0's auc: 0.82934
[412]	valid_0's auc: 0.829372
[413]	valid_0's auc: 0.829385
[414]	valid_0's auc: 0.829517
[415]	valid_0's auc: 0.829518
[416]	valid_0's auc: 0.829675
[417]	valid_0's auc: 0.829644
[418]	valid_0's auc: 0.829859
[419]	valid_0's auc: 0.830094
[420]	valid_0's auc: 0.830014
[421]	valid_0's auc: 0.82969
[422]	valid_0's auc: 0.829804
[423]	valid_0's auc: 0.82997
[424]	valid_0's auc: 0.829813
[425]	valid_0's auc: 0.829554
[426]	valid_0's auc: 0.829821
[427]	valid_0's auc: 0.829864
[428]	valid_0's auc: 0.829655
[429]	valid_0's auc: 0.829739
[430]	valid_0's auc: 0.829928
[431]	valid_0's auc: 0.829859
[432]	valid_0's auc: 0.829841
[433]	valid_0's auc: 0.829699
[434]	valid_0's auc: 0.829983
[435]	valid_0's auc: 0.830209
[436]	valid_0's auc: 0.830393
[437]	valid_0's auc: 0.830541
[438]	valid_0's auc: 0.830807
[439]	valid_0's auc: 0.83083
[440]	valid_0's auc: 0.83087
[441]	valid_0's auc: 0.831097
[442]	valid_0's auc: 0.831178
[443]	valid_0's auc: 0.831231
[444]	valid_0's auc: 0.831238
[445]	valid_0's auc: 0.831301
[446]	valid_0's auc: 0.831051
[447]	valid_0's auc: 0.831147
[448]	valid_0's auc: 0.831359
[449]	valid_0's auc: 0.831394
[450]	valid_0's auc: 0.831562
[451]	valid_0's auc: 0.831612
[452]	valid_0's auc: 0.831748
[453]	valid_0's auc: 0.831896
[454]	valid_0's auc: 0.831917
[455]	valid_0's auc: 0.832005
[456]	valid_0's auc: 0.831936
[457]	valid_0's auc: 0.832105
[458]	valid_0's auc: 0.832278
[459]	valid_0's auc: 0.832389
[460]	valid_0's auc: 0.83253
[461]	valid_0's auc: 0.832579
[462]	valid_0's auc: 0.832802
[463]	valid_0's auc: 0.833016
[464]	valid_0's auc: 0.833081
[465]	valid_0's auc: 0.832991
[466]	valid_0's auc: 0.832996
[467]	valid_0's auc: 0.833013
[468]	valid_0's auc: 0.833115
[469]	valid_0's auc: 0.833346
[470]	valid_0's auc: 0.833477
[471]	valid_0's auc: 0.833613
[472]	valid_0's auc: 0.833593
[473]	valid_0's auc: 0.833529
[474]	valid_0's auc: 0.83369
[475]	valid_0's auc: 0.833746
[476]	valid_0's auc: 0.833933
[477]	valid_0's auc: 0.834008
[478]	valid_0's auc: 0.834059
[479]	valid_0's auc: 0.834242
[480]	valid_0's auc: 0.834258
[481]	valid_0's auc: 0.834331
[482]	valid_0's auc: 0.834394
[483]	valid_0's auc: 0.834477
[484]	valid_0's auc: 0.834556
[485]	valid_0's auc: 0.834725
[486]	valid_0's auc: 0.834897
[487]	valid_0's auc: 0.834952
[488]	valid_0's auc: 0.835043
[489]	valid_0's auc: 0.835045
[490]	valid_0's auc: 0.835295
[491]	valid_0's auc: 0.835443
[492]	valid_0's auc: 0.835595
[493]	valid_0's auc: 0.835811
[494]	valid_0's auc: 0.835903
[495]	valid_0's auc: 0.835975
[496]	valid_0's auc: 0.836062
[497]	valid_0's auc: 0.83603
[498]	valid_0's auc: 0.835978
[499]	valid_0's auc: 0.836131
[500]	valid_0's auc: 0.836226
[501]	valid_0's auc: 0.836221
[502]	valid_0's auc: 0.836227
[503]	valid_0's auc: 0.836309
[504]	valid_0's auc: 0.836187
[505]	valid_0's auc: 0.836325
[506]	valid_0's auc: 0.836251
[507]	valid_0's auc: 0.836398
[508]	valid_0's auc: 0.836359
[509]	valid_0's auc: 0.836406
[510]	valid_0's auc: 0.836598
[511]	valid_0's auc: 0.836739
[512]	valid_0's auc: 0.836826
[513]	valid_0's auc: 0.836949
[514]	valid_0's auc: 0.837096
[515]	valid_0's auc: 0.837274
[516]	valid_0's auc: 0.837298
[517]	valid_0's auc: 0.837401
[518]	valid_0's auc: 0.837467
[519]	valid_0's auc: 0.837561
[520]	valid_0's auc: 0.837625
[521]	valid_0's auc: 0.837742
[522]	valid_0's auc: 0.837852
[523]	valid_0's auc: 0.837844
[524]	valid_0's auc: 0.838013
[525]	valid_0's auc: 0.837805
[526]	valid_0's auc: 0.837834
[527]	valid_0's auc: 0.837759
[528]	valid_0's auc: 0.837874
[529]	valid_0's auc: 0.837919
[530]	valid_0's auc: 0.837899
[531]	valid_0's auc: 0.837924
[532]	valid_0's auc: 0.837988
[533]	valid_0's auc: 0.838117
[534]	valid_0's auc: 0.838254
[535]	valid_0's auc: 0.838374
[536]	valid_0's auc: 0.838361
[537]	valid_0's auc: 0.838308
[538]	valid_0's auc: 0.838453
[539]	valid_0's auc: 0.838553
[540]	valid_0's auc: 0.838576
[541]	valid_0's auc: 0.838728
[542]	valid_0's auc: 0.838876
[543]	valid_0's auc: 0.838938
[544]	valid_0's auc: 0.838944
[545]	valid_0's auc: 0.838922
[546]	valid_0's auc: 0.838982
[547]	valid_0's auc: 0.839089
[548]	valid_0's auc: 0.839063
[549]	valid_0's auc: 0.839068
[550]	valid_0's auc: 0.83921
[551]	valid_0's auc: 0.839262
[552]	valid_0's auc: 0.839387
[553]	valid_0's auc: 0.839684
[554]	valid_0's auc: 0.839886
[555]	valid_0's auc: 0.840038
[556]	valid_0's auc: 0.84008
[557]	valid_0's auc: 0.84012
[558]	valid_0's auc: 0.84011
[559]	valid_0's auc: 0.840061
[560]	valid_0's auc: 0.84018
[561]	valid_0's auc: 0.840131
[562]	valid_0's auc: 0.840187
[563]	valid_0's auc: 0.840221
[564]	valid_0's auc: 0.840166
[565]	valid_0's auc: 0.840236
[566]	valid_0's auc: 0.840378
[567]	valid_0's auc: 0.840512
[568]	valid_0's auc: 0.840629
[569]	valid_0's auc: 0.840795
[570]	valid_0's auc: 0.840851
[571]	valid_0's auc: 0.84094
[572]	valid_0's auc: 0.840951
[573]	valid_0's auc: 0.840958
[574]	valid_0's auc: 0.841044
[575]	valid_0's auc: 0.841004
[576]	valid_0's auc: 0.841114
[577]	valid_0's auc: 0.841255
[578]	valid_0's auc: 0.841348
[579]	valid_0's auc: 0.841459
[580]	valid_0's auc: 0.841538
[581]	valid_0's auc: 0.841669
[582]	valid_0's auc: 0.841666
[583]	valid_0's auc: 0.841724
[584]	valid_0's auc: 0.841799
[585]	valid_0's auc: 0.841912
[586]	valid_0's auc: 0.841723
[587]	valid_0's auc: 0.841768
[588]	valid_0's auc: 0.841867
[589]	valid_0's auc: 0.842047
[590]	valid_0's auc: 0.841947
[591]	valid_0's auc: 0.841959
[592]	valid_0's auc: 0.842092
[593]	valid_0's auc: 0.842173
[594]	valid_0's auc: 0.842284
[595]	valid_0's auc: 0.842435
[596]	valid_0's auc: 0.842568
[597]	valid_0's auc: 0.842717
[598]	valid_0's auc: 0.842715
[599]	valid_0's auc: 0.842737
[600]	valid_0's auc: 0.842796
[601]	valid_0's auc: 0.842856
[602]	valid_0's auc: 0.842927
[603]	valid_0's auc: 0.843019
[604]	valid_0's auc: 0.843157
[605]	valid_0's auc: 0.843187
[606]	valid_0's auc: 0.84328
[607]	valid_0's auc: 0.843355
[608]	valid_0's auc: 0.843488
[609]	valid_0's auc: 0.843587
[610]	valid_0's auc: 0.843549
[611]	valid_0's auc: 0.843655
[612]	valid_0's auc: 0.843634
[613]	valid_0's auc: 0.843704
[614]	valid_0's auc: 0.84356
[615]	valid_0's auc: 0.843602
[616]	valid_0's auc: 0.843613
[617]	valid_0's auc: 0.843664
[618]	valid_0's auc: 0.84354
[619]	valid_0's auc: 0.843697
[620]	valid_0's auc: 0.843765
[621]	valid_0's auc: 0.843808
[622]	valid_0's auc: 0.843951
[623]	valid_0's auc: 0.84406
[624]	valid_0's auc: 0.844248
[625]	valid_0's auc: 0.844256
[626]	valid_0's auc: 0.844324
[627]	valid_0's auc: 0.844369
[628]	valid_0's auc: 0.844413
[629]	valid_0's auc: 0.844446
[630]	valid_0's auc: 0.844578
[631]	valid_0's auc: 0.844672
[632]	valid_0's auc: 0.844807
[633]	valid_0's auc: 0.844952
[634]	valid_0's auc: 0.845048
[635]	valid_0's auc: 0.84516
[636]	valid_0's auc: 0.84523
[637]	valid_0's auc: 0.845246
[638]	valid_0's auc: 0.845313
[639]	valid_0's auc: 0.845412
[640]	valid_0's auc: 0.845535
[641]	valid_0's auc: 0.845587
[642]	valid_0's auc: 0.845619
[643]	valid_0's auc: 0.845766
[644]	valid_0's auc: 0.845783
[645]	valid_0's auc: 0.845874
[646]	valid_0's auc: 0.846037
[647]	valid_0's auc: 0.846213
[648]	valid_0's auc: 0.846249
[649]	valid_0's auc: 0.846315
[650]	valid_0's auc: 0.846427
[651]	valid_0's auc: 0.846452
[652]	valid_0's auc: 0.846551
[653]	valid_0's auc: 0.846623
[654]	valid_0's auc: 0.846719
[655]	valid_0's auc: 0.846821
[656]	valid_0's auc: 0.84696
[657]	valid_0's auc: 0.847012
[658]	valid_0's auc: 0.846993
[659]	valid_0's auc: 0.847127
[660]	valid_0's auc: 0.847246
[661]	valid_0's auc: 0.847307
[662]	valid_0's auc: 0.847368
[663]	valid_0's auc: 0.847422
[664]	valid_0's auc: 0.847448
[665]	valid_0's auc: 0.847468
[666]	valid_0's auc: 0.847625
[667]	valid_0's auc: 0.847724
[668]	valid_0's auc: 0.847728
[669]	valid_0's auc: 0.847705
[670]	valid_0's auc: 0.84773
[671]	valid_0's auc: 0.847722
[672]	valid_0's auc: 0.847746
[673]	valid_0's auc: 0.847826
[674]	valid_0's auc: 0.847871
[675]	valid_0's auc: 0.847926
[676]	valid_0's auc: 0.847988
[677]	valid_0's auc: 0.848011
[678]	valid_0's auc: 0.848137
[679]	valid_0's auc: 0.848228
[680]	valid_0's auc: 0.848118
[681]	valid_0's auc: 0.848156
[682]	valid_0's auc: 0.848033
[683]	valid_0's auc: 0.848121
[684]	valid_0's auc: 0.848155
[685]	valid_0's auc: 0.84824
[686]	valid_0's auc: 0.848306
[687]	valid_0's auc: 0.848402
[688]	valid_0's auc: 0.848498
[689]	valid_0's auc: 0.848546
[690]	valid_0's auc: 0.848666
[691]	valid_0's auc: 0.848742
[692]	valid_0's auc: 0.848782
[693]	valid_0's auc: 0.848821
[694]	valid_0's auc: 0.848891
[695]	valid_0's auc: 0.848908
[696]	valid_0's auc: 0.848877
[697]	valid_0's auc: 0.84887
[698]	valid_0's auc: 0.848973
[699]	valid_0's auc: 0.849023
[700]	valid_0's auc: 0.849111
[701]	valid_0's auc: 0.849211
[702]	valid_0's auc: 0.849322
[703]	valid_0's auc: 0.849371
[704]	valid_0's auc: 0.849391
[705]	valid_0's auc: 0.849483
[706]	valid_0's auc: 0.84949
[707]	valid_0's auc: 0.849382
[708]	valid_0's auc: 0.849451
[709]	valid_0's auc: 0.849457
[710]	valid_0's auc: 0.849469
[711]	valid_0's auc: 0.849635
[712]	valid_0's auc: 0.849678
[713]	valid_0's auc: 0.849716
[714]	valid_0's auc: 0.84975
[715]	valid_0's auc: 0.849895
[716]	valid_0's auc: 0.849902
[717]	valid_0's auc: 0.849925
[718]	valid_0's auc: 0.849977
[719]	valid_0's auc: 0.850001
[720]	valid_0's auc: 0.850068
[721]	valid_0's auc: 0.850089
[722]	valid_0's auc: 0.850173
[723]	valid_0's auc: 0.850271
[724]	valid_0's auc: 0.850323
[725]	valid_0's auc: 0.850387
[726]	valid_0's auc: 0.850484
[727]	valid_0's auc: 0.850613
[728]	valid_0's auc: 0.850681
[729]	valid_0's auc: 0.850772
[730]	valid_0's auc: 0.850891
[731]	valid_0's auc: 0.850999
[732]	valid_0's auc: 0.85098
[733]	valid_0's auc: 0.851065
[734]	valid_0's auc: 0.851211
[735]	valid_0's auc: 0.851285
[736]	valid_0's auc: 0.851271
[737]	valid_0's auc: 0.85134
[738]	valid_0's auc: 0.851404
[739]	valid_0's auc: 0.851481
[740]	valid_0's auc: 0.851518
[741]	valid_0's auc: 0.851513
[742]	valid_0's auc: 0.851587
[743]	valid_0's auc: 0.851715
[744]	valid_0's auc: 0.851728
[745]	valid_0's auc: 0.851744
[746]	valid_0's auc: 0.851755
[747]	valid_0's auc: 0.851662
[748]	valid_0's auc: 0.851624
[749]	valid_0's auc: 0.851669
[750]	valid_0's auc: 0.851784
[751]	valid_0's auc: 0.851779
[752]	valid_0's auc: 0.851831
[753]	valid_0's auc: 0.851896
[754]	valid_0's auc: 0.851979
[755]	valid_0's auc: 0.851909
[756]	valid_0's auc: 0.851918
[757]	valid_0's auc: 0.852024
[758]	valid_0's auc: 0.852105
[759]	valid_0's auc: 0.852196
[760]	valid_0's auc: 0.852195
[761]	valid_0's auc: 0.852178
[762]	valid_0's auc: 0.852224
[763]	valid_0's auc: 0.852276
[764]	valid_0's auc: 0.85238
[765]	valid_0's auc: 0.852416
[766]	valid_0's auc: 0.852472
[767]	valid_0's auc: 0.8525
[768]	valid_0's auc: 0.852566
[769]	valid_0's auc: 0.852608
[770]	valid_0's auc: 0.852677
[771]	valid_0's auc: 0.852722
[772]	valid_0's auc: 0.852832
[773]	valid_0's auc: 0.852894
[774]	valid_0's auc: 0.852914
[775]	valid_0's auc: 0.852996
[776]	valid_0's auc: 0.85312
[777]	valid_0's auc: 0.853173
[778]	valid_0's auc: 0.853174
[779]	valid_0's auc: 0.853276
[780]	valid_0's auc: 0.853387
[781]	valid_0's auc: 0.853454
[782]	valid_0's auc: 0.853534
[783]	valid_0's auc: 0.853545
[784]	valid_0's auc: 0.853568
[785]	valid_0's auc: 0.853623
[786]	valid_0's auc: 0.853702
[787]	valid_0's auc: 0.85366
[788]	valid_0's auc: 0.853652
[789]	valid_0's auc: 0.853763
[790]	valid_0's auc: 0.853834
[791]	valid_0's auc: 0.853801
[792]	valid_0's auc: 0.853797
[793]	valid_0's auc: 0.853828
[794]	valid_0's auc: 0.853926
[795]	valid_0's auc: 0.853912
[796]	valid_0's auc: 0.853971
[797]	valid_0's auc: 0.853983
[798]	valid_0's auc: 0.854046
[799]	valid_0's auc: 0.85403
[800]	valid_0's auc: 0.854109
[801]	valid_0's auc: 0.854071
[802]	valid_0's auc: 0.85417
[803]	valid_0's auc: 0.85423
[804]	valid_0's auc: 0.854291
[805]	valid_0's auc: 0.854306
[806]	valid_0's auc: 0.854349
[807]	valid_0's auc: 0.854475
[808]	valid_0's auc: 0.854516
[809]	valid_0's auc: 0.85456
[810]	valid_0's auc: 0.854633
[811]	valid_0's auc: 0.854689
[812]	valid_0's auc: 0.854755
[813]	valid_0's auc: 0.854857
[814]	valid_0's auc: 0.855018
[815]	valid_0's auc: 0.855083
[816]	valid_0's auc: 0.855138
[817]	valid_0's auc: 0.855217
[818]	valid_0's auc: 0.855214
[819]	valid_0's auc: 0.855268
[820]	valid_0's auc: 0.855384
[821]	valid_0's auc: 0.85534
[822]	valid_0's auc: 0.855266
[823]	valid_0's auc: 0.855208
[824]	valid_0's auc: 0.85526
[825]	valid_0's auc: 0.855347
[826]	valid_0's auc: 0.855438
[827]	valid_0's auc: 0.855521
[828]	valid_0's auc: 0.855525
[829]	valid_0's auc: 0.855574
[830]	valid_0's auc: 0.855658
[831]	valid_0's auc: 0.855772
[832]	valid_0's auc: 0.855801
[833]	valid_0's auc: 0.855772
[834]	valid_0's auc: 0.855922
[835]	valid_0's auc: 0.85592
[836]	valid_0's auc: 0.855972
[837]	valid_0's auc: 0.855987
[838]	valid_0's auc: 0.85601
[839]	valid_0's auc: 0.856044
[840]	valid_0's auc: 0.856103
[841]	valid_0's auc: 0.856234
[842]	valid_0's auc: 0.856315
[843]	valid_0's auc: 0.856407
[844]	valid_0's auc: 0.856466
[845]	valid_0's auc: 0.856515
[846]	valid_0's auc: 0.856556
[847]	valid_0's auc: 0.856596
[848]	valid_0's auc: 0.856643
[849]	valid_0's auc: 0.856733
[850]	valid_0's auc: 0.856775
[851]	valid_0's auc: 0.856863
[852]	valid_0's auc: 0.856898
[853]	valid_0's auc: 0.856966
[854]	valid_0's auc: 0.857002
[855]	valid_0's auc: 0.857025
[856]	valid_0's auc: 0.857022
[857]	valid_0's auc: 0.857091
[858]	valid_0's auc: 0.857178
[859]	valid_0's auc: 0.8572
[860]	valid_0's auc: 0.857243
[861]	valid_0's auc: 0.857326
[862]	valid_0's auc: 0.857348
[863]	valid_0's auc: 0.857423
[864]	valid_0's auc: 0.857511
[865]	valid_0's auc: 0.857493
[866]	valid_0's auc: 0.857535
[867]	valid_0's auc: 0.857599
[868]	valid_0's auc: 0.857641
[869]	valid_0's auc: 0.857634
[870]	valid_0's auc: 0.857706
[871]	valid_0's auc: 0.857771
[872]	valid_0's auc: 0.857835
[873]	valid_0's auc: 0.857864
[874]	valid_0's auc: 0.857899
[875]	valid_0's auc: 0.857952
[876]	valid_0's auc: 0.857961
[877]	valid_0's auc: 0.858053
[878]	valid_0's auc: 0.85815
[879]	valid_0's auc: 0.858175
[880]	valid_0's auc: 0.858232
[881]	valid_0's auc: 0.858296
[882]	valid_0's auc: 0.858363
[883]	valid_0's auc: 0.858421
[884]	valid_0's auc: 0.858471
[885]	valid_0's auc: 0.858463
[886]	valid_0's auc: 0.858498
[887]	valid_0's auc: 0.858551
[888]	valid_0's auc: 0.858603
[889]	valid_0's auc: 0.858654
[890]	valid_0's auc: 0.858706
[891]	valid_0's auc: 0.858709
[892]	valid_0's auc: 0.858744
[893]	valid_0's auc: 0.858728
[894]	valid_0's auc: 0.858717
[895]	valid_0's auc: 0.858834
[896]	valid_0's auc: 0.858896
[897]	valid_0's auc: 0.85898
[898]	valid_0's auc: 0.859057
[899]	valid_0's auc: 0.859134
[900]	valid_0's auc: 0.859228
[901]	valid_0's auc: 0.859257
[902]	valid_0's auc: 0.8593
[903]	valid_0's auc: 0.859349
[904]	valid_0's auc: 0.859316
[905]	valid_0's auc: 0.859359
[906]	valid_0's auc: 0.85945
[907]	valid_0's auc: 0.859425
[908]	valid_0's auc: 0.859459
[909]	valid_0's auc: 0.859473
[910]	valid_0's auc: 0.859534
[911]	valid_0's auc: 0.859597
[912]	valid_0's auc: 0.859623
[913]	valid_0's auc: 0.859716
[914]	valid_0's auc: 0.859766
[915]	valid_0's auc: 0.859752
[916]	valid_0's auc: 0.859816
[917]	valid_0's auc: 0.859819
[918]	valid_0's auc: 0.859862
[919]	valid_0's auc: 0.859893
[920]	valid_0's auc: 0.859948
[921]	valid_0's auc: 0.859975
[922]	valid_0's auc: 0.86
[923]	valid_0's auc: 0.860063
[924]	valid_0's auc: 0.860075
[925]	valid_0's auc: 0.860148
[926]	valid_0's auc: 0.860151
[927]	valid_0's auc: 0.860219
[928]	valid_0's auc: 0.860234
[929]	valid_0's auc: 0.86033
[930]	valid_0's auc: 0.860347
[931]	valid_0's auc: 0.86043
[932]	valid_0's auc: 0.860473
[933]	valid_0's auc: 0.860502
[934]	valid_0's auc: 0.860589
[935]	valid_0's auc: 0.860642
[936]	valid_0's auc: 0.860687
[937]	valid_0's auc: 0.86072
[938]	valid_0's auc: 0.860743
[939]	valid_0's auc: 0.860806
[940]	valid_0's auc: 0.860856
[941]	valid_0's auc: 0.860861
[942]	valid_0's auc: 0.860863
[943]	valid_0's auc: 0.860843
[944]	valid_0's auc: 0.860873
[945]	valid_0's auc: 0.860922
[946]	valid_0's auc: 0.860965
[947]	valid_0's auc: 0.86101
[948]	valid_0's auc: 0.86103
[949]	valid_0's auc: 0.861059
[950]	valid_0's auc: 0.861086
[951]	valid_0's auc: 0.861109
[952]	valid_0's auc: 0.861169
[953]	valid_0's auc: 0.861245
[954]	valid_0's auc: 0.861192
[955]	valid_0's auc: 0.86119
[956]	valid_0's auc: 0.861246
[957]	valid_0's auc: 0.861267
[958]	valid_0's auc: 0.861268
[959]	valid_0's auc: 0.861268
[960]	valid_0's auc: 0.861384
[961]	valid_0's auc: 0.861405
[962]	valid_0's auc: 0.861467
[963]	valid_0's auc: 0.861506
[964]	valid_0's auc: 0.8616
[965]	valid_0's auc: 0.861618
[966]	valid_0's auc: 0.861691
[967]	valid_0's auc: 0.861692
[968]	valid_0's auc: 0.861724
[969]	valid_0's auc: 0.861774
[970]	valid_0's auc: 0.861844
[971]	valid_0's auc: 0.86189
[972]	valid_0's auc: 0.861955
[973]	valid_0's auc: 0.86203
[974]	valid_0's auc: 0.862088
[975]	valid_0's auc: 0.862157
[976]	valid_0's auc: 0.862161
[977]	valid_0's auc: 0.862193
[978]	valid_0's auc: 0.862263
[979]	valid_0's auc: 0.862276
[980]	valid_0's auc: 0.862329
[981]	valid_0's auc: 0.862385
[982]	valid_0's auc: 0.862371
[983]	valid_0's auc: 0.862391
[984]	valid_0's auc: 0.862376
[985]	valid_0's auc: 0.862417
[986]	valid_0's auc: 0.862417
[987]	valid_0's auc: 0.862487
[988]	valid_0's auc: 0.862535
[989]	valid_0's auc: 0.862582
[990]	valid_0's auc: 0.862562
[991]	valid_0's auc: 0.862628
[992]	valid_0's auc: 0.862653
[993]	valid_0's auc: 0.862657
[994]	valid_0's auc: 0.862686
[995]	valid_0's auc: 0.862739
[996]	valid_0's auc: 0.862772
[997]	valid_0's auc: 0.862751
[998]	valid_0's auc: 0.862808
[999]	valid_0's auc: 0.862857
[1000]	valid_0's auc: 0.862902
[1001]	valid_0's auc: 0.862926
[1002]	valid_0's auc: 0.862966
[1003]	valid_0's auc: 0.863042
[1004]	valid_0's auc: 0.863027
[1005]	valid_0's auc: 0.8631
[1006]	valid_0's auc: 0.863165
[1007]	valid_0's auc: 0.863157
[1008]	valid_0's auc: 0.86318
[1009]	valid_0's auc: 0.863234
[1010]	valid_0's auc: 0.863224
[1011]	valid_0's auc: 0.863274
[1012]	valid_0's auc: 0.86331
[1013]	valid_0's auc: 0.863381
[1014]	valid_0's auc: 0.863439
[1015]	valid_0's auc: 0.863522
[1016]	valid_0's auc: 0.863543
[1017]	valid_0's auc: 0.863574
[1018]	valid_0's auc: 0.863597
[1019]	valid_0's auc: 0.863605
[1020]	valid_0's auc: 0.863682
[1021]	valid_0's auc: 0.863668
[1022]	valid_0's auc: 0.863706
[1023]	valid_0's auc: 0.863722
[1024]	valid_0's auc: 0.86371
[1025]	valid_0's auc: 0.863716
[1026]	valid_0's auc: 0.86376
[1027]	valid_0's auc: 0.863808
[1028]	valid_0's auc: 0.863833
[1029]	valid_0's auc: 0.863867
[1030]	valid_0's auc: 0.863923
[1031]	valid_0's auc: 0.863961
[1032]	valid_0's auc: 0.863994
[1033]	valid_0's auc: 0.86405
[1034]	valid_0's auc: 0.864073
[1035]	valid_0's auc: 0.864086
[1036]	valid_0's auc: 0.864108
[1037]	valid_0's auc: 0.864157
[1038]	valid_0's auc: 0.86419
[1039]	valid_0's auc: 0.864215
[1040]	valid_0's auc: 0.864201
[1041]	valid_0's auc: 0.864284
[1042]	valid_0's auc: 0.864327
[1043]	valid_0's auc: 0.864312
[1044]	valid_0's auc: 0.864354
[1045]	valid_0's auc: 0.864282
[1046]	valid_0's auc: 0.864313
[1047]	valid_0's auc: 0.864362
[1048]	valid_0's auc: 0.864383
[1049]	valid_0's auc: 0.864428
[1050]	valid_0's auc: 0.864461
[1051]	valid_0's auc: 0.864511
[1052]	valid_0's auc: 0.864521
[1053]	valid_0's auc: 0.864569
[1054]	valid_0's auc: 0.864595
[1055]	valid_0's auc: 0.864631
[1056]	valid_0's auc: 0.864627
[1057]	valid_0's auc: 0.864647
[1058]	valid_0's auc: 0.864644
[1059]	valid_0's auc: 0.864667
[1060]	valid_0's auc: 0.864681
[1061]	valid_0's auc: 0.864734
[1062]	valid_0's auc: 0.864757
[1063]	valid_0's auc: 0.864736
[1064]	valid_0's auc: 0.864729
[1065]	valid_0's auc: 0.864763
[1066]	valid_0's auc: 0.864841
[1067]	valid_0's auc: 0.864882
[1068]	valid_0's auc: 0.864913
[1069]	valid_0's auc: 0.864967
[1070]	valid_0's auc: 0.864984
[1071]	valid_0's auc: 0.865047
[1072]	valid_0's auc: 0.865085
[1073]	valid_0's auc: 0.865129
[1074]	valid_0's auc: 0.865182
[1075]	valid_0's auc: 0.865238
[1076]	valid_0's auc: 0.865296
[1077]	valid_0's auc: 0.865336
[1078]	valid_0's auc: 0.865351
[1079]	valid_0's auc: 0.865444
[1080]	valid_0's auc: 0.865462
[1081]	valid_0's auc: 0.865481
[1082]	valid_0's auc: 0.865563
[1083]	valid_0's auc: 0.865609
[1084]	valid_0's auc: 0.865626
[1085]	valid_0's auc: 0.865673
[1086]	valid_0's auc: 0.865683
[1087]	valid_0's auc: 0.865709
[1088]	valid_0's auc: 0.865736
[1089]	valid_0's auc: 0.865749
[1090]	valid_0's auc: 0.865792
[1091]	valid_0's auc: 0.865793
[1092]	valid_0's auc: 0.865807
[1093]	valid_0's auc: 0.865852
[1094]	valid_0's auc: 0.86588
[1095]	valid_0's auc: 0.865922
[1096]	valid_0's auc: 0.865916
[1097]	valid_0's auc: 0.86596
[1098]	valid_0's auc: 0.865961
[1099]	valid_0's auc: 0.866004
[1100]	valid_0's auc: 0.866033
[1101]	valid_0's auc: 0.866063
[1102]	valid_0's auc: 0.866112
[1103]	valid_0's auc: 0.866138
[1104]	valid_0's auc: 0.866162
[1105]	valid_0's auc: 0.866172
[1106]	valid_0's auc: 0.866218
[1107]	valid_0's auc: 0.866257
[1108]	valid_0's auc: 0.86624
[1109]	valid_0's auc: 0.866218
[1110]	valid_0's auc: 0.866265
[1111]	valid_0's auc: 0.866302
[1112]	valid_0's auc: 0.86632
[1113]	valid_0's auc: 0.866332
[1114]	valid_0's auc: 0.866405
[1115]	valid_0's auc: 0.866489
[1116]	valid_0's auc: 0.866515
[1117]	valid_0's auc: 0.866562
[1118]	valid_0's auc: 0.866616
[1119]	valid_0's auc: 0.866609
[1120]	valid_0's auc: 0.86665
[1121]	valid_0's auc: 0.866659
[1122]	valid_0's auc: 0.866692
[1123]	valid_0's auc: 0.86675
[1124]	valid_0's auc: 0.866832
[1125]	valid_0's auc: 0.866889
[1126]	valid_0's auc: 0.866926
[1127]	valid_0's auc: 0.866913
[1128]	valid_0's auc: 0.866916
[1129]	valid_0's auc: 0.866949
[1130]	valid_0's auc: 0.867003
[1131]	valid_0's auc: 0.867028
[1132]	valid_0's auc: 0.867047
[1133]	valid_0's auc: 0.86706
[1134]	valid_0's auc: 0.867048
[1135]	valid_0's auc: 0.867075
[1136]	valid_0's auc: 0.867102
[1137]	valid_0's auc: 0.867099
[1138]	valid_0's auc: 0.867113
[1139]	valid_0's auc: 0.867065
[1140]	valid_0's auc: 0.86706
[1141]	valid_0's auc: 0.867099
[1142]	valid_0's auc: 0.867145
[1143]	valid_0's auc: 0.867207
[1144]	valid_0's auc: 0.867252
[1145]	valid_0's auc: 0.867253
[1146]	valid_0's auc: 0.867279
[1147]	valid_0's auc: 0.867336
[1148]	valid_0's auc: 0.867357
[1149]	valid_0's auc: 0.867397
[1150]	valid_0's auc: 0.86744
[1151]	valid_0's auc: 0.867525
[1152]	valid_0's auc: 0.867559
[1153]	valid_0's auc: 0.867596
[1154]	valid_0's auc: 0.867599
[1155]	valid_0's auc: 0.867636
[1156]	valid_0's auc: 0.867638
[1157]	valid_0's auc: 0.86766
[1158]	valid_0's auc: 0.867704
[1159]	valid_0's auc: 0.867732
[1160]	valid_0's auc: 0.867796
[1161]	valid_0's auc: 0.867851
[1162]	valid_0's auc: 0.867914
[1163]	valid_0's auc: 0.867963
[1164]	valid_0's auc: 0.868043
[1165]	valid_0's auc: 0.868107
[1166]	valid_0's auc: 0.868137
[1167]	valid_0's auc: 0.868179
[1168]	valid_0's auc: 0.868199
[1169]	valid_0's auc: 0.868237
[1170]	valid_0's auc: 0.868257
[1171]	valid_0's auc: 0.868267
[1172]	valid_0's auc: 0.868289
[1173]	valid_0's auc: 0.868318
[1174]	valid_0's auc: 0.868358
[1175]	valid_0's auc: 0.868371
[1176]	valid_0's auc: 0.868359
[1177]	valid_0's auc: 0.868385
[1178]	valid_0's auc: 0.868314
[1179]	valid_0's auc: 0.868343
[1180]	valid_0's auc: 0.868378
[1181]	valid_0's auc: 0.86838
[1182]	valid_0's auc: 0.868444
[1183]	valid_0's auc: 0.868495
[1184]	valid_0's auc: 0.868524
[1185]	valid_0's auc: 0.868555
[1186]	valid_0's auc: 0.868591
[1187]	valid_0's auc: 0.868637
[1188]	valid_0's auc: 0.868659
[1189]	valid_0's auc: 0.868666
[1190]	valid_0's auc: 0.868639
[1191]	valid_0's auc: 0.868661
[1192]	valid_0's auc: 0.868689
[1193]	valid_0's auc: 0.86871
[1194]	valid_0's auc: 0.868728
[1195]	valid_0's auc: 0.868785
[1196]	valid_0's auc: 0.868847
[1197]	valid_0's auc: 0.868879
[1198]	valid_0's auc: 0.868938
[1199]	valid_0's auc: 0.868946
[1200]	valid_0's auc: 0.868981
[1201]	valid_0's auc: 0.869026
[1202]	valid_0's auc: 0.869021
[1203]	valid_0's auc: 0.869045
[1204]	valid_0's auc: 0.869097
[1205]	valid_0's auc: 0.869116
[1206]	valid_0's auc: 0.869151
[1207]	valid_0's auc: 0.869171
[1208]	valid_0's auc: 0.869202
[1209]	valid_0's auc: 0.869201
[1210]	valid_0's auc: 0.869223
[1211]	valid_0's auc: 0.869226
[1212]	valid_0's auc: 0.86925
[1213]	valid_0's auc: 0.869278
[1214]	valid_0's auc: 0.869298
[1215]	valid_0's auc: 0.869338
[1216]	valid_0's auc: 0.869403
[1217]	valid_0's auc: 0.869459
[1218]	valid_0's auc: 0.869515
[1219]	valid_0's auc: 0.869569
[1220]	valid_0's auc: 0.869586
[1221]	valid_0's auc: 0.869634
[1222]	valid_0's auc: 0.869666
[1223]	valid_0's auc: 0.869716
[1224]	valid_0's auc: 0.869672
[1225]	valid_0's auc: 0.86973
[1226]	valid_0's auc: 0.869766
[1227]	valid_0's auc: 0.869811
[1228]	valid_0's auc: 0.869837
[1229]	valid_0's auc: 0.869855
[1230]	valid_0's auc: 0.869871
[1231]	valid_0's auc: 0.869896
[1232]	valid_0's auc: 0.869935
[1233]	valid_0's auc: 0.869938
[1234]	valid_0's auc: 0.869953
[1235]	valid_0's auc: 0.870016
[1236]	valid_0's auc: 0.870032
[1237]	valid_0's auc: 0.870091
[1238]	valid_0's auc: 0.870106
[1239]	valid_0's auc: 0.870101
[1240]	valid_0's auc: 0.8701
[1241]	valid_0's auc: 0.870116
[1242]	valid_0's auc: 0.870174
[1243]	valid_0's auc: 0.870199
[1244]	valid_0's auc: 0.870245
[1245]	valid_0's auc: 0.870278
[1246]	valid_0's auc: 0.870351
[1247]	valid_0's auc: 0.870369
[1248]	valid_0's auc: 0.870408
[1249]	valid_0's auc: 0.870432
[1250]	valid_0's auc: 0.870435
[1251]	valid_0's auc: 0.870466
[1252]	valid_0's auc: 0.8705
[1253]	valid_0's auc: 0.870538
[1254]	valid_0's auc: 0.870574
[1255]	valid_0's auc: 0.870635
[1256]	valid_0's auc: 0.870671
[1257]	valid_0's auc: 0.870675
[1258]	valid_0's auc: 0.870695
[1259]	valid_0's auc: 0.870709
[1260]	valid_0's auc: 0.870727
[1261]	valid_0's auc: 0.870773
[1262]	valid_0's auc: 0.870803
[1263]	valid_0's auc: 0.870816
[1264]	valid_0's auc: 0.870872
[1265]	valid_0's auc: 0.870914
[1266]	valid_0's auc: 0.870914
[1267]	valid_0's auc: 0.87094
[1268]	valid_0's auc: 0.870973
[1269]	valid_0's auc: 0.871015
[1270]	valid_0's auc: 0.871052
[1271]	valid_0's auc: 0.871035
[1272]	valid_0's auc: 0.871078
[1273]	valid_0's auc: 0.87108
[1274]	valid_0's auc: 0.871107
[1275]	valid_0's auc: 0.871092
[1276]	valid_0's auc: 0.871141
[1277]	valid_0's auc: 0.871178
[1278]	valid_0's auc: 0.871207
[1279]	valid_0's auc: 0.871229
[1280]	valid_0's auc: 0.87124
[1281]	valid_0's auc: 0.871257
[1282]	valid_0's auc: 0.871279
[1283]	valid_0's auc: 0.87131
[1284]	valid_0's auc: 0.871333
[1285]	valid_0's auc: 0.871358
[1286]	valid_0's auc: 0.87137
[1287]	valid_0's auc: 0.871409
[1288]	valid_0's auc: 0.871457
[1289]	valid_0's auc: 0.871469
[1290]	valid_0's auc: 0.87147
[1291]	valid_0's auc: 0.871458
[1292]	valid_0's auc: 0.871518
[1293]	valid_0's auc: 0.871564
[1294]	valid_0's auc: 0.871585
[1295]	valid_0's auc: 0.871644
[1296]	valid_0's auc: 0.871696
[1297]	valid_0's auc: 0.871739
[1298]	valid_0's auc: 0.871779
[1299]	valid_0's auc: 0.871808
[1300]	valid_0's auc: 0.871866
[1301]	valid_0's auc: 0.871903
[1302]	valid_0's auc: 0.871922
[1303]	valid_0's auc: 0.871909
[1304]	valid_0's auc: 0.871931
[1305]	valid_0's auc: 0.871964
[1306]	valid_0's auc: 0.871988
[1307]	valid_0's auc: 0.872005
[1308]	valid_0's auc: 0.872036
[1309]	valid_0's auc: 0.872066
[1310]	valid_0's auc: 0.872097
[1311]	valid_0's auc: 0.87211
[1312]	valid_0's auc: 0.872115
[1313]	valid_0's auc: 0.872121
[1314]	valid_0's auc: 0.872161
[1315]	valid_0's auc: 0.872203
[1316]	valid_0's auc: 0.872224
[1317]	valid_0's auc: 0.872216
[1318]	valid_0's auc: 0.872272
[1319]	valid_0's auc: 0.872293
[1320]	valid_0's auc: 0.872313
[1321]	valid_0's auc: 0.872346
[1322]	valid_0's auc: 0.872328
[1323]	valid_0's auc: 0.87235
[1324]	valid_0's auc: 0.872372
[1325]	valid_0's auc: 0.872424
[1326]	valid_0's auc: 0.872433
[1327]	valid_0's auc: 0.872487
[1328]	valid_0's auc: 0.872477
[1329]	valid_0's auc: 0.872511
[1330]	valid_0's auc: 0.872539
[1331]	valid_0's auc: 0.872572
[1332]	valid_0's auc: 0.872598
[1333]	valid_0's auc: 0.872639
[1334]	valid_0's auc: 0.87265
[1335]	valid_0's auc: 0.872694
[1336]	valid_0's auc: 0.872708
[1337]	valid_0's auc: 0.872757
[1338]	valid_0's auc: 0.872804
[1339]	valid_0's auc: 0.872837
[1340]	valid_0's auc: 0.872864
[1341]	valid_0's auc: 0.872887
[1342]	valid_0's auc: 0.872905
[1343]	valid_0's auc: 0.872965
[1344]	valid_0's auc: 0.872968
[1345]	valid_0's auc: 0.872994
[1346]	valid_0's auc: 0.873047
[1347]	valid_0's auc: 0.87313
[1348]	valid_0's auc: 0.87314
[1349]	valid_0's auc: 0.873179
[1350]	valid_0's auc: 0.873225
[1351]	valid_0's auc: 0.87326
[1352]	valid_0's auc: 0.873277
[1353]	valid_0's auc: 0.873305
[1354]	valid_0's auc: 0.873361
[1355]	valid_0's auc: 0.873395
[1356]	valid_0's auc: 0.873419
[1357]	valid_0's auc: 0.873435
[1358]	valid_0's auc: 0.87348
[1359]	valid_0's auc: 0.873502
[1360]	valid_0's auc: 0.873524
[1361]	valid_0's auc: 0.873533
[1362]	valid_0's auc: 0.873586
[1363]	valid_0's auc: 0.873627
[1364]	valid_0's auc: 0.873671
[1365]	valid_0's auc: 0.873685
[1366]	valid_0's auc: 0.873711
[1367]	valid_0's auc: 0.873742
[1368]	valid_0's auc: 0.873716
[1369]	valid_0's auc: 0.873735
[1370]	valid_0's auc: 0.873786
[1371]	valid_0's auc: 0.873827
[1372]	valid_0's auc: 0.873836
[1373]	valid_0's auc: 0.873904
[1374]	valid_0's auc: 0.873942
[1375]	valid_0's auc: 0.873967
[1376]	valid_0's auc: 0.873969
[1377]	valid_0's auc: 0.873974
[1378]	valid_0's auc: 0.874009
[1379]	valid_0's auc: 0.874062
[1380]	valid_0's auc: 0.874081
[1381]	valid_0's auc: 0.874118
[1382]	valid_0's auc: 0.874102
[1383]	valid_0's auc: 0.874125
[1384]	valid_0's auc: 0.874118
[1385]	valid_0's auc: 0.874124
[1386]	valid_0's auc: 0.874175
[1387]	valid_0's auc: 0.874225
[1388]	valid_0's auc: 0.874225
[1389]	valid_0's auc: 0.874242
[1390]	valid_0's auc: 0.874267
[1391]	valid_0's auc: 0.874281
[1392]	valid_0's auc: 0.874288
[1393]	valid_0's auc: 0.874303
[1394]	valid_0's auc: 0.874343
[1395]	valid_0's auc: 0.874364
[1396]	valid_0's auc: 0.874393
[1397]	valid_0's auc: 0.874459
[1398]	valid_0's auc: 0.874457
[1399]	valid_0's auc: 0.874438
[1400]	valid_0's auc: 0.874452
[1401]	valid_0's auc: 0.874478
[1402]	valid_0's auc: 0.874462
[1403]	valid_0's auc: 0.874479
[1404]	valid_0's auc: 0.874486
[1405]	valid_0's auc: 0.874486
[1406]	valid_0's auc: 0.87453
[1407]	valid_0's auc: 0.874548
[1408]	valid_0's auc: 0.874599
[1409]	valid_0's auc: 0.874625
[1410]	valid_0's auc: 0.874652
[1411]	valid_0's auc: 0.874645
[1412]	valid_0's auc: 0.874677
[1413]	valid_0's auc: 0.87469
[1414]	valid_0's auc: 0.874734
[1415]	valid_0's auc: 0.87474
[1416]	valid_0's auc: 0.874761
[1417]	valid_0's auc: 0.874765
[1418]	valid_0's auc: 0.874765
[1419]	valid_0's auc: 0.874781
[1420]	valid_0's auc: 0.874802
[1421]	valid_0's auc: 0.874826
[1422]	valid_0's auc: 0.874844
[1423]	valid_0's auc: 0.874866
[1424]	valid_0's auc: 0.874888
[1425]	valid_0's auc: 0.874883
[1426]	valid_0's auc: 0.874886
[1427]	valid_0's auc: 0.874894
[1428]	valid_0's auc: 0.874914
[1429]	valid_0's auc: 0.874937
[1430]	valid_0's auc: 0.874976
[1431]	valid_0's auc: 0.874974
[1432]	valid_0's auc: 0.874998
[1433]	valid_0's auc: 0.875038
[1434]	valid_0's auc: 0.875063
[1435]	valid_0's auc: 0.875065
[1436]	valid_0's auc: 0.875074
[1437]	valid_0's auc: 0.875042
[1438]	valid_0's auc: 0.875059
[1439]	valid_0's auc: 0.875088
[1440]	valid_0's auc: 0.875099
[1441]	valid_0's auc: 0.875126
[1442]	valid_0's auc: 0.875159
[1443]	valid_0's auc: 0.875198
[1444]	valid_0's auc: 0.875225
[1445]	valid_0's auc: 0.875259
[1446]	valid_0's auc: 0.87529
[1447]	valid_0's auc: 0.875318
[1448]	valid_0's auc: 0.875342
[1449]	valid_0's auc: 0.875379
[1450]	valid_0's auc: 0.875411
[1451]	valid_0's auc: 0.875457
[1452]	valid_0's auc: 0.875485
[1453]	valid_0's auc: 0.875497
[1454]	valid_0's auc: 0.875526
[1455]	valid_0's auc: 0.875574
[1456]	valid_0's auc: 0.875592
[1457]	valid_0's auc: 0.875562
[1458]	valid_0's auc: 0.875595
[1459]	valid_0's auc: 0.87562
[1460]	valid_0's auc: 0.8756
[1461]	valid_0's auc: 0.875638
[1462]	valid_0's auc: 0.875644
[1463]	valid_0's auc: 0.875665
[1464]	valid_0's auc: 0.875705
[1465]	valid_0's auc: 0.875717
[1466]	valid_0's auc: 0.875721
[1467]	valid_0's auc: 0.87573
[1468]	valid_0's auc: 0.875746
[1469]	valid_0's auc: 0.875775
[1470]	valid_0's auc: 0.87581
[1471]	valid_0's auc: 0.875837
[1472]	valid_0's auc: 0.875831
[1473]	valid_0's auc: 0.875873
[1474]	valid_0's auc: 0.875884
[1475]	valid_0's auc: 0.875888
[1476]	valid_0's auc: 0.875951
[1477]	valid_0's auc: 0.875957
[1478]	valid_0's auc: 0.87597
[1479]	valid_0's auc: 0.87599
[1480]	valid_0's auc: 0.875996
[1481]	valid_0's auc: 0.875996
[1482]	valid_0's auc: 0.876042
[1483]	valid_0's auc: 0.876093
[1484]	valid_0's auc: 0.876145
[1485]	valid_0's auc: 0.876166
[1486]	valid_0's auc: 0.876186
[1487]	valid_0's auc: 0.87619
[1488]	valid_0's auc: 0.876232
[1489]	valid_0's auc: 0.87625
[1490]	valid_0's auc: 0.876277
[1491]	valid_0's auc: 0.876328
[1492]	valid_0's auc: 0.876349
[1493]	valid_0's auc: 0.876376
[1494]	valid_0's auc: 0.876385
[1495]	valid_0's auc: 0.876388
[1496]	valid_0's auc: 0.876374
[1497]	valid_0's auc: 0.876393
[1498]	valid_0's auc: 0.876415
[1499]	valid_0's auc: 0.876422
[1500]	valid_0's auc: 0.876461
[1501]	valid_0's auc: 0.876491
[1502]	valid_0's auc: 0.876497
[1503]	valid_0's auc: 0.876502
[1504]	valid_0's auc: 0.876516
[1505]	valid_0's auc: 0.876529
[1506]	valid_0's auc: 0.87656
[1507]	valid_0's auc: 0.876586
[1508]	valid_0's auc: 0.876629
[1509]	valid_0's auc: 0.876663
[1510]	valid_0's auc: 0.87668
[1511]	valid_0's auc: 0.876678
[1512]	valid_0's auc: 0.876697
[1513]	valid_0's auc: 0.876707
[1514]	valid_0's auc: 0.876715
[1515]	valid_0's auc: 0.876741
[1516]	valid_0's auc: 0.876764
[1517]	valid_0's auc: 0.876795
[1518]	valid_0's auc: 0.876817
[1519]	valid_0's auc: 0.87685
[1520]	valid_0's auc: 0.876875
[1521]	valid_0's auc: 0.876879
[1522]	valid_0's auc: 0.876888
[1523]	valid_0's auc: 0.876895
[1524]	valid_0's auc: 0.876921
[1525]	valid_0's auc: 0.876943
[1526]	valid_0's auc: 0.876971
[1527]	valid_0's auc: 0.876997
[1528]	valid_0's auc: 0.87701
[1529]	valid_0's auc: 0.877025
[1530]	valid_0's auc: 0.87704
[1531]	valid_0's auc: 0.877056
[1532]	valid_0's auc: 0.87708
[1533]	valid_0's auc: 0.877118
[1534]	valid_0's auc: 0.877164
[1535]	valid_0's auc: 0.877161
[1536]	valid_0's auc: 0.87718
[1537]	valid_0's auc: 0.877204
[1538]	valid_0's auc: 0.877225
[1539]	valid_0's auc: 0.877246
[1540]	valid_0's auc: 0.877256
[1541]	valid_0's auc: 0.877263
[1542]	valid_0's auc: 0.877274
[1543]	valid_0's auc: 0.877303
[1544]	valid_0's auc: 0.87732
[1545]	valid_0's auc: 0.87735
[1546]	valid_0's auc: 0.877354
[1547]	valid_0's auc: 0.877376
[1548]	valid_0's auc: 0.877395
[1549]	valid_0's auc: 0.877422
[1550]	valid_0's auc: 0.877427
[1551]	valid_0's auc: 0.877429
[1552]	valid_0's auc: 0.877412
[1553]	valid_0's auc: 0.877413
[1554]	valid_0's auc: 0.877428
[1555]	valid_0's auc: 0.877456
[1556]	valid_0's auc: 0.877487
[1557]	valid_0's auc: 0.877522
[1558]	valid_0's auc: 0.877549
[1559]	valid_0's auc: 0.877549
[1560]	valid_0's auc: 0.877557
[1561]	valid_0's auc: 0.877574
[1562]	valid_0's auc: 0.877599
[1563]	valid_0's auc: 0.877637
[1564]	valid_0's auc: 0.877646
[1565]	valid_0's auc: 0.877638
[1566]	valid_0's auc: 0.877674
[1567]	valid_0's auc: 0.877684
[1568]	valid_0's auc: 0.877694
[1569]	valid_0's auc: 0.877696
[1570]	valid_0's auc: 0.877735
[1571]	valid_0's auc: 0.877732
[1572]	valid_0's auc: 0.877752
[1573]	valid_0's auc: 0.877763
[1574]	valid_0's auc: 0.877781
[1575]	valid_0's auc: 0.877798
[1576]	valid_0's auc: 0.877812
[1577]	valid_0's auc: 0.877825
[1578]	valid_0's auc: 0.877885
[1579]	valid_0's auc: 0.877899
[1580]	valid_0's auc: 0.877915
[1581]	valid_0's auc: 0.877955
[1582]	valid_0's auc: 0.877964
[1583]	valid_0's auc: 0.87798
[1584]	valid_0's auc: 0.878002
[1585]	valid_0's auc: 0.878002
[1586]	valid_0's auc: 0.878005
[1587]	valid_0's auc: 0.878017
[1588]	valid_0's auc: 0.878044
[1589]	valid_0's auc: 0.878054
[1590]	valid_0's auc: 0.878085
[1591]	valid_0's auc: 0.878092
[1592]	valid_0's auc: 0.878087
[1593]	valid_0's auc: 0.87811
[1594]	valid_0's auc: 0.878101
[1595]	valid_0's auc: 0.878118
[1596]	valid_0's auc: 0.878142
[1597]	valid_0's auc: 0.878148
[1598]	valid_0's auc: 0.878169
[1599]	valid_0's auc: 0.878181
[1600]	valid_0's auc: 0.87819
[1601]	valid_0's auc: 0.878198
[1602]	valid_0's auc: 0.878226
[1603]	valid_0's auc: 0.878266
[1604]	valid_0's auc: 0.87828
[1605]	valid_0's auc: 0.878299
[1606]	valid_0's auc: 0.878332
[1607]	valid_0's auc: 0.878387
[1608]	valid_0's auc: 0.878382
[1609]	valid_0's auc: 0.878375
[1610]	valid_0's auc: 0.878397
[1611]	valid_0's auc: 0.878424
[1612]	valid_0's auc: 0.878459
[1613]	valid_0's auc: 0.878466
[1614]	valid_0's auc: 0.878484
[1615]	valid_0's auc: 0.87853
[1616]	valid_0's auc: 0.878508
[1617]	valid_0's auc: 0.878522
[1618]	valid_0's auc: 0.878533
[1619]	valid_0's auc: 0.87854
[1620]	valid_0's auc: 0.87855
[1621]	valid_0's auc: 0.878581
[1622]	valid_0's auc: 0.878562
[1623]	valid_0's auc: 0.878582
[1624]	valid_0's auc: 0.878613
[1625]	valid_0's auc: 0.878634
[1626]	valid_0's auc: 0.878642
[1627]	valid_0's auc: 0.878665
[1628]	valid_0's auc: 0.878688
[1629]	valid_0's auc: 0.878718
[1630]	valid_0's auc: 0.878722
[1631]	valid_0's auc: 0.878771
[1632]	valid_0's auc: 0.878772
[1633]	valid_0's auc: 0.878786
[1634]	valid_0's auc: 0.878813
[1635]	valid_0's auc: 0.878845
[1636]	valid_0's auc: 0.87886
[1637]	valid_0's auc: 0.878865
[1638]	valid_0's auc: 0.878901
[1639]	valid_0's auc: 0.87892
[1640]	valid_0's auc: 0.878908
[1641]	valid_0's auc: 0.878933
[1642]	valid_0's auc: 0.878953
[1643]	valid_0's auc: 0.878984
[1644]	valid_0's auc: 0.878996
[1645]	valid_0's auc: 0.879019
[1646]	valid_0's auc: 0.878998
[1647]	valid_0's auc: 0.879027
[1648]	valid_0's auc: 0.879047
[1649]	valid_0's auc: 0.879084
[1650]	valid_0's auc: 0.879102
[1651]	valid_0's auc: 0.879099
[1652]	valid_0's auc: 0.879103
[1653]	valid_0's auc: 0.879129
[1654]	valid_0's auc: 0.879101
[1655]	valid_0's auc: 0.879135
[1656]	valid_0's auc: 0.87916
[1657]	valid_0's auc: 0.879142
[1658]	valid_0's auc: 0.879173
[1659]	valid_0's auc: 0.879196
[1660]	valid_0's auc: 0.879211
[1661]	valid_0's auc: 0.879233
[1662]	valid_0's auc: 0.879243
[1663]	valid_0's auc: 0.879243
[1664]	valid_0's auc: 0.879286
[1665]	valid_0's auc: 0.879294
[1666]	valid_0's auc: 0.879299
[1667]	valid_0's auc: 0.879294
[1668]	valid_0's auc: 0.879315
[1669]	valid_0's auc: 0.879333
[1670]	valid_0's auc: 0.879347
[1671]	valid_0's auc: 0.879344
[1672]	valid_0's auc: 0.879339
[1673]	valid_0's auc: 0.879358
[1674]	valid_0's auc: 0.879381
[1675]	valid_0's auc: 0.8794
[1676]	valid_0's auc: 0.879416
[1677]	valid_0's auc: 0.879448
[1678]	valid_0's auc: 0.879457
[1679]	valid_0's auc: 0.879483
[1680]	valid_0's auc: 0.879508
[1681]	valid_0's auc: 0.879543
[1682]	valid_0's auc: 0.879544
[1683]	valid_0's auc: 0.87955
[1684]	valid_0's auc: 0.879539
[1685]	valid_0's auc: 0.879567
[1686]	valid_0's auc: 0.879578
[1687]	valid_0's auc: 0.879597
[1688]	valid_0's auc: 0.879622
[1689]	valid_0's auc: 0.879624
[1690]	valid_0's auc: 0.87963
[1691]	valid_0's auc: 0.879662
[1692]	valid_0's auc: 0.879675
[1693]	valid_0's auc: 0.879723
[1694]	valid_0's auc: 0.879725
[1695]	valid_0's auc: 0.879736
[1696]	valid_0's auc: 0.879732
[1697]	valid_0's auc: 0.879734
[1698]	valid_0's auc: 0.879742
[1699]	valid_0's auc: 0.879771
[1700]	valid_0's auc: 0.87976
[1701]	valid_0's auc: 0.879767
[1702]	valid_0's auc: 0.879787
[1703]	valid_0's auc: 0.879799
[1704]	valid_0's auc: 0.879825
[1705]	valid_0's auc: 0.87983
[1706]	valid_0's auc: 0.879838
[1707]	valid_0's auc: 0.879811
[1708]	valid_0's auc: 0.879823
[1709]	valid_0's auc: 0.879827
[1710]	valid_0's auc: 0.879822
[1711]	valid_0's auc: 0.879815
[1712]	valid_0's auc: 0.87984
[1713]	valid_0's auc: 0.87987
[1714]	valid_0's auc: 0.879886
[1715]	valid_0's auc: 0.879858
[1716]	valid_0's auc: 0.879887
[1717]	valid_0's auc: 0.87992
[1718]	valid_0's auc: 0.879915
[1719]	valid_0's auc: 0.879905
[1720]	valid_0's auc: 0.879886
[1721]	valid_0's auc: 0.879888
[1722]	valid_0's auc: 0.879917
[1723]	valid_0's auc: 0.879939
[1724]	valid_0's auc: 0.879979
[1725]	valid_0's auc: 0.880009
[1726]	valid_0's auc: 0.880032
[1727]	valid_0's auc: 0.880043
[1728]	valid_0's auc: 0.880061
[1729]	valid_0's auc: 0.880086
[1730]	valid_0's auc: 0.88009
[1731]	valid_0's auc: 0.880089
[1732]	valid_0's auc: 0.8801
[1733]	valid_0's auc: 0.880109
[1734]	valid_0's auc: 0.880142
[1735]	valid_0's auc: 0.880155
[1736]	valid_0's auc: 0.880172
[1737]	valid_0's auc: 0.880179
[1738]	valid_0's auc: 0.880191
[1739]	valid_0's auc: 0.880189
[1740]	valid_0's auc: 0.880216
[1741]	valid_0's auc: 0.880261
[1742]	valid_0's auc: 0.880278
[1743]	valid_0's auc: 0.880309
[1744]	valid_0's auc: 0.880323
[1745]	valid_0's auc: 0.880341
[1746]	valid_0's auc: 0.880354
[1747]	valid_0's auc: 0.88037
[1748]	valid_0's auc: 0.880399
[1749]	valid_0's auc: 0.880401
[1750]	valid_0's auc: 0.880441
[1751]	valid_0's auc: 0.88042
[1752]	valid_0's auc: 0.880453
[1753]	valid_0's auc: 0.880468
[1754]	valid_0's auc: 0.880488
[1755]	valid_0's auc: 0.880498
[1756]	valid_0's auc: 0.880511
[1757]	valid_0's auc: 0.880508
[1758]	valid_0's auc: 0.880539
[1759]	valid_0's auc: 0.880534
[1760]	valid_0's auc: 0.880553
[1761]	valid_0's auc: 0.880576
[1762]	valid_0's auc: 0.880582
[1763]	valid_0's auc: 0.880589
[1764]	valid_0's auc: 0.880617
[1765]	valid_0's auc: 0.880614
[1766]	valid_0's auc: 0.880621
[1767]	valid_0's auc: 0.88064
[1768]	valid_0's auc: 0.880668
[1769]	valid_0's auc: 0.880684
[1770]	valid_0's auc: 0.880704
[1771]	valid_0's auc: 0.880721
[1772]	valid_0's auc: 0.880718
[1773]	valid_0's auc: 0.880732
[1774]	valid_0's auc: 0.880747
[1775]	valid_0's auc: 0.880729
[1776]	valid_0's auc: 0.88074
[1777]	valid_0's auc: 0.880738
[1778]	valid_0's auc: 0.880762
[1779]	valid_0's auc: 0.880778
[1780]	valid_0's auc: 0.880787
[1781]	valid_0's auc: 0.880798
[1782]	valid_0's auc: 0.880812
[1783]	valid_0's auc: 0.880821
[1784]	valid_0's auc: 0.88083
[1785]	valid_0's auc: 0.880869
[1786]	valid_0's auc: 0.880892
[1787]	valid_0's auc: 0.880914
[1788]	valid_0's auc: 0.88093
[1789]	valid_0's auc: 0.880968
[1790]	valid_0's auc: 0.880973
[1791]	valid_0's auc: 0.880985
[1792]	valid_0's auc: 0.880995
[1793]	valid_0's auc: 0.881001
[1794]	valid_0's auc: 0.88103
[1795]	valid_0's auc: 0.881079
[1796]	valid_0's auc: 0.881094
[1797]	valid_0's auc: 0.881093
[1798]	valid_0's auc: 0.8811
[1799]	valid_0's auc: 0.881107
[1800]	valid_0's auc: 0.881149
[1801]	valid_0's auc: 0.881168
[1802]	valid_0's auc: 0.881192
[1803]	valid_0's auc: 0.881188
[1804]	valid_0's auc: 0.881177
[1805]	valid_0's auc: 0.881189
[1806]	valid_0's auc: 0.8812
[1807]	valid_0's auc: 0.88123
[1808]	valid_0's auc: 0.88123
[1809]	valid_0's auc: 0.88124
[1810]	valid_0's auc: 0.881252
[1811]	valid_0's auc: 0.881295
[1812]	valid_0's auc: 0.881313
[1813]	valid_0's auc: 0.881341
[1814]	valid_0's auc: 0.88136
[1815]	valid_0's auc: 0.881353
[1816]	valid_0's auc: 0.881362
[1817]	valid_0's auc: 0.881344
[1818]	valid_0's auc: 0.881323
[1819]	valid_0's auc: 0.881311
[1820]	valid_0's auc: 0.881307
[1821]	valid_0's auc: 0.881307
[1822]	valid_0's auc: 0.881302
[1823]	valid_0's auc: 0.8813
[1824]	valid_0's auc: 0.881332
[1825]	valid_0's auc: 0.881335
[1826]	valid_0's auc: 0.881358
[1827]	valid_0's auc: 0.881394
[1828]	valid_0's auc: 0.881392
[1829]	valid_0's auc: 0.881428
[1830]	valid_0's auc: 0.881449
[1831]	valid_0's auc: 0.881469
[1832]	valid_0's auc: 0.881496
[1833]	valid_0's auc: 0.881519
[1834]	valid_0's auc: 0.881553
[1835]	valid_0's auc: 0.881564
[1836]	valid_0's auc: 0.881555
[1837]	valid_0's auc: 0.881574
[1838]	valid_0's auc: 0.881596
[1839]	valid_0's auc: 0.88161
[1840]	valid_0's auc: 0.881609
[1841]	valid_0's auc: 0.88163
[1842]	valid_0's auc: 0.881652
[1843]	valid_0's auc: 0.88166
[1844]	valid_0's auc: 0.881671
[1845]	valid_0's auc: 0.881696
[1846]	valid_0's auc: 0.881707
[1847]	valid_0's auc: 0.881702
[1848]	valid_0's auc: 0.881713
[1849]	valid_0's auc: 0.881745
[1850]	valid_0's auc: 0.881768
[1851]	valid_0's auc: 0.881791
[1852]	valid_0's auc: 0.881797
[1853]	valid_0's auc: 0.88182
[1854]	valid_0's auc: 0.881827
[1855]	valid_0's auc: 0.88184
[1856]	valid_0's auc: 0.881839
[1857]	valid_0's auc: 0.881873
[1858]	valid_0's auc: 0.881901
[1859]	valid_0's auc: 0.881905
[1860]	valid_0's auc: 0.88192
[1861]	valid_0's auc: 0.881908
[1862]	valid_0's auc: 0.881937
[1863]	valid_0's auc: 0.881953
[1864]	valid_0's auc: 0.881966
[1865]	valid_0's auc: 0.881999
[1866]	valid_0's auc: 0.882019
[1867]	valid_0's auc: 0.882049
[1868]	valid_0's auc: 0.882057
[1869]	valid_0's auc: 0.882046
[1870]	valid_0's auc: 0.882068
[1871]	valid_0's auc: 0.882101
[1872]	valid_0's auc: 0.882118
[1873]	valid_0's auc: 0.88214
[1874]	valid_0's auc: 0.882156
[1875]	valid_0's auc: 0.882176
[1876]	valid_0's auc: 0.882187
[1877]	valid_0's auc: 0.882205
[1878]	valid_0's auc: 0.882245
[1879]	valid_0's auc: 0.882245
[1880]	valid_0's auc: 0.882262
[1881]	valid_0's auc: 0.882267
[1882]	valid_0's auc: 0.882284
[1883]	valid_0's auc: 0.882274
[1884]	valid_0's auc: 0.882282
[1885]	valid_0's auc: 0.882279
[1886]	valid_0's auc: 0.882285
[1887]	valid_0's auc: 0.882293
[1888]	valid_0's auc: 0.882313
[1889]	valid_0's auc: 0.882335
[1890]	valid_0's auc: 0.88234
[1891]	valid_0's auc: 0.882353
[1892]	valid_0's auc: 0.882387
[1893]	valid_0's auc: 0.882407
[1894]	valid_0's auc: 0.882406
[1895]	valid_0's auc: 0.882431
[1896]	valid_0's auc: 0.882443
[1897]	valid_0's auc: 0.882438
[1898]	valid_0's auc: 0.882449
[1899]	valid_0's auc: 0.882438
[1900]	valid_0's auc: 0.88246
[1901]	valid_0's auc: 0.882482
[1902]	valid_0's auc: 0.882486
[1903]	valid_0's auc: 0.882503
[1904]	valid_0's auc: 0.882509
[1905]	valid_0's auc: 0.882518
[1906]	valid_0's auc: 0.882529
[1907]	valid_0's auc: 0.882519
[1908]	valid_0's auc: 0.882559
[1909]	valid_0's auc: 0.882569
[1910]	valid_0's auc: 0.882592
[1911]	valid_0's auc: 0.882585
[1912]	valid_0's auc: 0.882586
[1913]	valid_0's auc: 0.882597
[1914]	valid_0's auc: 0.882607
[1915]	valid_0's auc: 0.882611
[1916]	valid_0's auc: 0.882582
[1917]	valid_0's auc: 0.882595
[1918]	valid_0's auc: 0.882618
[1919]	valid_0's auc: 0.88263
[1920]	valid_0's auc: 0.882649
[1921]	valid_0's auc: 0.882666
[1922]	valid_0's auc: 0.882687
[1923]	valid_0's auc: 0.882691
[1924]	valid_0's auc: 0.882708
[1925]	valid_0's auc: 0.882731
[1926]	valid_0's auc: 0.882736
[1927]	valid_0's auc: 0.882728
[1928]	valid_0's auc: 0.88274
[1929]	valid_0's auc: 0.882741
[1930]	valid_0's auc: 0.882748
[1931]	valid_0's auc: 0.882774
[1932]	valid_0's auc: 0.882777
[1933]	valid_0's auc: 0.882792
[1934]	valid_0's auc: 0.882801
[1935]	valid_0's auc: 0.882805
[1936]	valid_0's auc: 0.882798
[1937]	valid_0's auc: 0.882825
[1938]	valid_0's auc: 0.882861
[1939]	valid_0's auc: 0.882862
[1940]	valid_0's auc: 0.882876
[1941]	valid_0's auc: 0.882891
[1942]	valid_0's auc: 0.882898
[1943]	valid_0's auc: 0.882895
[1944]	valid_0's auc: 0.882896
[1945]	valid_0's auc: 0.882901
[1946]	valid_0's auc: 0.882896
[1947]	valid_0's auc: 0.882915
[1948]	valid_0's auc: 0.882933
[1949]	valid_0's auc: 0.882941
[1950]	valid_0's auc: 0.882953
[1951]	valid_0's auc: 0.882954
[1952]	valid_0's auc: 0.882948
[1953]	valid_0's auc: 0.882966
[1954]	valid_0's auc: 0.882988
[1955]	valid_0's auc: 0.882978
[1956]	valid_0's auc: 0.882998
[1957]	valid_0's auc: 0.883014
[1958]	valid_0's auc: 0.883038
[1959]	valid_0's auc: 0.883069
[1960]	valid_0's auc: 0.883078
[1961]	valid_0's auc: 0.883097
[1962]	valid_0's auc: 0.883113
[1963]	valid_0's auc: 0.883134
[1964]	valid_0's auc: 0.883146
[1965]	valid_0's auc: 0.883151
[1966]	valid_0's auc: 0.88316
[1967]	valid_0's auc: 0.883161
[1968]	valid_0's auc: 0.883177
[1969]	valid_0's auc: 0.883186
[1970]	valid_0's auc: 0.8832
[1971]	valid_0's auc: 0.883222
[1972]	valid_0's auc: 0.883228
[1973]	valid_0's auc: 0.883252
[1974]	valid_0's auc: 0.883285
[1975]	valid_0's auc: 0.883285
[1976]	valid_0's auc: 0.883287
[1977]	valid_0's auc: 0.88329
[1978]	valid_0's auc: 0.883308
[1979]	valid_0's auc: 0.883324
[1980]	valid_0's auc: 0.883338
[1981]	valid_0's auc: 0.883345
[1982]	valid_0's auc: 0.883338
[1983]	valid_0's auc: 0.883336
[1984]	valid_0's auc: 0.883369
[1985]	valid_0's auc: 0.883376
[1986]	valid_0's auc: 0.883406
[1987]	valid_0's auc: 0.883406
[1988]	valid_0's auc: 0.883426
[1989]	valid_0's auc: 0.883445
[1990]	valid_0's auc: 0.883461
[1991]	valid_0's auc: 0.883483
[1992]	valid_0's auc: 0.883495
[1993]	valid_0's auc: 0.88348
[1994]	valid_0's auc: 0.883482
[1995]	valid_0's auc: 0.883467
[1996]	valid_0's auc: 0.883476
[1997]	valid_0's auc: 0.883491
[1998]	valid_0's auc: 0.883495
[1999]	valid_0's auc: 0.883506
[2000]	valid_0's auc: 0.883511
[2001]	valid_0's auc: 0.883534
[2002]	valid_0's auc: 0.883547
[2003]	valid_0's auc: 0.883554
[2004]	valid_0's auc: 0.883554
[2005]	valid_0's auc: 0.883566
[2006]	valid_0's auc: 0.883591
[2007]	valid_0's auc: 0.883591
[2008]	valid_0's auc: 0.883587
[2009]	valid_0's auc: 0.883605
[2010]	valid_0's auc: 0.883628
[2011]	valid_0's auc: 0.883636
[2012]	valid_0's auc: 0.883646
[2013]	valid_0's auc: 0.883661
[2014]	valid_0's auc: 0.883681
[2015]	valid_0's auc: 0.883707
[2016]	valid_0's auc: 0.883735
[2017]	valid_0's auc: 0.883744
[2018]	valid_0's auc: 0.883765
[2019]	valid_0's auc: 0.883786
[2020]	valid_0's auc: 0.883811
[2021]	valid_0's auc: 0.883824
[2022]	valid_0's auc: 0.883837
[2023]	valid_0's auc: 0.883838
[2024]	valid_0's auc: 0.883849
[2025]	valid_0's auc: 0.883867
[2026]	valid_0's auc: 0.883875
[2027]	valid_0's auc: 0.883881
[2028]	valid_0's auc: 0.883894
[2029]	valid_0's auc: 0.883908
[2030]	valid_0's auc: 0.883922
[2031]	valid_0's auc: 0.88392
[2032]	valid_0's auc: 0.883945
[2033]	valid_0's auc: 0.883961
[2034]	valid_0's auc: 0.88398
[2035]	valid_0's auc: 0.884011
[2036]	valid_0's auc: 0.884024
[2037]	valid_0's auc: 0.884033
[2038]	valid_0's auc: 0.884042
[2039]	valid_0's auc: 0.884065
[2040]	valid_0's auc: 0.884093
[2041]	valid_0's auc: 0.88409
[2042]	valid_0's auc: 0.884087
[2043]	valid_0's auc: 0.88408
[2044]	valid_0's auc: 0.884091
[2045]	valid_0's auc: 0.884088
[2046]	valid_0's auc: 0.884098
[2047]	valid_0's auc: 0.884103
[2048]	valid_0's auc: 0.884122
[2049]	valid_0's auc: 0.884132
[2050]	valid_0's auc: 0.884136
[2051]	valid_0's auc: 0.884146
[2052]	valid_0's auc: 0.884159
[2053]	valid_0's auc: 0.88418
[2054]	valid_0's auc: 0.884211
[2055]	valid_0's auc: 0.884205
[2056]	valid_0's auc: 0.884217
[2057]	valid_0's auc: 0.884246
[2058]	valid_0's auc: 0.884258
[2059]	valid_0's auc: 0.884279
[2060]	valid_0's auc: 0.884287
[2061]	valid_0's auc: 0.884304
[2062]	valid_0's auc: 0.884314
[2063]	valid_0's auc: 0.884345
[2064]	valid_0's auc: 0.884359
[2065]	valid_0's auc: 0.884372
[2066]	valid_0's auc: 0.884383
[2067]	valid_0's auc: 0.884409
[2068]	valid_0's auc: 0.884416
[2069]	valid_0's auc: 0.884429
[2070]	valid_0's auc: 0.884434
[2071]	valid_0's auc: 0.884463
[2072]	valid_0's auc: 0.884483
[2073]	valid_0's auc: 0.884503
[2074]	valid_0's auc: 0.884526
[2075]	valid_0's auc: 0.884558
[2076]	valid_0's auc: 0.884583
[2077]	valid_0's auc: 0.884599
[2078]	valid_0's auc: 0.884635
[2079]	valid_0's auc: 0.884634
[2080]	valid_0's auc: 0.884659
[2081]	valid_0's auc: 0.884678
[2082]	valid_0's auc: 0.884688
[2083]	valid_0's auc: 0.884694
[2084]	valid_0's auc: 0.884703
[2085]	valid_0's auc: 0.884709
[2086]	valid_0's auc: 0.884722
[2087]	valid_0's auc: 0.884724
[2088]	valid_0's auc: 0.884737
[2089]	valid_0's auc: 0.884737
[2090]	valid_0's auc: 0.884738
[2091]	valid_0's auc: 0.884753
[2092]	valid_0's auc: 0.884759
[2093]	valid_0's auc: 0.884776
[2094]	valid_0's auc: 0.884783
[2095]	valid_0's auc: 0.884794
[2096]	valid_0's auc: 0.884805
[2097]	valid_0's auc: 0.884792
[2098]	valid_0's auc: 0.884796
[2099]	valid_0's auc: 0.884798
[2100]	valid_0's auc: 0.884772
[2101]	valid_0's auc: 0.884787
[2102]	valid_0's auc: 0.884803
[2103]	valid_0's auc: 0.884809
[2104]	valid_0's auc: 0.884823
[2105]	valid_0's auc: 0.884854
[2106]	valid_0's auc: 0.884863
[2107]	valid_0's auc: 0.884883
[2108]	valid_0's auc: 0.884902
[2109]	valid_0's auc: 0.884901
[2110]	valid_0's auc: 0.884928
[2111]	valid_0's auc: 0.884919
[2112]	valid_0's auc: 0.88493
[2113]	valid_0's auc: 0.884951
[2114]	valid_0's auc: 0.884978
[2115]	valid_0's auc: 0.884982
[2116]	valid_0's auc: 0.884994
[2117]	valid_0's auc: 0.885008
[2118]	valid_0's auc: 0.88501
[2119]	valid_0's auc: 0.885027
[2120]	valid_0's auc: 0.885032
[2121]	valid_0's auc: 0.88505
[2122]	valid_0's auc: 0.885074
[2123]	valid_0's auc: 0.88509
[2124]	valid_0's auc: 0.885106
[2125]	valid_0's auc: 0.885123
[2126]	valid_0's auc: 0.885131
[2127]	valid_0's auc: 0.88514
[2128]	valid_0's auc: 0.885149
[2129]	valid_0's auc: 0.885151
[2130]	valid_0's auc: 0.885152
[2131]	valid_0's auc: 0.885154
[2132]	valid_0's auc: 0.885171
[2133]	valid_0's auc: 0.885176
[2134]	valid_0's auc: 0.885194
[2135]	valid_0's auc: 0.885208
[2136]	valid_0's auc: 0.885208
[2137]	valid_0's auc: 0.885211
[2138]	valid_0's auc: 0.88522
[2139]	valid_0's auc: 0.885226
[2140]	valid_0's auc: 0.885235
[2141]	valid_0's auc: 0.885251
[2142]	valid_0's auc: 0.885252
[2143]	valid_0's auc: 0.885261
[2144]	valid_0's auc: 0.885266
[2145]	valid_0's auc: 0.885269
[2146]	valid_0's auc: 0.885281
[2147]	valid_0's auc: 0.88532
[2148]	valid_0's auc: 0.885334
[2149]	valid_0's auc: 0.885343
[2150]	valid_0's auc: 0.885356
[2151]	valid_0's auc: 0.885364
[2152]	valid_0's auc: 0.885367
[2153]	valid_0's auc: 0.885383
[2154]	valid_0's auc: 0.885388
[2155]	valid_0's auc: 0.885415
[2156]	valid_0's auc: 0.885432
[2157]	valid_0's auc: 0.885453
[2158]	valid_0's auc: 0.885476
[2159]	valid_0's auc: 0.885477
[2160]	valid_0's auc: 0.88548
[2161]	valid_0's auc: 0.885509
[2162]	valid_0's auc: 0.885523
[2163]	valid_0's auc: 0.885531
[2164]	valid_0's auc: 0.885542
[2165]	valid_0's auc: 0.88558
[2166]	valid_0's auc: 0.88559
[2167]	valid_0's auc: 0.885613
[2168]	valid_0's auc: 0.885623
[2169]	valid_0's auc: 0.885651
[2170]	valid_0's auc: 0.88566
[2171]	valid_0's auc: 0.885682
[2172]	valid_0's auc: 0.885698
[2173]	valid_0's auc: 0.885718
[2174]	valid_0's auc: 0.885725
[2175]	valid_0's auc: 0.88574
[2176]	valid_0's auc: 0.885745
[2177]	valid_0's auc: 0.885755
[2178]	valid_0's auc: 0.885752
[2179]	valid_0's auc: 0.885773
[2180]	valid_0's auc: 0.885786
[2181]	valid_0's auc: 0.885812
[2182]	valid_0's auc: 0.885831
[2183]	valid_0's auc: 0.885847
[2184]	valid_0's auc: 0.885847
[2185]	valid_0's auc: 0.88586
[2186]	valid_0's auc: 0.885879
[2187]	valid_0's auc: 0.885877
[2188]	valid_0's auc: 0.885881
[2189]	valid_0's auc: 0.885881
[2190]	valid_0's auc: 0.885899
[2191]	valid_0's auc: 0.885886
[2192]	valid_0's auc: 0.885909
[2193]	valid_0's auc: 0.885922
[2194]	valid_0's auc: 0.885931
[2195]	valid_0's auc: 0.885944
[2196]	valid_0's auc: 0.885963
[2197]	valid_0's auc: 0.885985
[2198]	valid_0's auc: 0.886008
[2199]	valid_0's auc: 0.886027
[2200]	valid_0's auc: 0.886015
[2201]	valid_0's auc: 0.886043
[2202]	valid_0's auc: 0.886052
[2203]	valid_0's auc: 0.886067
[2204]	valid_0's auc: 0.886067
[2205]	valid_0's auc: 0.88609
[2206]	valid_0's auc: 0.886096
[2207]	valid_0's auc: 0.886066
[2208]	valid_0's auc: 0.886088
[2209]	valid_0's auc: 0.886103
[2210]	valid_0's auc: 0.88612
[2211]	valid_0's auc: 0.886144
[2212]	valid_0's auc: 0.886144
[2213]	valid_0's auc: 0.886142
[2214]	valid_0's auc: 0.886153
[2215]	valid_0's auc: 0.886156
[2216]	valid_0's auc: 0.886181
[2217]	valid_0's auc: 0.886194
[2218]	valid_0's auc: 0.88621
[2219]	valid_0's auc: 0.886219
[2220]	valid_0's auc: 0.886225
[2221]	valid_0's auc: 0.886211
[2222]	valid_0's auc: 0.886211
[2223]	valid_0's auc: 0.886214
[2224]	valid_0's auc: 0.886224
[2225]	valid_0's auc: 0.886228
[2226]	valid_0's auc: 0.886242
[2227]	valid_0's auc: 0.886261
[2228]	valid_0's auc: 0.886258
[2229]	valid_0's auc: 0.886272
[2230]	valid_0's auc: 0.886269
[2231]	valid_0's auc: 0.886291
[2232]	valid_0's auc: 0.886293
[2233]	valid_0's auc: 0.886287
[2234]	valid_0's auc: 0.886306
[2235]	valid_0's auc: 0.886325
[2236]	valid_0's auc: 0.886339
[2237]	valid_0's auc: 0.886328
[2238]	valid_0's auc: 0.886357
[2239]	valid_0's auc: 0.886376
[2240]	valid_0's auc: 0.88639
[2241]	valid_0's auc: 0.886402
[2242]	valid_0's auc: 0.88641
[2243]	valid_0's auc: 0.886403
[2244]	valid_0's auc: 0.886397
[2245]	valid_0's auc: 0.886409
[2246]	valid_0's auc: 0.886421
[2247]	valid_0's auc: 0.886442
[2248]	valid_0's auc: 0.886467
[2249]	valid_0's auc: 0.886465
[2250]	valid_0's auc: 0.886467
[2251]	valid_0's auc: 0.886482
[2252]	valid_0's auc: 0.886504
[2253]	valid_0's auc: 0.8865
[2254]	valid_0's auc: 0.886524
[2255]	valid_0's auc: 0.886526
[2256]	valid_0's auc: 0.886549
[2257]	valid_0's auc: 0.886565
[2258]	valid_0's auc: 0.886568
[2259]	valid_0's auc: 0.886595
[2260]	valid_0's auc: 0.886587
[2261]	valid_0's auc: 0.886621
[2262]	valid_0's auc: 0.886635
[2263]	valid_0's auc: 0.886649
[2264]	valid_0's auc: 0.886648
[2265]	valid_0's auc: 0.886657
[2266]	valid_0's auc: 0.886666
[2267]	valid_0's auc: 0.886658
[2268]	valid_0's auc: 0.886666
[2269]	valid_0's auc: 0.886665
[2270]	valid_0's auc: 0.886674
[2271]	valid_0's auc: 0.886684
[2272]	valid_0's auc: 0.886679
[2273]	valid_0's auc: 0.886669
[2274]	valid_0's auc: 0.886664
[2275]	valid_0's auc: 0.886651
[2276]	valid_0's auc: 0.886664
[2277]	valid_0's auc: 0.886689
[2278]	valid_0's auc: 0.886692
[2279]	valid_0's auc: 0.8867
[2280]	valid_0's auc: 0.88671
[2281]	valid_0's auc: 0.886725
[2282]	valid_0's auc: 0.886731
[2283]	valid_0's auc: 0.886732
[2284]	valid_0's auc: 0.886755
[2285]	valid_0's auc: 0.886767
[2286]	valid_0's auc: 0.88678
[2287]	valid_0's auc: 0.886787
[2288]	valid_0's auc: 0.886801
[2289]	valid_0's auc: 0.886809
[2290]	valid_0's auc: 0.886835
[2291]	valid_0's auc: 0.886804
[2292]	valid_0's auc: 0.886802
[2293]	valid_0's auc: 0.886805
[2294]	valid_0's auc: 0.886822
[2295]	valid_0's auc: 0.886824
[2296]	valid_0's auc: 0.886834
[2297]	valid_0's auc: 0.886855
[2298]	valid_0's auc: 0.886872
[2299]	valid_0's auc: 0.886868
[2300]	valid_0's auc: 0.886884
[2301]	valid_0's auc: 0.886884
[2302]	valid_0's auc: 0.886898
[2303]	valid_0's auc: 0.886933
[2304]	valid_0's auc: 0.886943
[2305]	valid_0's auc: 0.886947
[2306]	valid_0's auc: 0.886958
[2307]	valid_0's auc: 0.886969
[2308]	valid_0's auc: 0.886973
[2309]	valid_0's auc: 0.886994
[2310]	valid_0's auc: 0.886988
[2311]	valid_0's auc: 0.887003
[2312]	valid_0's auc: 0.887024
[2313]	valid_0's auc: 0.887032
[2314]	valid_0's auc: 0.887049
[2315]	valid_0's auc: 0.887053
[2316]	valid_0's auc: 0.887062
[2317]	valid_0's auc: 0.887067
[2318]	valid_0's auc: 0.887079
[2319]	valid_0's auc: 0.887077
[2320]	valid_0's auc: 0.887101
[2321]	valid_0's auc: 0.887112
[2322]	valid_0's auc: 0.887118
[2323]	valid_0's auc: 0.887123
[2324]	valid_0's auc: 0.887137
[2325]	valid_0's auc: 0.887146
[2326]	valid_0's auc: 0.88715
[2327]	valid_0's auc: 0.887169
[2328]	valid_0's auc: 0.887182
[2329]	valid_0's auc: 0.887192
[2330]	valid_0's auc: 0.8872
[2331]	valid_0's auc: 0.887206
[2332]	valid_0's auc: 0.887213
[2333]	valid_0's auc: 0.887246
[2334]	valid_0's auc: 0.887245
[2335]	valid_0's auc: 0.887244
[2336]	valid_0's auc: 0.887257
[2337]	valid_0's auc: 0.887277
[2338]	valid_0's auc: 0.887292
[2339]	valid_0's auc: 0.887298
[2340]	valid_0's auc: 0.887322
[2341]	valid_0's auc: 0.887349
[2342]	valid_0's auc: 0.887358
[2343]	valid_0's auc: 0.887362
[2344]	valid_0's auc: 0.887378
[2345]	valid_0's auc: 0.887399
[2346]	valid_0's auc: 0.887397
[2347]	valid_0's auc: 0.887391
[2348]	valid_0's auc: 0.887401
[2349]	valid_0's auc: 0.887411
[2350]	valid_0's auc: 0.887402
[2351]	valid_0's auc: 0.887407
[2352]	valid_0's auc: 0.887405
[2353]	valid_0's auc: 0.887423
[2354]	valid_0's auc: 0.887441
[2355]	valid_0's auc: 0.887453
[2356]	valid_0's auc: 0.887462
[2357]	valid_0's auc: 0.88748
[2358]	valid_0's auc: 0.887481
[2359]	valid_0's auc: 0.887492
[2360]	valid_0's auc: 0.887489
[2361]	valid_0's auc: 0.887498
[2362]	valid_0's auc: 0.88751
[2363]	valid_0's auc: 0.887531
[2364]	valid_0's auc: 0.887546
[2365]	valid_0's auc: 0.887563
[2366]	valid_0's auc: 0.887572
[2367]	valid_0's auc: 0.887581
[2368]	valid_0's auc: 0.887588
[2369]	valid_0's auc: 0.887572
[2370]	valid_0's auc: 0.887579
[2371]	valid_0's auc: 0.887585
[2372]	valid_0's auc: 0.887579
[2373]	valid_0's auc: 0.887603
[2374]	valid_0's auc: 0.887622
[2375]	valid_0's auc: 0.887631
[2376]	valid_0's auc: 0.887651
[2377]	valid_0's auc: 0.887678
[2378]	valid_0's auc: 0.887694
[2379]	valid_0's auc: 0.887715
[2380]	valid_0's auc: 0.887715
[2381]	valid_0's auc: 0.887717
[2382]	valid_0's auc: 0.887731
[2383]	valid_0's auc: 0.887746
[2384]	valid_0's auc: 0.887737
[2385]	valid_0's auc: 0.887756
[2386]	valid_0's auc: 0.887751
[2387]	valid_0's auc: 0.887755
[2388]	valid_0's auc: 0.88776
[2389]	valid_0's auc: 0.887758
[2390]	valid_0's auc: 0.887762
[2391]	valid_0's auc: 0.887774
[2392]	valid_0's auc: 0.887784
[2393]	valid_0's auc: 0.887791
[2394]	valid_0's auc: 0.887808
[2395]	valid_0's auc: 0.88781
[2396]	valid_0's auc: 0.887824
[2397]	valid_0's auc: 0.887826
[2398]	valid_0's auc: 0.887835
[2399]	valid_0's auc: 0.887847
[2400]	valid_0's auc: 0.887865
[2401]	valid_0's auc: 0.887882
[2402]	valid_0's auc: 0.887873
[2403]	valid_0's auc: 0.887889
[2404]	valid_0's auc: 0.887897
[2405]	valid_0's auc: 0.887907
[2406]	valid_0's auc: 0.88792
[2407]	valid_0's auc: 0.887923
[2408]	valid_0's auc: 0.887942
[2409]	valid_0's auc: 0.887935
[2410]	valid_0's auc: 0.887952
[2411]	valid_0's auc: 0.887973
[2412]	valid_0's auc: 0.888
[2413]	valid_0's auc: 0.888007
[2414]	valid_0's auc: 0.888016
[2415]	valid_0's auc: 0.888024
[2416]	valid_0's auc: 0.888043
[2417]	valid_0's auc: 0.888049
[2418]	valid_0's auc: 0.888066
[2419]	valid_0's auc: 0.888074
[2420]	valid_0's auc: 0.88808
[2421]	valid_0's auc: 0.888076
[2422]	valid_0's auc: 0.888088
[2423]	valid_0's auc: 0.888096
[2424]	valid_0's auc: 0.888123
[2425]	valid_0's auc: 0.888146
[2426]	valid_0's auc: 0.888149
[2427]	valid_0's auc: 0.888165
[2428]	valid_0's auc: 0.888171
[2429]	valid_0's auc: 0.888181
[2430]	valid_0's auc: 0.888182
[2431]	valid_0's auc: 0.888212
[2432]	valid_0's auc: 0.888219
[2433]	valid_0's auc: 0.888227
[2434]	valid_0's auc: 0.888229
[2435]	valid_0's auc: 0.888242
[2436]	valid_0's auc: 0.888259
[2437]	valid_0's auc: 0.888274
[2438]	valid_0's auc: 0.888285
[2439]	valid_0's auc: 0.888294
[2440]	valid_0's auc: 0.888302
[2441]	valid_0's auc: 0.888321
[2442]	valid_0's auc: 0.888318
[2443]	valid_0's auc: 0.88835
[2444]	valid_0's auc: 0.888348
[2445]	valid_0's auc: 0.888356
[2446]	valid_0's auc: 0.888359
[2447]	valid_0's auc: 0.888368
[2448]	valid_0's auc: 0.888364
[2449]	valid_0's auc: 0.888364
[2450]	valid_0's auc: 0.888364
[2451]	valid_0's auc: 0.888367
[2452]	valid_0's auc: 0.888369
[2453]	valid_0's auc: 0.888388
[2454]	valid_0's auc: 0.888395
[2455]	valid_0's auc: 0.888397
[2456]	valid_0's auc: 0.888412
[2457]	valid_0's auc: 0.888404
[2458]	valid_0's auc: 0.888409
[2459]	valid_0's auc: 0.888425
[2460]	valid_0's auc: 0.888429
[2461]	valid_0's auc: 0.888439
[2462]	valid_0's auc: 0.888451
[2463]	valid_0's auc: 0.888475
[2464]	valid_0's auc: 0.888466
[2465]	valid_0's auc: 0.88848
[2466]	valid_0's auc: 0.888499
[2467]	valid_0's auc: 0.888505
[2468]	valid_0's auc: 0.888517
[2469]	valid_0's auc: 0.888531
[2470]	valid_0's auc: 0.888545
[2471]	valid_0's auc: 0.888543
[2472]	valid_0's auc: 0.888555
[2473]	valid_0's auc: 0.888578
[2474]	valid_0's auc: 0.888602
[2475]	valid_0's auc: 0.888625
[2476]	valid_0's auc: 0.888634
[2477]	valid_0's auc: 0.888648
[2478]	valid_0's auc: 0.888654
[2479]	valid_0's auc: 0.888664
[2480]	valid_0's auc: 0.888678
[2481]	valid_0's auc: 0.888687
[2482]	valid_0's auc: 0.888684
[2483]	valid_0's auc: 0.888699
[2484]	valid_0's auc: 0.888711
[2485]	valid_0's auc: 0.888725
[2486]	valid_0's auc: 0.888735
[2487]	valid_0's auc: 0.888732
[2488]	valid_0's auc: 0.88874
[2489]	valid_0's auc: 0.888741
[2490]	valid_0's auc: 0.888744
[2491]	valid_0's auc: 0.888757
[2492]	valid_0's auc: 0.888773
[2493]	valid_0's auc: 0.888762
[2494]	valid_0's auc: 0.88876
[2495]	valid_0's auc: 0.888766
[2496]	valid_0's auc: 0.888771
[2497]	valid_0's auc: 0.888773
[2498]	valid_0's auc: 0.888766
[2499]	valid_0's auc: 0.888762
[2500]	valid_0's auc: 0.888765
[2501]	valid_0's auc: 0.888768
[2502]	valid_0's auc: 0.888771
[2503]	valid_0's auc: 0.88877
[2504]	valid_0's auc: 0.888774
[2505]	valid_0's auc: 0.88878
[2506]	valid_0's auc: 0.888791
[2507]	valid_0's auc: 0.888792
[2508]	valid_0's auc: 0.888792
[2509]	valid_0's auc: 0.8888
[2510]	valid_0's auc: 0.888802
[2511]	valid_0's auc: 0.888804
[2512]	valid_0's auc: 0.888808
[2513]	valid_0's auc: 0.8888
[2514]	valid_0's auc: 0.88881
[2515]	valid_0's auc: 0.888813
[2516]	valid_0's auc: 0.888828
[2517]	valid_0's auc: 0.888852
[2518]	valid_0's auc: 0.888856
[2519]	valid_0's auc: 0.888859
[2520]	valid_0's auc: 0.888875
[2521]	valid_0's auc: 0.888869
[2522]	valid_0's auc: 0.88888
[2523]	valid_0's auc: 0.888892
[2524]	valid_0's auc: 0.888898
[2525]	valid_0's auc: 0.888917
[2526]	valid_0's auc: 0.888916
[2527]	valid_0's auc: 0.888925
[2528]	valid_0's auc: 0.88894
[2529]	valid_0's auc: 0.888939
[2530]	valid_0's auc: 0.888938
[2531]	valid_0's auc: 0.888938
[2532]	valid_0's auc: 0.888948
[2533]	valid_0's auc: 0.888958
[2534]	valid_0's auc: 0.888966
[2535]	valid_0's auc: 0.888962
[2536]	valid_0's auc: 0.888987
[2537]	valid_0's auc: 0.888973
[2538]	valid_0's auc: 0.888982
[2539]	valid_0's auc: 0.888972
[2540]	valid_0's auc: 0.888981
[2541]	valid_0's auc: 0.888989
[2542]	valid_0's auc: 0.889008
[2543]	valid_0's auc: 0.889025
[2544]	valid_0's auc: 0.889042
[2545]	valid_0's auc: 0.889059
[2546]	valid_0's auc: 0.889047
[2547]	valid_0's auc: 0.889048
[2548]	valid_0's auc: 0.889055
[2549]	valid_0's auc: 0.889064
[2550]	valid_0's auc: 0.889062
[2551]	valid_0's auc: 0.889061
[2552]	valid_0's auc: 0.889065
[2553]	valid_0's auc: 0.889063
[2554]	valid_0's auc: 0.889068
[2555]	valid_0's auc: 0.889081
[2556]	valid_0's auc: 0.889083
[2557]	valid_0's auc: 0.889081
[2558]	valid_0's auc: 0.889086
[2559]	valid_0's auc: 0.889101
[2560]	valid_0's auc: 0.889103
[2561]	valid_0's auc: 0.889125
[2562]	valid_0's auc: 0.889131
[2563]	valid_0's auc: 0.889144
[2564]	valid_0's auc: 0.889161
[2565]	valid_0's auc: 0.889169
[2566]	valid_0's auc: 0.889177
[2567]	valid_0's auc: 0.889187
[2568]	valid_0's auc: 0.889205
[2569]	valid_0's auc: 0.889225
[2570]	valid_0's auc: 0.889221
[2571]	valid_0's auc: 0.889236
[2572]	valid_0's auc: 0.88925
[2573]	valid_0's auc: 0.889262
[2574]	valid_0's auc: 0.889269
[2575]	valid_0's auc: 0.889269
[2576]	valid_0's auc: 0.889268
[2577]	valid_0's auc: 0.889272
[2578]	valid_0's auc: 0.889271
[2579]	valid_0's auc: 0.889284
[2580]	valid_0's auc: 0.889298
[2581]	valid_0's auc: 0.889304
[2582]	valid_0's auc: 0.889315
[2583]	valid_0's auc: 0.889324
[2584]	valid_0's auc: 0.889336
[2585]	valid_0's auc: 0.889352
[2586]	valid_0's auc: 0.889357
[2587]	valid_0's auc: 0.889372
[2588]	valid_0's auc: 0.889388
[2589]	valid_0's auc: 0.889401
[2590]	valid_0's auc: 0.889403
[2591]	valid_0's auc: 0.889406
[2592]	valid_0's auc: 0.889419
[2593]	valid_0's auc: 0.88942
[2594]	valid_0's auc: 0.889426
[2595]	valid_0's auc: 0.889437
[2596]	valid_0's auc: 0.889428
[2597]	valid_0's auc: 0.889425
[2598]	valid_0's auc: 0.889439
[2599]	valid_0's auc: 0.889438
[2600]	valid_0's auc: 0.889444
[2601]	valid_0's auc: 0.889439
[2602]	valid_0's auc: 0.889465
[2603]	valid_0's auc: 0.889471
[2604]	valid_0's auc: 0.889467
[2605]	valid_0's auc: 0.889484
[2606]	valid_0's auc: 0.889489
[2607]	valid_0's auc: 0.889493
[2608]	valid_0's auc: 0.889498
[2609]	valid_0's auc: 0.889502
[2610]	valid_0's auc: 0.889501
[2611]	valid_0's auc: 0.889509
[2612]	valid_0's auc: 0.889509
[2613]	valid_0's auc: 0.88952
[2614]	valid_0's auc: 0.88954
[2615]	valid_0's auc: 0.88954
[2616]	valid_0's auc: 0.889547
[2617]	valid_0's auc: 0.889565
[2618]	valid_0's auc: 0.889573
[2619]	valid_0's auc: 0.889581
[2620]	valid_0's auc: 0.889583
[2621]	valid_0's auc: 0.889588
[2622]	valid_0's auc: 0.889586
[2623]	valid_0's auc: 0.889595
[2624]	valid_0's auc: 0.889597
[2625]	valid_0's auc: 0.8896
[2626]	valid_0's auc: 0.889615
[2627]	valid_0's auc: 0.889629
[2628]	valid_0's auc: 0.889642
[2629]	valid_0's auc: 0.88965
[2630]	valid_0's auc: 0.889663
[2631]	valid_0's auc: 0.889662
[2632]	valid_0's auc: 0.889665
[2633]	valid_0's auc: 0.889669
[2634]	valid_0's auc: 0.889678
[2635]	valid_0's auc: 0.889684
[2636]	valid_0's auc: 0.889691
[2637]	valid_0's auc: 0.889702
[2638]	valid_0's auc: 0.889708
[2639]	valid_0's auc: 0.889716
[2640]	valid_0's auc: 0.889718
[2641]	valid_0's auc: 0.889733
[2642]	valid_0's auc: 0.889726
[2643]	valid_0's auc: 0.889742
[2644]	valid_0's auc: 0.889737
[2645]	valid_0's auc: 0.889738
[2646]	valid_0's auc: 0.889753
[2647]	valid_0's auc: 0.889777
[2648]	valid_0's auc: 0.889796
[2649]	valid_0's auc: 0.889809
[2650]	valid_0's auc: 0.889827
[2651]	valid_0's auc: 0.889827
[2652]	valid_0's auc: 0.889836
[2653]	valid_0's auc: 0.889838
[2654]	valid_0's auc: 0.889844
[2655]	valid_0's auc: 0.889846
[2656]	valid_0's auc: 0.889864
[2657]	valid_0's auc: 0.889871
[2658]	valid_0's auc: 0.889883
[2659]	valid_0's auc: 0.889881
[2660]	valid_0's auc: 0.889882
[2661]	valid_0's auc: 0.889893
[2662]	valid_0's auc: 0.889895
[2663]	valid_0's auc: 0.889913
[2664]	valid_0's auc: 0.889925
[2665]	valid_0's auc: 0.889925
[2666]	valid_0's auc: 0.889939
[2667]	valid_0's auc: 0.889945
[2668]	valid_0's auc: 0.889969
[2669]	valid_0's auc: 0.889967
[2670]	valid_0's auc: 0.889988
[2671]	valid_0's auc: 0.889988
[2672]	valid_0's auc: 0.889974
[2673]	valid_0's auc: 0.889989
[2674]	valid_0's auc: 0.889993
[2675]	valid_0's auc: 0.89
[2676]	valid_0's auc: 0.890007
[2677]	valid_0's auc: 0.890002
[2678]	valid_0's auc: 0.890005
[2679]	valid_0's auc: 0.890001
[2680]	valid_0's auc: 0.890007
[2681]	valid_0's auc: 0.890023
[2682]	valid_0's auc: 0.890036
[2683]	valid_0's auc: 0.89003
[2684]	valid_0's auc: 0.890032
[2685]	valid_0's auc: 0.890047
[2686]	valid_0's auc: 0.890056
[2687]	valid_0's auc: 0.890068
[2688]	valid_0's auc: 0.890075
[2689]	valid_0's auc: 0.890065
[2690]	valid_0's auc: 0.890062
[2691]	valid_0's auc: 0.890058
[2692]	valid_0's auc: 0.89006
[2693]	valid_0's auc: 0.890066
[2694]	valid_0's auc: 0.890068
[2695]	valid_0's auc: 0.89007
[2696]	valid_0's auc: 0.890077
[2697]	valid_0's auc: 0.890086
[2698]	valid_0's auc: 0.890101
[2699]	valid_0's auc: 0.890116
[2700]	valid_0's auc: 0.890124
[2701]	valid_0's auc: 0.89014
[2702]	valid_0's auc: 0.890162
[2703]	valid_0's auc: 0.890163
[2704]	valid_0's auc: 0.890163
[2705]	valid_0's auc: 0.890171
[2706]	valid_0's auc: 0.890171
[2707]	valid_0's auc: 0.890179
[2708]	valid_0's auc: 0.890192
[2709]	valid_0's auc: 0.890207
[2710]	valid_0's auc: 0.890208
[2711]	valid_0's auc: 0.89022
[2712]	valid_0's auc: 0.890215
[2713]	valid_0's auc: 0.890223
[2714]	valid_0's auc: 0.890229
[2715]	valid_0's auc: 0.890243
[2716]	valid_0's auc: 0.890244
[2717]	valid_0's auc: 0.890249
[2718]	valid_0's auc: 0.890255
[2719]	valid_0's auc: 0.890262
[2720]	valid_0's auc: 0.890266
[2721]	valid_0's auc: 0.890273
[2722]	valid_0's auc: 0.890281
[2723]	valid_0's auc: 0.890277
[2724]	valid_0's auc: 0.890291
[2725]	valid_0's auc: 0.890304
[2726]	valid_0's auc: 0.890307
[2727]	valid_0's auc: 0.890313
[2728]	valid_0's auc: 0.890301
[2729]	valid_0's auc: 0.890292
[2730]	valid_0's auc: 0.890308
[2731]	valid_0's auc: 0.890322
[2732]	valid_0's auc: 0.890329
[2733]	valid_0's auc: 0.890331
[2734]	valid_0's auc: 0.890354
[2735]	valid_0's auc: 0.890366
[2736]	valid_0's auc: 0.890389
[2737]	valid_0's auc: 0.890402
[2738]	valid_0's auc: 0.890407
[2739]	valid_0's auc: 0.890412
[2740]	valid_0's auc: 0.890411
[2741]	valid_0's auc: 0.890413
[2742]	valid_0's auc: 0.890423
[2743]	valid_0's auc: 0.890434
[2744]	valid_0's auc: 0.89045
[2745]	valid_0's auc: 0.890453
[2746]	valid_0's auc: 0.890459
[2747]	valid_0's auc: 0.890456
[2748]	valid_0's auc: 0.890443
[2749]	valid_0's auc: 0.890452
[2750]	valid_0's auc: 0.890453
[2751]	valid_0's auc: 0.890463
[2752]	valid_0's auc: 0.890467
[2753]	valid_0's auc: 0.890481
[2754]	valid_0's auc: 0.890489
[2755]	valid_0's auc: 0.890503
[2756]	valid_0's auc: 0.89051
[2757]	valid_0's auc: 0.890517
[2758]	valid_0's auc: 0.890516
[2759]	valid_0's auc: 0.890509
[2760]	valid_0's auc: 0.890517
[2761]	valid_0's auc: 0.890533
[2762]	valid_0's auc: 0.890539
[2763]	valid_0's auc: 0.890541
[2764]	valid_0's auc: 0.890534
[2765]	valid_0's auc: 0.890544
[2766]	valid_0's auc: 0.89055
[2767]	valid_0's auc: 0.890562
[2768]	valid_0's auc: 0.890566
[2769]	valid_0's auc: 0.890585
[2770]	valid_0's auc: 0.890608
[2771]	valid_0's auc: 0.890622
[2772]	valid_0's auc: 0.890631
[2773]	valid_0's auc: 0.890633
[2774]	valid_0's auc: 0.89063
[2775]	valid_0's auc: 0.890626
[2776]	valid_0's auc: 0.890638
[2777]	valid_0's auc: 0.890634
[2778]	valid_0's auc: 0.890642
[2779]	valid_0's auc: 0.890643
[2780]	valid_0's auc: 0.890649
[2781]	valid_0's auc: 0.890654
[2782]	valid_0's auc: 0.890663
[2783]	valid_0's auc: 0.890673
[2784]	valid_0's auc: 0.890691
[2785]	valid_0's auc: 0.890701
[2786]	valid_0's auc: 0.890711
[2787]	valid_0's auc: 0.890719
[2788]	valid_0's auc: 0.890724
[2789]	valid_0's auc: 0.890727
[2790]	valid_0's auc: 0.890732
[2791]	valid_0's auc: 0.890722
[2792]	valid_0's auc: 0.890725
[2793]	valid_0's auc: 0.890733
[2794]	valid_0's auc: 0.89074
[2795]	valid_0's auc: 0.890735
[2796]	valid_0's auc: 0.89075
[2797]	valid_0's auc: 0.890764
[2798]	valid_0's auc: 0.890769
[2799]	valid_0's auc: 0.890767
[2800]	valid_0's auc: 0.890774
[2801]	valid_0's auc: 0.890781
[2802]	valid_0's auc: 0.890798
[2803]	valid_0's auc: 0.890801
[2804]	valid_0's auc: 0.890791
[2805]	valid_0's auc: 0.890803
[2806]	valid_0's auc: 0.89081
[2807]	valid_0's auc: 0.89082
[2808]	valid_0's auc: 0.890814
[2809]	valid_0's auc: 0.890821
[2810]	valid_0's auc: 0.890818
[2811]	valid_0's auc: 0.890831
[2812]	valid_0's auc: 0.890851
[2813]	valid_0's auc: 0.89085
[2814]	valid_0's auc: 0.89086
[2815]	valid_0's auc: 0.890865
[2816]	valid_0's auc: 0.890861
[2817]	valid_0's auc: 0.890869
[2818]	valid_0's auc: 0.890856
[2819]	valid_0's auc: 0.890863
[2820]	valid_0's auc: 0.89087
[2821]	valid_0's auc: 0.89087
[2822]	valid_0's auc: 0.890866
[2823]	valid_0's auc: 0.890878
[2824]	valid_0's auc: 0.890871
[2825]	valid_0's auc: 0.890876
[2826]	valid_0's auc: 0.89088
[2827]	valid_0's auc: 0.890891
[2828]	valid_0's auc: 0.890896
[2829]	valid_0's auc: 0.890894
[2830]	valid_0's auc: 0.890889
[2831]	valid_0's auc: 0.890886
[2832]	valid_0's auc: 0.890901
[2833]	valid_0's auc: 0.890905
[2834]	valid_0's auc: 0.890903
[2835]	valid_0's auc: 0.890903
[2836]	valid_0's auc: 0.890904
[2837]	valid_0's auc: 0.890909
[2838]	valid_0's auc: 0.890926
[2839]	valid_0's auc: 0.890919
[2840]	valid_0's auc: 0.890918
[2841]	valid_0's auc: 0.890932
[2842]	valid_0's auc: 0.890937
[2843]	valid_0's auc: 0.890945
[2844]	valid_0's auc: 0.890947
[2845]	valid_0's auc: 0.890955
[2846]	valid_0's auc: 0.890954
[2847]	valid_0's auc: 0.890961
[2848]	valid_0's auc: 0.89097
[2849]	valid_0's auc: 0.890982
[2850]	valid_0's auc: 0.890988
[2851]	valid_0's auc: 0.890985
[2852]	valid_0's auc: 0.890986
[2853]	valid_0's auc: 0.890984
[2854]	valid_0's auc: 0.890998
[2855]	valid_0's auc: 0.891021
[2856]	valid_0's auc: 0.891027
[2857]	valid_0's auc: 0.891043
[2858]	valid_0's auc: 0.891055
[2859]	valid_0's auc: 0.891059
[2860]	valid_0's auc: 0.891066
[2861]	valid_0's auc: 0.891074
[2862]	valid_0's auc: 0.891081
[2863]	valid_0's auc: 0.891092
[2864]	valid_0's auc: 0.891079
[2865]	valid_0's auc: 0.891094
[2866]	valid_0's auc: 0.891095
[2867]	valid_0's auc: 0.891098
[2868]	valid_0's auc: 0.891102
[2869]	valid_0's auc: 0.891109
[2870]	valid_0's auc: 0.89111
[2871]	valid_0's auc: 0.891107
[2872]	valid_0's auc: 0.891117
[2873]	valid_0's auc: 0.891129
[2874]	valid_0's auc: 0.891131
[2875]	valid_0's auc: 0.891138
[2876]	valid_0's auc: 0.891141
[2877]	valid_0's auc: 0.891155
[2878]	valid_0's auc: 0.891155
[2879]	valid_0's auc: 0.891157
[2880]	valid_0's auc: 0.891162
[2881]	valid_0's auc: 0.891179
[2882]	valid_0's auc: 0.891186
[2883]	valid_0's auc: 0.891178
[2884]	valid_0's auc: 0.891187
[2885]	valid_0's auc: 0.891201
[2886]	valid_0's auc: 0.891212
[2887]	valid_0's auc: 0.891216
[2888]	valid_0's auc: 0.891226
[2889]	valid_0's auc: 0.89123
[2890]	valid_0's auc: 0.891232
[2891]	valid_0's auc: 0.891249
[2892]	valid_0's auc: 0.89125
[2893]	valid_0's auc: 0.891242
[2894]	valid_0's auc: 0.891239
[2895]	valid_0's auc: 0.891237
[2896]	valid_0's auc: 0.891241
[2897]	valid_0's auc: 0.891247
[2898]	valid_0's auc: 0.891248
[2899]	valid_0's auc: 0.891258
[2900]	valid_0's auc: 0.891263
[2901]	valid_0's auc: 0.891266
[2902]	valid_0's auc: 0.89127
[2903]	valid_0's auc: 0.891272
[2904]	valid_0's auc: 0.89128
[2905]	valid_0's auc: 0.891283
[2906]	valid_0's auc: 0.891288
[2907]	valid_0's auc: 0.891283
[2908]	valid_0's auc: 0.891282
[2909]	valid_0's auc: 0.89129
[2910]	valid_0's auc: 0.89129
[2911]	valid_0's auc: 0.891296
[2912]	valid_0's auc: 0.891301
[2913]	valid_0's auc: 0.891305
[2914]	valid_0's auc: 0.891311
[2915]	valid_0's auc: 0.891313
[2916]	valid_0's auc: 0.891316
[2917]	valid_0's auc: 0.891333
[2918]	valid_0's auc: 0.891331
[2919]	valid_0's auc: 0.89134
[2920]	valid_0's auc: 0.89133
[2921]	valid_0's auc: 0.891342
[2922]	valid_0's auc: 0.891344
[2923]	valid_0's auc: 0.891341
[2924]	valid_0's auc: 0.891345
[2925]	valid_0's auc: 0.89135
[2926]	valid_0's auc: 0.89135
[2927]	valid_0's auc: 0.891355
[2928]	valid_0's auc: 0.891358
[2929]	valid_0's auc: 0.891358
[2930]	valid_0's auc: 0.891372
[2931]	valid_0's auc: 0.891385
[2932]	valid_0's auc: 0.891389
[2933]	valid_0's auc: 0.891391
[2934]	valid_0's auc: 0.891389
[2935]	valid_0's auc: 0.891386
[2936]	valid_0's auc: 0.891399
[2937]	valid_0's auc: 0.891412
[2938]	valid_0's auc: 0.891424
[2939]	valid_0's auc: 0.891433
[2940]	valid_0's auc: 0.891445
[2941]	valid_0's auc: 0.891452
[2942]	valid_0's auc: 0.891461
[2943]	valid_0's auc: 0.891461
[2944]	valid_0's auc: 0.891462
[2945]	valid_0's auc: 0.891477
[2946]	valid_0's auc: 0.891483
[2947]	valid_0's auc: 0.891497
[2948]	valid_0's auc: 0.891494
[2949]	valid_0's auc: 0.891491
[2950]	valid_0's auc: 0.891504
[2951]	valid_0's auc: 0.891516
[2952]	valid_0's auc: 0.891533
[2953]	valid_0's auc: 0.891536
[2954]	valid_0's auc: 0.891528
[2955]	valid_0's auc: 0.891517
[2956]	valid_0's auc: 0.891527
[2957]	valid_0's auc: 0.891522
[2958]	valid_0's auc: 0.891529
[2959]	valid_0's auc: 0.891531
[2960]	valid_0's auc: 0.891531
[2961]	valid_0's auc: 0.891545
[2962]	valid_0's auc: 0.89155
[2963]	valid_0's auc: 0.891552
[2964]	valid_0's auc: 0.891551
[2965]	valid_0's auc: 0.891551
[2966]	valid_0's auc: 0.891548
[2967]	valid_0's auc: 0.891563
[2968]	valid_0's auc: 0.891587
[2969]	valid_0's auc: 0.891598
[2970]	valid_0's auc: 0.891597
[2971]	valid_0's auc: 0.891606
[2972]	valid_0's auc: 0.891621
[2973]	valid_0's auc: 0.89164
[2974]	valid_0's auc: 0.891648
[2975]	valid_0's auc: 0.891653
[2976]	valid_0's auc: 0.891649
[2977]	valid_0's auc: 0.891653
[2978]	valid_0's auc: 0.891658
[2979]	valid_0's auc: 0.89167
[2980]	valid_0's auc: 0.891675
[2981]	valid_0's auc: 0.891684
[2982]	valid_0's auc: 0.891703
[2983]	valid_0's auc: 0.89171
[2984]	valid_0's auc: 0.891708
[2985]	valid_0's auc: 0.891722
[2986]	valid_0's auc: 0.891713
[2987]	valid_0's auc: 0.891719
[2988]	valid_0's auc: 0.891736
[2989]	valid_0's auc: 0.891756
[2990]	valid_0's auc: 0.891767
[2991]	valid_0's auc: 0.89178
[2992]	valid_0's auc: 0.891791
[2993]	valid_0's auc: 0.891802
[2994]	valid_0's auc: 0.891807
[2995]	valid_0's auc: 0.891822
[2996]	valid_0's auc: 0.891819
[2997]	valid_0's auc: 0.891814
[2998]	valid_0's auc: 0.891822
[2999]	valid_0's auc: 0.891838
[3000]	valid_0's auc: 0.891835
[3001]	valid_0's auc: 0.89184
[3002]	valid_0's auc: 0.891843
[3003]	valid_0's auc: 0.891864
[3004]	valid_0's auc: 0.891872
[3005]	valid_0's auc: 0.891873
[3006]	valid_0's auc: 0.891885
[3007]	valid_0's auc: 0.89191
[3008]	valid_0's auc: 0.891931
[3009]	valid_0's auc: 0.891935
[3010]	valid_0's auc: 0.891948
[3011]	valid_0's auc: 0.891955
[3012]	valid_0's auc: 0.89196
[3013]	valid_0's auc: 0.891957
[3014]	valid_0's auc: 0.891963
[3015]	valid_0's auc: 0.891979
[3016]	valid_0's auc: 0.891981
[3017]	valid_0's auc: 0.891993
[3018]	valid_0's auc: 0.891997
[3019]	valid_0's auc: 0.892004
[3020]	valid_0's auc: 0.892018
[3021]	valid_0's auc: 0.892024
[3022]	valid_0's auc: 0.892033
[3023]	valid_0's auc: 0.892045
[3024]	valid_0's auc: 0.892044
[3025]	valid_0's auc: 0.892054
[3026]	valid_0's auc: 0.892064
[3027]	valid_0's auc: 0.892078
[3028]	valid_0's auc: 0.892082
[3029]	valid_0's auc: 0.892079
[3030]	valid_0's auc: 0.89209
[3031]	valid_0's auc: 0.8921
[3032]	valid_0's auc: 0.892109
[3033]	valid_0's auc: 0.892113
[3034]	valid_0's auc: 0.892122
[3035]	valid_0's auc: 0.892125
[3036]	valid_0's auc: 0.892124
[3037]	valid_0's auc: 0.892137
[3038]	valid_0's auc: 0.892137
[3039]	valid_0's auc: 0.892152
[3040]	valid_0's auc: 0.892148
[3041]	valid_0's auc: 0.89216
[3042]	valid_0's auc: 0.89217
[3043]	valid_0's auc: 0.892176
[3044]	valid_0's auc: 0.892184
[3045]	valid_0's auc: 0.892181
[3046]	valid_0's auc: 0.892185
[3047]	valid_0's auc: 0.892193
[3048]	valid_0's auc: 0.892191
[3049]	valid_0's auc: 0.892187
[3050]	valid_0's auc: 0.892193
[3051]	valid_0's auc: 0.892198
[3052]	valid_0's auc: 0.892211
[3053]	valid_0's auc: 0.892214
[3054]	valid_0's auc: 0.892211
[3055]	valid_0's auc: 0.892218
[3056]	valid_0's auc: 0.892233
[3057]	valid_0's auc: 0.89223
[3058]	valid_0's auc: 0.892239
[3059]	valid_0's auc: 0.89224
[3060]	valid_0's auc: 0.892245
[3061]	valid_0's auc: 0.892252
[3062]	valid_0's auc: 0.89226
[3063]	valid_0's auc: 0.892257
[3064]	valid_0's auc: 0.892257
[3065]	valid_0's auc: 0.892267
[3066]	valid_0's auc: 0.892276
[3067]	valid_0's auc: 0.89228
[3068]	valid_0's auc: 0.892286
[3069]	valid_0's auc: 0.892287
[3070]	valid_0's auc: 0.892295
[3071]	valid_0's auc: 0.892303
[3072]	valid_0's auc: 0.892311
[3073]	valid_0's auc: 0.892312
[3074]	valid_0's auc: 0.892314
[3075]	valid_0's auc: 0.892318
[3076]	valid_0's auc: 0.892327
[3077]	valid_0's auc: 0.892331
[3078]	valid_0's auc: 0.892338
[3079]	valid_0's auc: 0.892344
[3080]	valid_0's auc: 0.892347
[3081]	valid_0's auc: 0.892351
[3082]	valid_0's auc: 0.892346
[3083]	valid_0's auc: 0.89235
[3084]	valid_0's auc: 0.892349
[3085]	valid_0's auc: 0.892357
[3086]	valid_0's auc: 0.892351
[3087]	valid_0's auc: 0.892356
[3088]	valid_0's auc: 0.892354
[3089]	valid_0's auc: 0.892359
[3090]	valid_0's auc: 0.892366
[3091]	valid_0's auc: 0.892372
[3092]	valid_0's auc: 0.892377
[3093]	valid_0's auc: 0.892384
[3094]	valid_0's auc: 0.892392
[3095]	valid_0's auc: 0.892389
[3096]	valid_0's auc: 0.892394
[3097]	valid_0's auc: 0.892399
[3098]	valid_0's auc: 0.892413
[3099]	valid_0's auc: 0.892427
[3100]	valid_0's auc: 0.892434
[3101]	valid_0's auc: 0.89242
[3102]	valid_0's auc: 0.892419
[3103]	valid_0's auc: 0.892421
[3104]	valid_0's auc: 0.892438
[3105]	valid_0's auc: 0.892444
[3106]	valid_0's auc: 0.892446
[3107]	valid_0's auc: 0.892455
[3108]	valid_0's auc: 0.892465
[3109]	valid_0's auc: 0.892485
[3110]	valid_0's auc: 0.892491
[3111]	valid_0's auc: 0.892495
[3112]	valid_0's auc: 0.892503
[3113]	valid_0's auc: 0.8925
[3114]	valid_0's auc: 0.89249
[3115]	valid_0's auc: 0.892498
[3116]	valid_0's auc: 0.892502
[3117]	valid_0's auc: 0.892509
[3118]	valid_0's auc: 0.89251
[3119]	valid_0's auc: 0.892526
[3120]	valid_0's auc: 0.892522
[3121]	valid_0's auc: 0.892515
[3122]	valid_0's auc: 0.892524
[3123]	valid_0's auc: 0.892529
[3124]	valid_0's auc: 0.892533
[3125]	valid_0's auc: 0.892536
[3126]	valid_0's auc: 0.892548
[3127]	valid_0's auc: 0.89256
[3128]	valid_0's auc: 0.892565
[3129]	valid_0's auc: 0.892565
[3130]	valid_0's auc: 0.892568
[3131]	valid_0's auc: 0.892564
[3132]	valid_0's auc: 0.89256
[3133]	valid_0's auc: 0.892561
[3134]	valid_0's auc: 0.892569
[3135]	valid_0's auc: 0.892572
[3136]	valid_0's auc: 0.89257
[3137]	valid_0's auc: 0.892573
[3138]	valid_0's auc: 0.892586
[3139]	valid_0's auc: 0.892584
[3140]	valid_0's auc: 0.892586
[3141]	valid_0's auc: 0.892588
[3142]	valid_0's auc: 0.892593
[3143]	valid_0's auc: 0.8926
[3144]	valid_0's auc: 0.892608
[3145]	valid_0's auc: 0.892622
[3146]	valid_0's auc: 0.892625
[3147]	valid_0's auc: 0.892617
[3148]	valid_0's auc: 0.892626
[3149]	valid_0's auc: 0.892622
[3150]	valid_0's auc: 0.892618
[3151]	valid_0's auc: 0.892621
[3152]	valid_0's auc: 0.892624
[3153]	valid_0's auc: 0.892634
[3154]	valid_0's auc: 0.892638
[3155]	valid_0's auc: 0.892646
[3156]	valid_0's auc: 0.89266
[3157]	valid_0's auc: 0.89267
[3158]	valid_0's auc: 0.892673
[3159]	valid_0's auc: 0.892692
[3160]	valid_0's auc: 0.892682
[3161]	valid_0's auc: 0.892686
[3162]	valid_0's auc: 0.892688
[3163]	valid_0's auc: 0.892692
[3164]	valid_0's auc: 0.892687
[3165]	valid_0's auc: 0.89269
[3166]	valid_0's auc: 0.892693
[3167]	valid_0's auc: 0.892696
[3168]	valid_0's auc: 0.892695
[3169]	valid_0's auc: 0.892694
[3170]	valid_0's auc: 0.892692
[3171]	valid_0's auc: 0.892714
[3172]	valid_0's auc: 0.892721
[3173]	valid_0's auc: 0.892724
[3174]	valid_0's auc: 0.892721
[3175]	valid_0's auc: 0.892743
[3176]	valid_0's auc: 0.892737
[3177]	valid_0's auc: 0.89275
[3178]	valid_0's auc: 0.892752
[3179]	valid_0's auc: 0.89277
[3180]	valid_0's auc: 0.892776
[3181]	valid_0's auc: 0.892767
[3182]	valid_0's auc: 0.892769
[3183]	valid_0's auc: 0.892771
[3184]	valid_0's auc: 0.892776
[3185]	valid_0's auc: 0.892772
[3186]	valid_0's auc: 0.89277
[3187]	valid_0's auc: 0.892768
[3188]	valid_0's auc: 0.892779
[3189]	valid_0's auc: 0.892785
[3190]	valid_0's auc: 0.892793
[3191]	valid_0's auc: 0.8928
[3192]	valid_0's auc: 0.89281
[3193]	valid_0's auc: 0.89281
[3194]	valid_0's auc: 0.892806
[3195]	valid_0's auc: 0.892813
[3196]	valid_0's auc: 0.892829
[3197]	valid_0's auc: 0.892832
[3198]	valid_0's auc: 0.892842
[3199]	valid_0's auc: 0.892845
[3200]	valid_0's auc: 0.892849
[3201]	valid_0's auc: 0.892848
[3202]	valid_0's auc: 0.892854
[3203]	valid_0's auc: 0.892859
[3204]	valid_0's auc: 0.892859
[3205]	valid_0's auc: 0.892863
[3206]	valid_0's auc: 0.892861
[3207]	valid_0's auc: 0.892884
[3208]	valid_0's auc: 0.892886
[3209]	valid_0's auc: 0.892894
[3210]	valid_0's auc: 0.892909
[3211]	valid_0's auc: 0.892914
[3212]	valid_0's auc: 0.892918
[3213]	valid_0's auc: 0.892928
[3214]	valid_0's auc: 0.892932
[3215]	valid_0's auc: 0.892927
[3216]	valid_0's auc: 0.892941
[3217]	valid_0's auc: 0.892956
[3218]	valid_0's auc: 0.892969
[3219]	valid_0's auc: 0.89299
[3220]	valid_0's auc: 0.893003
[3221]	valid_0's auc: 0.893006
[3222]	valid_0's auc: 0.893016
[3223]	valid_0's auc: 0.893018
[3224]	valid_0's auc: 0.893023
[3225]	valid_0's auc: 0.893025
[3226]	valid_0's auc: 0.893034
[3227]	valid_0's auc: 0.893034
[3228]	valid_0's auc: 0.893044
[3229]	valid_0's auc: 0.893059
[3230]	valid_0's auc: 0.893055
[3231]	valid_0's auc: 0.893059
[3232]	valid_0's auc: 0.893068
[3233]	valid_0's auc: 0.89307
[3234]	valid_0's auc: 0.893084
[3235]	valid_0's auc: 0.893094
[3236]	valid_0's auc: 0.8931
[3237]	valid_0's auc: 0.893111
[3238]	valid_0's auc: 0.893105
[3239]	valid_0's auc: 0.893112
[3240]	valid_0's auc: 0.893117
[3241]	valid_0's auc: 0.893118
[3242]	valid_0's auc: 0.893117
[3243]	valid_0's auc: 0.893109
[3244]	valid_0's auc: 0.893119
[3245]	valid_0's auc: 0.893137
[3246]	valid_0's auc: 0.893138
[3247]	valid_0's auc: 0.893127
[3248]	valid_0's auc: 0.893128
[3249]	valid_0's auc: 0.893145
[3250]	valid_0's auc: 0.893148
[3251]	valid_0's auc: 0.893146
[3252]	valid_0's auc: 0.893154
[3253]	valid_0's auc: 0.893158
[3254]	valid_0's auc: 0.893158
[3255]	valid_0's auc: 0.89316
[3256]	valid_0's auc: 0.893163
[3257]	valid_0's auc: 0.893158
[3258]	valid_0's auc: 0.89315
[3259]	valid_0's auc: 0.893155
[3260]	valid_0's auc: 0.893157
[3261]	valid_0's auc: 0.89317
[3262]	valid_0's auc: 0.893176
[3263]	valid_0's auc: 0.893189
[3264]	valid_0's auc: 0.893201
[3265]	valid_0's auc: 0.893201
[3266]	valid_0's auc: 0.893198
[3267]	valid_0's auc: 0.893209
[3268]	valid_0's auc: 0.89321
[3269]	valid_0's auc: 0.893221
[3270]	valid_0's auc: 0.89323
[3271]	valid_0's auc: 0.893237
[3272]	valid_0's auc: 0.89324
[3273]	valid_0's auc: 0.893252
[3274]	valid_0's auc: 0.893257
[3275]	valid_0's auc: 0.893258
[3276]	valid_0's auc: 0.89326
[3277]	valid_0's auc: 0.893277
[3278]	valid_0's auc: 0.893284
[3279]	valid_0's auc: 0.893289
[3280]	valid_0's auc: 0.893292
[3281]	valid_0's auc: 0.893304
[3282]	valid_0's auc: 0.893307
[3283]	valid_0's auc: 0.893309
[3284]	valid_0's auc: 0.893321
[3285]	valid_0's auc: 0.893322
[3286]	valid_0's auc: 0.893315
[3287]	valid_0's auc: 0.893315
[3288]	valid_0's auc: 0.893327
[3289]	valid_0's auc: 0.893342
[3290]	valid_0's auc: 0.893348
[3291]	valid_0's auc: 0.893353
[3292]	valid_0's auc: 0.893351
[3293]	valid_0's auc: 0.893365
[3294]	valid_0's auc: 0.893368
[3295]	valid_0's auc: 0.893374
[3296]	valid_0's auc: 0.893383
[3297]	valid_0's auc: 0.893389
[3298]	valid_0's auc: 0.893396
[3299]	valid_0's auc: 0.8934
[3300]	valid_0's auc: 0.893408
[3301]	valid_0's auc: 0.893415
[3302]	valid_0's auc: 0.893421
[3303]	valid_0's auc: 0.893434
[3304]	valid_0's auc: 0.893435
[3305]	valid_0's auc: 0.893444
[3306]	valid_0's auc: 0.893447
[3307]	valid_0's auc: 0.893452
[3308]	valid_0's auc: 0.893458
[3309]	valid_0's auc: 0.893462
[3310]	valid_0's auc: 0.893469
[3311]	valid_0's auc: 0.893472
[3312]	valid_0's auc: 0.893483
[3313]	valid_0's auc: 0.89349
[3314]	valid_0's auc: 0.89349
[3315]	valid_0's auc: 0.8935
[3316]	valid_0's auc: 0.893502
[3317]	valid_0's auc: 0.893507
[3318]	valid_0's auc: 0.893512
[3319]	valid_0's auc: 0.893518
[3320]	valid_0's auc: 0.893507
[3321]	valid_0's auc: 0.893504
[3322]	valid_0's auc: 0.8935
[3323]	valid_0's auc: 0.893502
[3324]	valid_0's auc: 0.893515
[3325]	valid_0's auc: 0.893513
[3326]	valid_0's auc: 0.893523
[3327]	valid_0's auc: 0.893534
[3328]	valid_0's auc: 0.893565
[3329]	valid_0's auc: 0.893565
[3330]	valid_0's auc: 0.893588
[3331]	valid_0's auc: 0.89359
[3332]	valid_0's auc: 0.893594
[3333]	valid_0's auc: 0.89361
[3334]	valid_0's auc: 0.893613
[3335]	valid_0's auc: 0.89362
[3336]	valid_0's auc: 0.893618
[3337]	valid_0's auc: 0.893626
[3338]	valid_0's auc: 0.893635
[3339]	valid_0's auc: 0.893636
[3340]	valid_0's auc: 0.893634
[3341]	valid_0's auc: 0.89365
[3342]	valid_0's auc: 0.893657
[3343]	valid_0's auc: 0.893662
[3344]	valid_0's auc: 0.89366
[3345]	valid_0's auc: 0.893678
[3346]	valid_0's auc: 0.893696
[3347]	valid_0's auc: 0.8937
[3348]	valid_0's auc: 0.893712
[3349]	valid_0's auc: 0.893714
[3350]	valid_0's auc: 0.893707
[3351]	valid_0's auc: 0.893703
[3352]	valid_0's auc: 0.8937
[3353]	valid_0's auc: 0.893713
[3354]	valid_0's auc: 0.893708
[3355]	valid_0's auc: 0.893714
[3356]	valid_0's auc: 0.893718
[3357]	valid_0's auc: 0.893729
[3358]	valid_0's auc: 0.89374
[3359]	valid_0's auc: 0.89375
[3360]	valid_0's auc: 0.893739
[3361]	valid_0's auc: 0.893741
[3362]	valid_0's auc: 0.893743
[3363]	valid_0's auc: 0.893745
[3364]	valid_0's auc: 0.893745
[3365]	valid_0's auc: 0.893753
[3366]	valid_0's auc: 0.893759
[3367]	valid_0's auc: 0.893752
[3368]	valid_0's auc: 0.893755
[3369]	valid_0's auc: 0.893752
[3370]	valid_0's auc: 0.893758
[3371]	valid_0's auc: 0.89376
[3372]	valid_0's auc: 0.893761
[3373]	valid_0's auc: 0.893761
[3374]	valid_0's auc: 0.893756
[3375]	valid_0's auc: 0.893756
[3376]	valid_0's auc: 0.893773
[3377]	valid_0's auc: 0.893781
[3378]	valid_0's auc: 0.893784
[3379]	valid_0's auc: 0.893794
[3380]	valid_0's auc: 0.893807
[3381]	valid_0's auc: 0.893807
[3382]	valid_0's auc: 0.893814
[3383]	valid_0's auc: 0.893811
[3384]	valid_0's auc: 0.893808
[3385]	valid_0's auc: 0.893816
[3386]	valid_0's auc: 0.893823
[3387]	valid_0's auc: 0.893816
[3388]	valid_0's auc: 0.893824
[3389]	valid_0's auc: 0.893831
[3390]	valid_0's auc: 0.893826
[3391]	valid_0's auc: 0.893831
[3392]	valid_0's auc: 0.893842
[3393]	valid_0's auc: 0.893845
[3394]	valid_0's auc: 0.893855
[3395]	valid_0's auc: 0.893861
[3396]	valid_0's auc: 0.893856
[3397]	valid_0's auc: 0.893856
[3398]	valid_0's auc: 0.893864
[3399]	valid_0's auc: 0.893864
[3400]	valid_0's auc: 0.893877
[3401]	valid_0's auc: 0.893889
[3402]	valid_0's auc: 0.893893
[3403]	valid_0's auc: 0.893898
[3404]	valid_0's auc: 0.893901
[3405]	valid_0's auc: 0.893901
[3406]	valid_0's auc: 0.893907
[3407]	valid_0's auc: 0.893924
[3408]	valid_0's auc: 0.893924
[3409]	valid_0's auc: 0.893925
[3410]	valid_0's auc: 0.893932
[3411]	valid_0's auc: 0.893926
[3412]	valid_0's auc: 0.893927
[3413]	valid_0's auc: 0.893923
[3414]	valid_0's auc: 0.893928
[3415]	valid_0's auc: 0.893928
[3416]	valid_0's auc: 0.893934
[3417]	valid_0's auc: 0.893934
[3418]	valid_0's auc: 0.893941
[3419]	valid_0's auc: 0.893955
[3420]	valid_0's auc: 0.893954
[3421]	valid_0's auc: 0.893961
[3422]	valid_0's auc: 0.893969
[3423]	valid_0's auc: 0.893976
[3424]	valid_0's auc: 0.893973
[3425]	valid_0's auc: 0.893963
[3426]	valid_0's auc: 0.893965
[3427]	valid_0's auc: 0.893981
[3428]	valid_0's auc: 0.893983
[3429]	valid_0's auc: 0.893994
[3430]	valid_0's auc: 0.893988
[3431]	valid_0's auc: 0.894004
[3432]	valid_0's auc: 0.89401
[3433]	valid_0's auc: 0.894001
[3434]	valid_0's auc: 0.894009
[3435]	valid_0's auc: 0.894025
[3436]	valid_0's auc: 0.894024
[3437]	valid_0's auc: 0.894025
[3438]	valid_0's auc: 0.89404
[3439]	valid_0's auc: 0.894048
[3440]	valid_0's auc: 0.894055
[3441]	valid_0's auc: 0.894061
[3442]	valid_0's auc: 0.894058
[3443]	valid_0's auc: 0.894062
[3444]	valid_0's auc: 0.894069
[3445]	valid_0's auc: 0.894065
[3446]	valid_0's auc: 0.894059
[3447]	valid_0's auc: 0.89407
[3448]	valid_0's auc: 0.894075
[3449]	valid_0's auc: 0.894073
[3450]	valid_0's auc: 0.894082
[3451]	valid_0's auc: 0.894088
[3452]	valid_0's auc: 0.894088
[3453]	valid_0's auc: 0.894095
[3454]	valid_0's auc: 0.894099
[3455]	valid_0's auc: 0.8941
[3456]	valid_0's auc: 0.894108
[3457]	valid_0's auc: 0.894101
[3458]	valid_0's auc: 0.89409
[3459]	valid_0's auc: 0.894105
[3460]	valid_0's auc: 0.894112
[3461]	valid_0's auc: 0.894113
[3462]	valid_0's auc: 0.894117
[3463]	valid_0's auc: 0.894127
[3464]	valid_0's auc: 0.89413
[3465]	valid_0's auc: 0.894141
[3466]	valid_0's auc: 0.894157
[3467]	valid_0's auc: 0.894156
[3468]	valid_0's auc: 0.894156
[3469]	valid_0's auc: 0.894163
[3470]	valid_0's auc: 0.894173
[3471]	valid_0's auc: 0.894181
[3472]	valid_0's auc: 0.894186
[3473]	valid_0's auc: 0.894192
[3474]	valid_0's auc: 0.894203
[3475]	valid_0's auc: 0.894206
[3476]	valid_0's auc: 0.894213
[3477]	valid_0's auc: 0.894218
[3478]	valid_0's auc: 0.894215
[3479]	valid_0's auc: 0.89422
[3480]	valid_0's auc: 0.89423
[3481]	valid_0's auc: 0.894225
[3482]	valid_0's auc: 0.894221
[3483]	valid_0's auc: 0.89423
[3484]	valid_0's auc: 0.894226
[3485]	valid_0's auc: 0.894233
[3486]	valid_0's auc: 0.894235
[3487]	valid_0's auc: 0.894242
[3488]	valid_0's auc: 0.894248
[3489]	valid_0's auc: 0.894247
[3490]	valid_0's auc: 0.894243
[3491]	valid_0's auc: 0.894247
[3492]	valid_0's auc: 0.894253
[3493]	valid_0's auc: 0.894261
[3494]	valid_0's auc: 0.894262
[3495]	valid_0's auc: 0.894269
[3496]	valid_0's auc: 0.894272
[3497]	valid_0's auc: 0.894273
[3498]	valid_0's auc: 0.894278
[3499]	valid_0's auc: 0.894278
[3500]	valid_0's auc: 0.894277
[3501]	valid_0's auc: 0.894278
[3502]	valid_0's auc: 0.894273
[3503]	valid_0's auc: 0.894273
[3504]	valid_0's auc: 0.894278
[3505]	valid_0's auc: 0.89429
[3506]	valid_0's auc: 0.894294
[3507]	valid_0's auc: 0.894292
[3508]	valid_0's auc: 0.894293
[3509]	valid_0's auc: 0.894293
[3510]	valid_0's auc: 0.894299
[3511]	valid_0's auc: 0.8943
[3512]	valid_0's auc: 0.8943
[3513]	valid_0's auc: 0.894304
[3514]	valid_0's auc: 0.894311
[3515]	valid_0's auc: 0.894308
[3516]	valid_0's auc: 0.89431
[3517]	valid_0's auc: 0.894326
[3518]	valid_0's auc: 0.89433
[3519]	valid_0's auc: 0.894335
[3520]	valid_0's auc: 0.894352
[3521]	valid_0's auc: 0.894349
[3522]	valid_0's auc: 0.894368
[3523]	valid_0's auc: 0.894367
[3524]	valid_0's auc: 0.894376
[3525]	valid_0's auc: 0.894374
[3526]	valid_0's auc: 0.894392
[3527]	valid_0's auc: 0.894391
[3528]	valid_0's auc: 0.894395
[3529]	valid_0's auc: 0.894398
[3530]	valid_0's auc: 0.894393
[3531]	valid_0's auc: 0.894391
[3532]	valid_0's auc: 0.894395
[3533]	valid_0's auc: 0.89439
[3534]	valid_0's auc: 0.894393
[3535]	valid_0's auc: 0.894393
[3536]	valid_0's auc: 0.894401
[3537]	valid_0's auc: 0.894394
[3538]	valid_0's auc: 0.894402
[3539]	valid_0's auc: 0.894412
[3540]	valid_0's auc: 0.894415
[3541]	valid_0's auc: 0.894414
[3542]	valid_0's auc: 0.894417
[3543]	valid_0's auc: 0.894416
[3544]	valid_0's auc: 0.894416
[3545]	valid_0's auc: 0.894426
[3546]	valid_0's auc: 0.894425
[3547]	valid_0's auc: 0.894426
[3548]	valid_0's auc: 0.894434
[3549]	valid_0's auc: 0.894439
[3550]	valid_0's auc: 0.894442
[3551]	valid_0's auc: 0.894452
[3552]	valid_0's auc: 0.894438
[3553]	valid_0's auc: 0.894446
[3554]	valid_0's auc: 0.894461
[3555]	valid_0's auc: 0.894468
[3556]	valid_0's auc: 0.894469
[3557]	valid_0's auc: 0.89447
[3558]	valid_0's auc: 0.894479
[3559]	valid_0's auc: 0.894477
[3560]	valid_0's auc: 0.894487
[3561]	valid_0's auc: 0.894495
[3562]	valid_0's auc: 0.894488
[3563]	valid_0's auc: 0.894492
[3564]	valid_0's auc: 0.8945
[3565]	valid_0's auc: 0.894503
[3566]	valid_0's auc: 0.894499
[3567]	valid_0's auc: 0.894505
[3568]	valid_0's auc: 0.894505
[3569]	valid_0's auc: 0.894496
[3570]	valid_0's auc: 0.8945
[3571]	valid_0's auc: 0.894498
[3572]	valid_0's auc: 0.894496
[3573]	valid_0's auc: 0.894491
[3574]	valid_0's auc: 0.89449
[3575]	valid_0's auc: 0.894491
[3576]	valid_0's auc: 0.894503
[3577]	valid_0's auc: 0.894505
[3578]	valid_0's auc: 0.894513
[3579]	valid_0's auc: 0.894524
[3580]	valid_0's auc: 0.894531
[3581]	valid_0's auc: 0.894531
[3582]	valid_0's auc: 0.894536
[3583]	valid_0's auc: 0.894534
[3584]	valid_0's auc: 0.894541
[3585]	valid_0's auc: 0.894549
[3586]	valid_0's auc: 0.894561
[3587]	valid_0's auc: 0.894563
[3588]	valid_0's auc: 0.894573
[3589]	valid_0's auc: 0.894573
[3590]	valid_0's auc: 0.894574
[3591]	valid_0's auc: 0.894572
[3592]	valid_0's auc: 0.894581
[3593]	valid_0's auc: 0.894587
[3594]	valid_0's auc: 0.894599
[3595]	valid_0's auc: 0.894595
[3596]	valid_0's auc: 0.894598
[3597]	valid_0's auc: 0.894601
[3598]	valid_0's auc: 0.894592
[3599]	valid_0's auc: 0.894592
[3600]	valid_0's auc: 0.894596
[3601]	valid_0's auc: 0.894591
[3602]	valid_0's auc: 0.894603
[3603]	valid_0's auc: 0.894609
[3604]	valid_0's auc: 0.894612
[3605]	valid_0's auc: 0.894622
[3606]	valid_0's auc: 0.894631
[3607]	valid_0's auc: 0.894628
[3608]	valid_0's auc: 0.894635
[3609]	valid_0's auc: 0.894638
[3610]	valid_0's auc: 0.894642
[3611]	valid_0's auc: 0.894642
[3612]	valid_0's auc: 0.894642
[3613]	valid_0's auc: 0.894656
[3614]	valid_0's auc: 0.894665
[3615]	valid_0's auc: 0.894661
[3616]	valid_0's auc: 0.894655
[3617]	valid_0's auc: 0.894666
[3618]	valid_0's auc: 0.894655
[3619]	valid_0's auc: 0.89466
[3620]	valid_0's auc: 0.894658
[3621]	valid_0's auc: 0.894655
[3622]	valid_0's auc: 0.89465
[3623]	valid_0's auc: 0.894647
[3624]	valid_0's auc: 0.894651
[3625]	valid_0's auc: 0.894654
[3626]	valid_0's auc: 0.894663
[3627]	valid_0's auc: 0.894674
[3628]	valid_0's auc: 0.894677
[3629]	valid_0's auc: 0.894685
[3630]	valid_0's auc: 0.894681
[3631]	valid_0's auc: 0.894697
[3632]	valid_0's auc: 0.894712
[3633]	valid_0's auc: 0.894725
[3634]	valid_0's auc: 0.894729
[3635]	valid_0's auc: 0.894742
[3636]	valid_0's auc: 0.894753
[3637]	valid_0's auc: 0.894751
[3638]	valid_0's auc: 0.894751
[3639]	valid_0's auc: 0.894754
[3640]	valid_0's auc: 0.894755
[3641]	valid_0's auc: 0.894754
[3642]	valid_0's auc: 0.894758
[3643]	valid_0's auc: 0.894761
[3644]	valid_0's auc: 0.894758
[3645]	valid_0's auc: 0.894756
[3646]	valid_0's auc: 0.894763
[3647]	valid_0's auc: 0.894777
[3648]	valid_0's auc: 0.894775
[3649]	valid_0's auc: 0.894779
[3650]	valid_0's auc: 0.894772
[3651]	valid_0's auc: 0.89477
[3652]	valid_0's auc: 0.894768
[3653]	valid_0's auc: 0.894768
[3654]	valid_0's auc: 0.894773
[3655]	valid_0's auc: 0.894783
[3656]	valid_0's auc: 0.894788
[3657]	valid_0's auc: 0.894789
[3658]	valid_0's auc: 0.89479
[3659]	valid_0's auc: 0.894803
[3660]	valid_0's auc: 0.894802
[3661]	valid_0's auc: 0.894808
[3662]	valid_0's auc: 0.894811
[3663]	valid_0's auc: 0.894817
[3664]	valid_0's auc: 0.894824
[3665]	valid_0's auc: 0.894826
[3666]	valid_0's auc: 0.894821
[3667]	valid_0's auc: 0.894827
[3668]	valid_0's auc: 0.894822
[3669]	valid_0's auc: 0.89483
[3670]	valid_0's auc: 0.894831
[3671]	valid_0's auc: 0.894834
[3672]	valid_0's auc: 0.894828
[3673]	valid_0's auc: 0.894832
[3674]	valid_0's auc: 0.894831
[3675]	valid_0's auc: 0.894828
[3676]	valid_0's auc: 0.894833
[3677]	valid_0's auc: 0.89484
[3678]	valid_0's auc: 0.894841
[3679]	valid_0's auc: 0.894853
[3680]	valid_0's auc: 0.894859
[3681]	valid_0's auc: 0.894866
[3682]	valid_0's auc: 0.894877
[3683]	valid_0's auc: 0.894876
[3684]	valid_0's auc: 0.894874
[3685]	valid_0's auc: 0.894876
[3686]	valid_0's auc: 0.894881
[3687]	valid_0's auc: 0.894874
[3688]	valid_0's auc: 0.89488
[3689]	valid_0's auc: 0.894886
[3690]	valid_0's auc: 0.894887
[3691]	valid_0's auc: 0.894895
[3692]	valid_0's auc: 0.894907
[3693]	valid_0's auc: 0.894913
[3694]	valid_0's auc: 0.894924
[3695]	valid_0's auc: 0.894931
[3696]	valid_0's auc: 0.894926
[3697]	valid_0's auc: 0.89492
[3698]	valid_0's auc: 0.894916
[3699]	valid_0's auc: 0.89491
[3700]	valid_0's auc: 0.894909
[3701]	valid_0's auc: 0.894919
[3702]	valid_0's auc: 0.894915
[3703]	valid_0's auc: 0.894907
[3704]	valid_0's auc: 0.894913
[3705]	valid_0's auc: 0.894912
[3706]	valid_0's auc: 0.894917
[3707]	valid_0's auc: 0.894916
[3708]	valid_0's auc: 0.894907
[3709]	valid_0's auc: 0.894912
[3710]	valid_0's auc: 0.8949
[3711]	valid_0's auc: 0.894908
[3712]	valid_0's auc: 0.894907
[3713]	valid_0's auc: 0.89491
[3714]	valid_0's auc: 0.894907
[3715]	valid_0's auc: 0.894924
[3716]	valid_0's auc: 0.89493
[3717]	valid_0's auc: 0.894934
[3718]	valid_0's auc: 0.894941
[3719]	valid_0's auc: 0.894938
[3720]	valid_0's auc: 0.894934
[3721]	valid_0's auc: 0.894939
[3722]	valid_0's auc: 0.894938
[3723]	valid_0's auc: 0.894939
[3724]	valid_0's auc: 0.894938
[3725]	valid_0's auc: 0.894934
[3726]	valid_0's auc: 0.894944
[3727]	valid_0's auc: 0.894951
[3728]	valid_0's auc: 0.89495
[3729]	valid_0's auc: 0.894961
[3730]	valid_0's auc: 0.894968
[3731]	valid_0's auc: 0.894973
[3732]	valid_0's auc: 0.894986
[3733]	valid_0's auc: 0.894984
[3734]	valid_0's auc: 0.894982
[3735]	valid_0's auc: 0.894983
[3736]	valid_0's auc: 0.894988
[3737]	valid_0's auc: 0.894982
[3738]	valid_0's auc: 0.894995
[3739]	valid_0's auc: 0.894993
[3740]	valid_0's auc: 0.894994
[3741]	valid_0's auc: 0.895003
[3742]	valid_0's auc: 0.895015
[3743]	valid_0's auc: 0.895018
[3744]	valid_0's auc: 0.895018
[3745]	valid_0's auc: 0.895028
[3746]	valid_0's auc: 0.89503
[3747]	valid_0's auc: 0.89502
[3748]	valid_0's auc: 0.895032
[3749]	valid_0's auc: 0.895037
[3750]	valid_0's auc: 0.895047
[3751]	valid_0's auc: 0.895049
[3752]	valid_0's auc: 0.895051
[3753]	valid_0's auc: 0.895051
[3754]	valid_0's auc: 0.895053
[3755]	valid_0's auc: 0.895056
[3756]	valid_0's auc: 0.895058
[3757]	valid_0's auc: 0.895059
[3758]	valid_0's auc: 0.895056
[3759]	valid_0's auc: 0.895051
[3760]	valid_0's auc: 0.895052
[3761]	valid_0's auc: 0.895055
[3762]	valid_0's auc: 0.895053
[3763]	valid_0's auc: 0.895056
[3764]	valid_0's auc: 0.895058
[3765]	valid_0's auc: 0.895062
[3766]	valid_0's auc: 0.895065
[3767]	valid_0's auc: 0.895074
[3768]	valid_0's auc: 0.895073
[3769]	valid_0's auc: 0.895076
[3770]	valid_0's auc: 0.89509
[3771]	valid_0's auc: 0.895095
[3772]	valid_0's auc: 0.895092
[3773]	valid_0's auc: 0.895089
[3774]	valid_0's auc: 0.895084
[3775]	valid_0's auc: 0.895085
[3776]	valid_0's auc: 0.895093
[3777]	valid_0's auc: 0.895091
[3778]	valid_0's auc: 0.895082
[3779]	valid_0's auc: 0.895087
[3780]	valid_0's auc: 0.895087
[3781]	valid_0's auc: 0.895091
[3782]	valid_0's auc: 0.8951
[3783]	valid_0's auc: 0.8951
[3784]	valid_0's auc: 0.8951
[3785]	valid_0's auc: 0.895109
[3786]	valid_0's auc: 0.895114
[3787]	valid_0's auc: 0.895117
[3788]	valid_0's auc: 0.895111
[3789]	valid_0's auc: 0.895114
[3790]	valid_0's auc: 0.895122
[3791]	valid_0's auc: 0.895126
[3792]	valid_0's auc: 0.895126
[3793]	valid_0's auc: 0.895131
[3794]	valid_0's auc: 0.895129
[3795]	valid_0's auc: 0.895141
[3796]	valid_0's auc: 0.895138
[3797]	valid_0's auc: 0.895133
[3798]	valid_0's auc: 0.895142
[3799]	valid_0's auc: 0.895137
[3800]	valid_0's auc: 0.895139
[3801]	valid_0's auc: 0.895146
[3802]	valid_0's auc: 0.895151
[3803]	valid_0's auc: 0.895153
[3804]	valid_0's auc: 0.895155
[3805]	valid_0's auc: 0.895154
[3806]	valid_0's auc: 0.895158
[3807]	valid_0's auc: 0.895156
[3808]	valid_0's auc: 0.895162
[3809]	valid_0's auc: 0.895164
[3810]	valid_0's auc: 0.895161
[3811]	valid_0's auc: 0.895159
[3812]	valid_0's auc: 0.895162
[3813]	valid_0's auc: 0.895157
[3814]	valid_0's auc: 0.895168
[3815]	valid_0's auc: 0.895167
[3816]	valid_0's auc: 0.895174
[3817]	valid_0's auc: 0.895174
[3818]	valid_0's auc: 0.895189
[3819]	valid_0's auc: 0.895189
[3820]	valid_0's auc: 0.895205
[3821]	valid_0's auc: 0.895209
[3822]	valid_0's auc: 0.895223
[3823]	valid_0's auc: 0.895229
[3824]	valid_0's auc: 0.895242
[3825]	valid_0's auc: 0.895252
[3826]	valid_0's auc: 0.895255
[3827]	valid_0's auc: 0.895262
[3828]	valid_0's auc: 0.895269
[3829]	valid_0's auc: 0.89527
[3830]	valid_0's auc: 0.89527
[3831]	valid_0's auc: 0.895259
[3832]	valid_0's auc: 0.895274
[3833]	valid_0's auc: 0.895279
[3834]	valid_0's auc: 0.89528
[3835]	valid_0's auc: 0.895278
[3836]	valid_0's auc: 0.895284
[3837]	valid_0's auc: 0.895277
[3838]	valid_0's auc: 0.89528
[3839]	valid_0's auc: 0.895282
[3840]	valid_0's auc: 0.89529
[3841]	valid_0's auc: 0.895304
[3842]	valid_0's auc: 0.89531
[3843]	valid_0's auc: 0.895312
[3844]	valid_0's auc: 0.895314
[3845]	valid_0's auc: 0.895312
[3846]	valid_0's auc: 0.895317
[3847]	valid_0's auc: 0.895323
[3848]	valid_0's auc: 0.895321
[3849]	valid_0's auc: 0.895327
[3850]	valid_0's auc: 0.895332
[3851]	valid_0's auc: 0.895345
[3852]	valid_0's auc: 0.895343
[3853]	valid_0's auc: 0.895347
[3854]	valid_0's auc: 0.895347
[3855]	valid_0's auc: 0.89535
[3856]	valid_0's auc: 0.895357
[3857]	valid_0's auc: 0.895364
[3858]	valid_0's auc: 0.89536
[3859]	valid_0's auc: 0.895363
[3860]	valid_0's auc: 0.895366
[3861]	valid_0's auc: 0.895366
[3862]	valid_0's auc: 0.89537
[3863]	valid_0's auc: 0.895388
[3864]	valid_0's auc: 0.8954
[3865]	valid_0's auc: 0.895403
[3866]	valid_0's auc: 0.895409
[3867]	valid_0's auc: 0.895422
[3868]	valid_0's auc: 0.895437
[3869]	valid_0's auc: 0.895443
[3870]	valid_0's auc: 0.895445
[3871]	valid_0's auc: 0.895448
[3872]	valid_0's auc: 0.895454
[3873]	valid_0's auc: 0.895457
[3874]	valid_0's auc: 0.895458
[3875]	valid_0's auc: 0.895468
[3876]	valid_0's auc: 0.895474
[3877]	valid_0's auc: 0.895481
[3878]	valid_0's auc: 0.895485
[3879]	valid_0's auc: 0.895478
[3880]	valid_0's auc: 0.895482
[3881]	valid_0's auc: 0.895484
[3882]	valid_0's auc: 0.895491
[3883]	valid_0's auc: 0.895491
[3884]	valid_0's auc: 0.895494
[3885]	valid_0's auc: 0.895497
[3886]	valid_0's auc: 0.895501
[3887]	valid_0's auc: 0.895504
[3888]	valid_0's auc: 0.89551
[3889]	valid_0's auc: 0.895514
[3890]	valid_0's auc: 0.895525
[3891]	valid_0's auc: 0.895525
[3892]	valid_0's auc: 0.895531
[3893]	valid_0's auc: 0.895538
[3894]	valid_0's auc: 0.89554
[3895]	valid_0's auc: 0.895534
[3896]	valid_0's auc: 0.895538
[3897]	valid_0's auc: 0.895543
[3898]	valid_0's auc: 0.895543
[3899]	valid_0's auc: 0.895558
[3900]	valid_0's auc: 0.895563
[3901]	valid_0's auc: 0.895559
[3902]	valid_0's auc: 0.895554
[3903]	valid_0's auc: 0.895562
[3904]	valid_0's auc: 0.895561
[3905]	valid_0's auc: 0.895562
[3906]	valid_0's auc: 0.895573
[3907]	valid_0's auc: 0.895573
[3908]	valid_0's auc: 0.895578
[3909]	valid_0's auc: 0.895573
[3910]	valid_0's auc: 0.895572
[3911]	valid_0's auc: 0.895574
[3912]	valid_0's auc: 0.895572
[3913]	valid_0's auc: 0.895574
[3914]	valid_0's auc: 0.895587
[3915]	valid_0's auc: 0.895597
[3916]	valid_0's auc: 0.895599
[3917]	valid_0's auc: 0.895602
[3918]	valid_0's auc: 0.895604
[3919]	valid_0's auc: 0.895612
[3920]	valid_0's auc: 0.895616
[3921]	valid_0's auc: 0.895612
[3922]	valid_0's auc: 0.895624
[3923]	valid_0's auc: 0.895622
[3924]	valid_0's auc: 0.895618
[3925]	valid_0's auc: 0.89561
[3926]	valid_0's auc: 0.895601
[3927]	valid_0's auc: 0.89561
[3928]	valid_0's auc: 0.895613
[3929]	valid_0's auc: 0.895618
[3930]	valid_0's auc: 0.895618
[3931]	valid_0's auc: 0.895625
[3932]	valid_0's auc: 0.89562
[3933]	valid_0's auc: 0.895617
[3934]	valid_0's auc: 0.895615
[3935]	valid_0's auc: 0.895618
[3936]	valid_0's auc: 0.895612
[3937]	valid_0's auc: 0.895619
[3938]	valid_0's auc: 0.895624
[3939]	valid_0's auc: 0.895632
[3940]	valid_0's auc: 0.895641
[3941]	valid_0's auc: 0.895651
[3942]	valid_0's auc: 0.895647
[3943]	valid_0's auc: 0.895645
[3944]	valid_0's auc: 0.895652
[3945]	valid_0's auc: 0.895649
[3946]	valid_0's auc: 0.895663
[3947]	valid_0's auc: 0.895667
[3948]	valid_0's auc: 0.895669
[3949]	valid_0's auc: 0.895673
[3950]	valid_0's auc: 0.895678
[3951]	valid_0's auc: 0.895677
[3952]	valid_0's auc: 0.895688
[3953]	valid_0's auc: 0.895692
[3954]	valid_0's auc: 0.895698
[3955]	valid_0's auc: 0.895703
[3956]	valid_0's auc: 0.895702
[3957]	valid_0's auc: 0.895702
[3958]	valid_0's auc: 0.895704
[3959]	valid_0's auc: 0.895705
[3960]	valid_0's auc: 0.895709
[3961]	valid_0's auc: 0.895703
[3962]	valid_0's auc: 0.895698
[3963]	valid_0's auc: 0.895704
[3964]	valid_0's auc: 0.895704
[3965]	valid_0's auc: 0.895701
[3966]	valid_0's auc: 0.895699
[3967]	valid_0's auc: 0.895701
[3968]	valid_0's auc: 0.895704
[3969]	valid_0's auc: 0.895707
[3970]	valid_0's auc: 0.895704
[3971]	valid_0's auc: 0.895714
[3972]	valid_0's auc: 0.89572
[3973]	valid_0's auc: 0.895722
[3974]	valid_0's auc: 0.895727
[3975]	valid_0's auc: 0.895723
[3976]	valid_0's auc: 0.89573
[3977]	valid_0's auc: 0.89573
[3978]	valid_0's auc: 0.895717
[3979]	valid_0's auc: 0.895718
[3980]	valid_0's auc: 0.895718
[3981]	valid_0's auc: 0.895719
[3982]	valid_0's auc: 0.89572
[3983]	valid_0's auc: 0.895723
[3984]	valid_0's auc: 0.895727
[3985]	valid_0's auc: 0.895729
[3986]	valid_0's auc: 0.895727
[3987]	valid_0's auc: 0.89574
[3988]	valid_0's auc: 0.895734
[3989]	valid_0's auc: 0.895732
[3990]	valid_0's auc: 0.895731
[3991]	valid_0's auc: 0.895734
[3992]	valid_0's auc: 0.895745
[3993]	valid_0's auc: 0.895738
[3994]	valid_0's auc: 0.895733
[3995]	valid_0's auc: 0.895744
[3996]	valid_0's auc: 0.895748
[3997]	valid_0's auc: 0.895752
[3998]	valid_0's auc: 0.895758
[3999]	valid_0's auc: 0.895756
[4000]	valid_0's auc: 0.895759
[4001]	valid_0's auc: 0.895766
[4002]	valid_0's auc: 0.895772
[4003]	valid_0's auc: 0.895778
[4004]	valid_0's auc: 0.895775
[4005]	valid_0's auc: 0.895776
[4006]	valid_0's auc: 0.895769
[4007]	valid_0's auc: 0.89577
[4008]	valid_0's auc: 0.895767
[4009]	valid_0's auc: 0.895764
[4010]	valid_0's auc: 0.895771
[4011]	valid_0's auc: 0.895782
[4012]	valid_0's auc: 0.89578
[4013]	valid_0's auc: 0.895788
[4014]	valid_0's auc: 0.895787
[4015]	valid_0's auc: 0.895788
[4016]	valid_0's auc: 0.895797
[4017]	valid_0's auc: 0.895808
[4018]	valid_0's auc: 0.895816
[4019]	valid_0's auc: 0.895822
[4020]	valid_0's auc: 0.895829
[4021]	valid_0's auc: 0.895828
[4022]	valid_0's auc: 0.895835
[4023]	valid_0's auc: 0.895847
[4024]	valid_0's auc: 0.895851
[4025]	valid_0's auc: 0.895855
[4026]	valid_0's auc: 0.895858
[4027]	valid_0's auc: 0.895862
[4028]	valid_0's auc: 0.89587
[4029]	valid_0's auc: 0.89588
[4030]	valid_0's auc: 0.895882
[4031]	valid_0's auc: 0.895892
[4032]	valid_0's auc: 0.895893
[4033]	valid_0's auc: 0.8959
[4034]	valid_0's auc: 0.895901
[4035]	valid_0's auc: 0.895903
[4036]	valid_0's auc: 0.895903
[4037]	valid_0's auc: 0.89592
[4038]	valid_0's auc: 0.895926
[4039]	valid_0's auc: 0.895929
[4040]	valid_0's auc: 0.895923
[4041]	valid_0's auc: 0.895921
[4042]	valid_0's auc: 0.895926
[4043]	valid_0's auc: 0.895937
[4044]	valid_0's auc: 0.895947
[4045]	valid_0's auc: 0.895952
[4046]	valid_0's auc: 0.895963
[4047]	valid_0's auc: 0.895955
[4048]	valid_0's auc: 0.895947
[4049]	valid_0's auc: 0.895948
[4050]	valid_0's auc: 0.895946
[4051]	valid_0's auc: 0.895942
[4052]	valid_0's auc: 0.895949
[4053]	valid_0's auc: 0.895955
[4054]	valid_0's auc: 0.895952
[4055]	valid_0's auc: 0.895946
[4056]	valid_0's auc: 0.895943
[4057]	valid_0's auc: 0.895937
[4058]	valid_0's auc: 0.895935
[4059]	valid_0's auc: 0.895939
[4060]	valid_0's auc: 0.89594
[4061]	valid_0's auc: 0.895949
[4062]	valid_0's auc: 0.89595
[4063]	valid_0's auc: 0.895947
[4064]	valid_0's auc: 0.895959
[4065]	valid_0's auc: 0.895976
[4066]	valid_0's auc: 0.895981
[4067]	valid_0's auc: 0.895982
[4068]	valid_0's auc: 0.895986
[4069]	valid_0's auc: 0.895989
[4070]	valid_0's auc: 0.895989
[4071]	valid_0's auc: 0.896006
[4072]	valid_0's auc: 0.896005
[4073]	valid_0's auc: 0.896014
[4074]	valid_0's auc: 0.896022
[4075]	valid_0's auc: 0.896024
[4076]	valid_0's auc: 0.896025
[4077]	valid_0's auc: 0.89602
[4078]	valid_0's auc: 0.896024
[4079]	valid_0's auc: 0.896028
[4080]	valid_0's auc: 0.896031
[4081]	valid_0's auc: 0.896035
[4082]	valid_0's auc: 0.896037
[4083]	valid_0's auc: 0.896031
[4084]	valid_0's auc: 0.896034
[4085]	valid_0's auc: 0.89603
[4086]	valid_0's auc: 0.896025
[4087]	valid_0's auc: 0.896027
[4088]	valid_0's auc: 0.896021
[4089]	valid_0's auc: 0.896019
[4090]	valid_0's auc: 0.89602
[4091]	valid_0's auc: 0.896021
[4092]	valid_0's auc: 0.896025
[4093]	valid_0's auc: 0.896019
[4094]	valid_0's auc: 0.896022
[4095]	valid_0's auc: 0.896026
[4096]	valid_0's auc: 0.896028
[4097]	valid_0's auc: 0.896031
[4098]	valid_0's auc: 0.896029
[4099]	valid_0's auc: 0.896035
[4100]	valid_0's auc: 0.896041
[4101]	valid_0's auc: 0.896038
[4102]	valid_0's auc: 0.896042
[4103]	valid_0's auc: 0.896041
[4104]	valid_0's auc: 0.896046
[4105]	valid_0's auc: 0.896051
[4106]	valid_0's auc: 0.896059
[4107]	valid_0's auc: 0.896058
[4108]	valid_0's auc: 0.896074
[4109]	valid_0's auc: 0.896074
[4110]	valid_0's auc: 0.89608
[4111]	valid_0's auc: 0.896076
[4112]	valid_0's auc: 0.896071
[4113]	valid_0's auc: 0.896074
[4114]	valid_0's auc: 0.896071
[4115]	valid_0's auc: 0.896073
[4116]	valid_0's auc: 0.896075
[4117]	valid_0's auc: 0.896069
[4118]	valid_0's auc: 0.896065
[4119]	valid_0's auc: 0.896068
[4120]	valid_0's auc: 0.896072
[4121]	valid_0's auc: 0.896066
[4122]	valid_0's auc: 0.896075
[4123]	valid_0's auc: 0.896076
[4124]	valid_0's auc: 0.896074
[4125]	valid_0's auc: 0.896074
[4126]	valid_0's auc: 0.896074
[4127]	valid_0's auc: 0.896075
[4128]	valid_0's auc: 0.896071
[4129]	valid_0's auc: 0.896061
[4130]	valid_0's auc: 0.896059
[4131]	valid_0's auc: 0.896065
[4132]	valid_0's auc: 0.89607
[4133]	valid_0's auc: 0.896082
[4134]	valid_0's auc: 0.896078
[4135]	valid_0's auc: 0.896079
[4136]	valid_0's auc: 0.896078
[4137]	valid_0's auc: 0.896077
[4138]	valid_0's auc: 0.896076
[4139]	valid_0's auc: 0.896076
[4140]	valid_0's auc: 0.896078
[4141]	valid_0's auc: 0.896091
[4142]	valid_0's auc: 0.896094
[4143]	valid_0's auc: 0.896098
[4144]	valid_0's auc: 0.896097
[4145]	valid_0's auc: 0.8961
[4146]	valid_0's auc: 0.896109
[4147]	valid_0's auc: 0.896113
[4148]	valid_0's auc: 0.896111
[4149]	valid_0's auc: 0.896111
[4150]	valid_0's auc: 0.896115
[4151]	valid_0's auc: 0.896112
[4152]	valid_0's auc: 0.896114
[4153]	valid_0's auc: 0.896118
[4154]	valid_0's auc: 0.896112
[4155]	valid_0's auc: 0.896117
[4156]	valid_0's auc: 0.896111
[4157]	valid_0's auc: 0.896105
[4158]	valid_0's auc: 0.896104
[4159]	valid_0's auc: 0.896115
[4160]	valid_0's auc: 0.896115
[4161]	valid_0's auc: 0.89611
[4162]	valid_0's auc: 0.896108
[4163]	valid_0's auc: 0.896111
[4164]	valid_0's auc: 0.896105
[4165]	valid_0's auc: 0.896112
[4166]	valid_0's auc: 0.896104
[4167]	valid_0's auc: 0.896114
[4168]	valid_0's auc: 0.896115
[4169]	valid_0's auc: 0.896109
[4170]	valid_0's auc: 0.896119
[4171]	valid_0's auc: 0.896122
[4172]	valid_0's auc: 0.896124
[4173]	valid_0's auc: 0.896126
[4174]	valid_0's auc: 0.896129
[4175]	valid_0's auc: 0.896139
[4176]	valid_0's auc: 0.896141
[4177]	valid_0's auc: 0.896144
[4178]	valid_0's auc: 0.896145
[4179]	valid_0's auc: 0.896147
[4180]	valid_0's auc: 0.896153
[4181]	valid_0's auc: 0.896157
[4182]	valid_0's auc: 0.89616
[4183]	valid_0's auc: 0.896165
[4184]	valid_0's auc: 0.896169
[4185]	valid_0's auc: 0.89618
[4186]	valid_0's auc: 0.896173
[4187]	valid_0's auc: 0.896181
[4188]	valid_0's auc: 0.896182
[4189]	valid_0's auc: 0.896192
[4190]	valid_0's auc: 0.896199
[4191]	valid_0's auc: 0.896198
[4192]	valid_0's auc: 0.8962
[4193]	valid_0's auc: 0.896203
[4194]	valid_0's auc: 0.896202
[4195]	valid_0's auc: 0.896206
[4196]	valid_0's auc: 0.896205
[4197]	valid_0's auc: 0.896206
[4198]	valid_0's auc: 0.89621
[4199]	valid_0's auc: 0.896215
[4200]	valid_0's auc: 0.896217
[4201]	valid_0's auc: 0.896225
[4202]	valid_0's auc: 0.896229
[4203]	valid_0's auc: 0.896234
[4204]	valid_0's auc: 0.896236
[4205]	valid_0's auc: 0.896244
[4206]	valid_0's auc: 0.896245
[4207]	valid_0's auc: 0.896244
[4208]	valid_0's auc: 0.896244
[4209]	valid_0's auc: 0.896246
[4210]	valid_0's auc: 0.896246
[4211]	valid_0's auc: 0.896246
[4212]	valid_0's auc: 0.896259
[4213]	valid_0's auc: 0.896262
[4214]	valid_0's auc: 0.896271
[4215]	valid_0's auc: 0.896262
[4216]	valid_0's auc: 0.896257
[4217]	valid_0's auc: 0.896269
[4218]	valid_0's auc: 0.896275
[4219]	valid_0's auc: 0.896273
[4220]	valid_0's auc: 0.89628
[4221]	valid_0's auc: 0.896274
[4222]	valid_0's auc: 0.896284
[4223]	valid_0's auc: 0.896277
[4224]	valid_0's auc: 0.89628
[4225]	valid_0's auc: 0.896275
[4226]	valid_0's auc: 0.896283
[4227]	valid_0's auc: 0.896283
[4228]	valid_0's auc: 0.896293
[4229]	valid_0's auc: 0.896288
[4230]	valid_0's auc: 0.896283
[4231]	valid_0's auc: 0.896292
[4232]	valid_0's auc: 0.896289
[4233]	valid_0's auc: 0.896296
[4234]	valid_0's auc: 0.896302
[4235]	valid_0's auc: 0.896304
[4236]	valid_0's auc: 0.896302
[4237]	valid_0's auc: 0.896302
[4238]	valid_0's auc: 0.896305
[4239]	valid_0's auc: 0.896304
[4240]	valid_0's auc: 0.896309
[4241]	valid_0's auc: 0.896307
[4242]	valid_0's auc: 0.896311
[4243]	valid_0's auc: 0.896319
[4244]	valid_0's auc: 0.896325
[4245]	valid_0's auc: 0.896326
[4246]	valid_0's auc: 0.896323
[4247]	valid_0's auc: 0.896327
[4248]	valid_0's auc: 0.896323
[4249]	valid_0's auc: 0.896328
[4250]	valid_0's auc: 0.89633
[4251]	valid_0's auc: 0.896321
[4252]	valid_0's auc: 0.896323
[4253]	valid_0's auc: 0.896326
[4254]	valid_0's auc: 0.896324
[4255]	valid_0's auc: 0.896332
[4256]	valid_0's auc: 0.896338
[4257]	valid_0's auc: 0.896338
[4258]	valid_0's auc: 0.896341
[4259]	valid_0's auc: 0.896346
[4260]	valid_0's auc: 0.896352
[4261]	valid_0's auc: 0.896355
[4262]	valid_0's auc: 0.896353
[4263]	valid_0's auc: 0.89636
[4264]	valid_0's auc: 0.896364
[4265]	valid_0's auc: 0.896361
[4266]	valid_0's auc: 0.896357
[4267]	valid_0's auc: 0.896364
[4268]	valid_0's auc: 0.896364
[4269]	valid_0's auc: 0.896364
[4270]	valid_0's auc: 0.896371
[4271]	valid_0's auc: 0.896371
[4272]	valid_0's auc: 0.896365
[4273]	valid_0's auc: 0.896373
[4274]	valid_0's auc: 0.896377
[4275]	valid_0's auc: 0.89638
[4276]	valid_0's auc: 0.896381
[4277]	valid_0's auc: 0.89638
[4278]	valid_0's auc: 0.896381
[4279]	valid_0's auc: 0.896379
[4280]	valid_0's auc: 0.896378
[4281]	valid_0's auc: 0.896379
[4282]	valid_0's auc: 0.896377
[4283]	valid_0's auc: 0.896384
[4284]	valid_0's auc: 0.896388
[4285]	valid_0's auc: 0.896392
[4286]	valid_0's auc: 0.896404
[4287]	valid_0's auc: 0.896405
[4288]	valid_0's auc: 0.896407
[4289]	valid_0's auc: 0.896412
[4290]	valid_0's auc: 0.896411
[4291]	valid_0's auc: 0.896414
[4292]	valid_0's auc: 0.89642
[4293]	valid_0's auc: 0.896414
[4294]	valid_0's auc: 0.896416
[4295]	valid_0's auc: 0.896416
[4296]	valid_0's auc: 0.896417
[4297]	valid_0's auc: 0.896409
[4298]	valid_0's auc: 0.896401
[4299]	valid_0's auc: 0.896404
[4300]	valid_0's auc: 0.896403
[4301]	valid_0's auc: 0.896413
[4302]	valid_0's auc: 0.896415
[4303]	valid_0's auc: 0.896417
[4304]	valid_0's auc: 0.896416
[4305]	valid_0's auc: 0.896412
[4306]	valid_0's auc: 0.896411
[4307]	valid_0's auc: 0.896411
[4308]	valid_0's auc: 0.89642
[4309]	valid_0's auc: 0.896421
[4310]	valid_0's auc: 0.896421
[4311]	valid_0's auc: 0.896424
[4312]	valid_0's auc: 0.896432
[4313]	valid_0's auc: 0.896434
[4314]	valid_0's auc: 0.896437
[4315]	valid_0's auc: 0.896435
[4316]	valid_0's auc: 0.896441
[4317]	valid_0's auc: 0.896443
[4318]	valid_0's auc: 0.896443
[4319]	valid_0's auc: 0.896441
[4320]	valid_0's auc: 0.89645
[4321]	valid_0's auc: 0.896448
[4322]	valid_0's auc: 0.896463
[4323]	valid_0's auc: 0.896467
[4324]	valid_0's auc: 0.896465
[4325]	valid_0's auc: 0.896466
[4326]	valid_0's auc: 0.896454
[4327]	valid_0's auc: 0.896457
[4328]	valid_0's auc: 0.896461
[4329]	valid_0's auc: 0.896467
[4330]	valid_0's auc: 0.896473
[4331]	valid_0's auc: 0.896477
[4332]	valid_0's auc: 0.896478
[4333]	valid_0's auc: 0.896483
[4334]	valid_0's auc: 0.896489
[4335]	valid_0's auc: 0.896486
[4336]	valid_0's auc: 0.89649
[4337]	valid_0's auc: 0.896489
[4338]	valid_0's auc: 0.896494
[4339]	valid_0's auc: 0.896494
[4340]	valid_0's auc: 0.896492
[4341]	valid_0's auc: 0.896492
[4342]	valid_0's auc: 0.896494
[4343]	valid_0's auc: 0.896495
[4344]	valid_0's auc: 0.896504
[4345]	valid_0's auc: 0.896507
[4346]	valid_0's auc: 0.896509
[4347]	valid_0's auc: 0.896519
[4348]	valid_0's auc: 0.896517
[4349]	valid_0's auc: 0.896516
[4350]	valid_0's auc: 0.896519
[4351]	valid_0's auc: 0.896517
[4352]	valid_0's auc: 0.89652
[4353]	valid_0's auc: 0.896526
[4354]	valid_0's auc: 0.89653
[4355]	valid_0's auc: 0.896537
[4356]	valid_0's auc: 0.896531
[4357]	valid_0's auc: 0.896527
[4358]	valid_0's auc: 0.89653
[4359]	valid_0's auc: 0.89653
[4360]	valid_0's auc: 0.896527
[4361]	valid_0's auc: 0.896537
[4362]	valid_0's auc: 0.896534
[4363]	valid_0's auc: 0.896546
[4364]	valid_0's auc: 0.896558
[4365]	valid_0's auc: 0.896556
[4366]	valid_0's auc: 0.896564
[4367]	valid_0's auc: 0.896566
[4368]	valid_0's auc: 0.896565
[4369]	valid_0's auc: 0.896562
[4370]	valid_0's auc: 0.896567
[4371]	valid_0's auc: 0.896571
[4372]	valid_0's auc: 0.896573
[4373]	valid_0's auc: 0.896574
[4374]	valid_0's auc: 0.896585
[4375]	valid_0's auc: 0.896588
[4376]	valid_0's auc: 0.896586
[4377]	valid_0's auc: 0.896592
[4378]	valid_0's auc: 0.896596
[4379]	valid_0's auc: 0.896599
[4380]	valid_0's auc: 0.896606
[4381]	valid_0's auc: 0.896613
[4382]	valid_0's auc: 0.896614
[4383]	valid_0's auc: 0.89661
[4384]	valid_0's auc: 0.896615
[4385]	valid_0's auc: 0.896619
[4386]	valid_0's auc: 0.896627
[4387]	valid_0's auc: 0.896634
[4388]	valid_0's auc: 0.896636
[4389]	valid_0's auc: 0.896627
[4390]	valid_0's auc: 0.896629
[4391]	valid_0's auc: 0.896629
[4392]	valid_0's auc: 0.896631
[4393]	valid_0's auc: 0.896637
[4394]	valid_0's auc: 0.89664
[4395]	valid_0's auc: 0.896651
[4396]	valid_0's auc: 0.896656
[4397]	valid_0's auc: 0.896661
[4398]	valid_0's auc: 0.896663
[4399]	valid_0's auc: 0.896665
[4400]	valid_0's auc: 0.896668
[4401]	valid_0's auc: 0.896663
[4402]	valid_0's auc: 0.896668
[4403]	valid_0's auc: 0.896667
[4404]	valid_0's auc: 0.896673
[4405]	valid_0's auc: 0.896678
[4406]	valid_0's auc: 0.896676
[4407]	valid_0's auc: 0.896682
[4408]	valid_0's auc: 0.896689
[4409]	valid_0's auc: 0.896691
[4410]	valid_0's auc: 0.896693
[4411]	valid_0's auc: 0.896702
[4412]	valid_0's auc: 0.896705
[4413]	valid_0's auc: 0.896706
[4414]	valid_0's auc: 0.896715
[4415]	valid_0's auc: 0.896715
[4416]	valid_0's auc: 0.896713
[4417]	valid_0's auc: 0.896713
[4418]	valid_0's auc: 0.896711
[4419]	valid_0's auc: 0.896707
[4420]	valid_0's auc: 0.896712
[4421]	valid_0's auc: 0.896703
[4422]	valid_0's auc: 0.896712
[4423]	valid_0's auc: 0.896721
[4424]	valid_0's auc: 0.896709
[4425]	valid_0's auc: 0.896712
[4426]	valid_0's auc: 0.896711
[4427]	valid_0's auc: 0.896708
[4428]	valid_0's auc: 0.896725
[4429]	valid_0's auc: 0.896726
[4430]	valid_0's auc: 0.896729
[4431]	valid_0's auc: 0.896733
[4432]	valid_0's auc: 0.896733
[4433]	valid_0's auc: 0.896733
[4434]	valid_0's auc: 0.896737
[4435]	valid_0's auc: 0.896746
[4436]	valid_0's auc: 0.896747
[4437]	valid_0's auc: 0.896743
[4438]	valid_0's auc: 0.896741
[4439]	valid_0's auc: 0.896748
[4440]	valid_0's auc: 0.89675
[4441]	valid_0's auc: 0.896752
[4442]	valid_0's auc: 0.896753
[4443]	valid_0's auc: 0.896763
[4444]	valid_0's auc: 0.896765
[4445]	valid_0's auc: 0.896768
[4446]	valid_0's auc: 0.89676
[4447]	valid_0's auc: 0.896756
[4448]	valid_0's auc: 0.896753
[4449]	valid_0's auc: 0.896762
[4450]	valid_0's auc: 0.896767
[4451]	valid_0's auc: 0.896769
[4452]	valid_0's auc: 0.896769
[4453]	valid_0's auc: 0.896767
[4454]	valid_0's auc: 0.896772
[4455]	valid_0's auc: 0.89678
[4456]	valid_0's auc: 0.896785
[4457]	valid_0's auc: 0.89679
[4458]	valid_0's auc: 0.896791
[4459]	valid_0's auc: 0.896793
[4460]	valid_0's auc: 0.8968
[4461]	valid_0's auc: 0.896805
[4462]	valid_0's auc: 0.896805
[4463]	valid_0's auc: 0.896806
[4464]	valid_0's auc: 0.896802
[4465]	valid_0's auc: 0.896804
[4466]	valid_0's auc: 0.896811
[4467]	valid_0's auc: 0.896805
[4468]	valid_0's auc: 0.896807
[4469]	valid_0's auc: 0.896812
[4470]	valid_0's auc: 0.896817
[4471]	valid_0's auc: 0.896815
[4472]	valid_0's auc: 0.896807
[4473]	valid_0's auc: 0.896806
[4474]	valid_0's auc: 0.896815
[4475]	valid_0's auc: 0.896817
[4476]	valid_0's auc: 0.896818
[4477]	valid_0's auc: 0.89682
[4478]	valid_0's auc: 0.896822
[4479]	valid_0's auc: 0.896818
[4480]	valid_0's auc: 0.896826
[4481]	valid_0's auc: 0.896829
[4482]	valid_0's auc: 0.896828
[4483]	valid_0's auc: 0.896827
[4484]	valid_0's auc: 0.89683
[4485]	valid_0's auc: 0.896828
[4486]	valid_0's auc: 0.896824
[4487]	valid_0's auc: 0.89683
[4488]	valid_0's auc: 0.896838
[4489]	valid_0's auc: 0.896838
[4490]	valid_0's auc: 0.896847
[4491]	valid_0's auc: 0.896844
[4492]	valid_0's auc: 0.896845
[4493]	valid_0's auc: 0.89685
[4494]	valid_0's auc: 0.896853
[4495]	valid_0's auc: 0.896852
[4496]	valid_0's auc: 0.896851
[4497]	valid_0's auc: 0.896845
[4498]	valid_0's auc: 0.896847
[4499]	valid_0's auc: 0.896847
[4500]	valid_0's auc: 0.896859
[4501]	valid_0's auc: 0.896861
[4502]	valid_0's auc: 0.896865
[4503]	valid_0's auc: 0.896873
[4504]	valid_0's auc: 0.896874
[4505]	valid_0's auc: 0.896872
[4506]	valid_0's auc: 0.896867
[4507]	valid_0's auc: 0.896867
[4508]	valid_0's auc: 0.896863
[4509]	valid_0's auc: 0.896861
[4510]	valid_0's auc: 0.896861
[4511]	valid_0's auc: 0.896864
[4512]	valid_0's auc: 0.896859
[4513]	valid_0's auc: 0.89686
[4514]	valid_0's auc: 0.896869
[4515]	valid_0's auc: 0.896868
[4516]	valid_0's auc: 0.896863
[4517]	valid_0's auc: 0.896869
[4518]	valid_0's auc: 0.896875
[4519]	valid_0's auc: 0.896866
[4520]	valid_0's auc: 0.89687
[4521]	valid_0's auc: 0.896877
[4522]	valid_0's auc: 0.896878
[4523]	valid_0's auc: 0.896878
[4524]	valid_0's auc: 0.896887
[4525]	valid_0's auc: 0.896897
[4526]	valid_0's auc: 0.896899
[4527]	valid_0's auc: 0.896897
[4528]	valid_0's auc: 0.896893
[4529]	valid_0's auc: 0.896891
[4530]	valid_0's auc: 0.896885
[4531]	valid_0's auc: 0.896885
[4532]	valid_0's auc: 0.89689
[4533]	valid_0's auc: 0.896892
[4534]	valid_0's auc: 0.896894
[4535]	valid_0's auc: 0.896893
[4536]	valid_0's auc: 0.896888
[4537]	valid_0's auc: 0.896885
[4538]	valid_0's auc: 0.89689
[4539]	valid_0's auc: 0.896888
[4540]	valid_0's auc: 0.896882
[4541]	valid_0's auc: 0.896888
[4542]	valid_0's auc: 0.896897
[4543]	valid_0's auc: 0.896902
[4544]	valid_0's auc: 0.896905
[4545]	valid_0's auc: 0.896899
[4546]	valid_0's auc: 0.896899
[4547]	valid_0's auc: 0.896903
[4548]	valid_0's auc: 0.896915
[4549]	valid_0's auc: 0.896918
[4550]	valid_0's auc: 0.896925
[4551]	valid_0's auc: 0.896931
[4552]	valid_0's auc: 0.896937
[4553]	valid_0's auc: 0.896939
[4554]	valid_0's auc: 0.896936
[4555]	valid_0's auc: 0.896934
[4556]	valid_0's auc: 0.896934
[4557]	valid_0's auc: 0.896934
[4558]	valid_0's auc: 0.896935
[4559]	valid_0's auc: 0.896937
[4560]	valid_0's auc: 0.896935
[4561]	valid_0's auc: 0.896933
[4562]	valid_0's auc: 0.896934
[4563]	valid_0's auc: 0.896938
[4564]	valid_0's auc: 0.896939
[4565]	valid_0's auc: 0.896945
[4566]	valid_0's auc: 0.89695
[4567]	valid_0's auc: 0.896951
[4568]	valid_0's auc: 0.896956
[4569]	valid_0's auc: 0.896948
[4570]	valid_0's auc: 0.896946
[4571]	valid_0's auc: 0.89695
[4572]	valid_0's auc: 0.896957
[4573]	valid_0's auc: 0.896955
[4574]	valid_0's auc: 0.896952
[4575]	valid_0's auc: 0.896954
[4576]	valid_0's auc: 0.89696
[4577]	valid_0's auc: 0.896966
[4578]	valid_0's auc: 0.896974
[4579]	valid_0's auc: 0.896976
[4580]	valid_0's auc: 0.896982
[4581]	valid_0's auc: 0.896988
[4582]	valid_0's auc: 0.896984
[4583]	valid_0's auc: 0.896986
[4584]	valid_0's auc: 0.896985
[4585]	valid_0's auc: 0.896986
[4586]	valid_0's auc: 0.896984
[4587]	valid_0's auc: 0.896986
[4588]	valid_0's auc: 0.896988
[4589]	valid_0's auc: 0.896996
[4590]	valid_0's auc: 0.897001
[4591]	valid_0's auc: 0.897008
[4592]	valid_0's auc: 0.897005
[4593]	valid_0's auc: 0.897008
[4594]	valid_0's auc: 0.897006
[4595]	valid_0's auc: 0.897006
[4596]	valid_0's auc: 0.897003
[4597]	valid_0's auc: 0.897008
[4598]	valid_0's auc: 0.89701
[4599]	valid_0's auc: 0.897007
[4600]	valid_0's auc: 0.897005
[4601]	valid_0's auc: 0.897011
[4602]	valid_0's auc: 0.897006
[4603]	valid_0's auc: 0.897005
[4604]	valid_0's auc: 0.897011
[4605]	valid_0's auc: 0.897006
[4606]	valid_0's auc: 0.897013
[4607]	valid_0's auc: 0.897022
[4608]	valid_0's auc: 0.897013
[4609]	valid_0's auc: 0.897013
[4610]	valid_0's auc: 0.89701
[4611]	valid_0's auc: 0.897007
[4612]	valid_0's auc: 0.89701
[4613]	valid_0's auc: 0.897005
[4614]	valid_0's auc: 0.897001
[4615]	valid_0's auc: 0.896998
[4616]	valid_0's auc: 0.896998
[4617]	valid_0's auc: 0.896998
[4618]	valid_0's auc: 0.897004
[4619]	valid_0's auc: 0.897004
[4620]	valid_0's auc: 0.897004
[4621]	valid_0's auc: 0.897009
[4622]	valid_0's auc: 0.897016
[4623]	valid_0's auc: 0.89702
[4624]	valid_0's auc: 0.897029
[4625]	valid_0's auc: 0.897033
[4626]	valid_0's auc: 0.897032
[4627]	valid_0's auc: 0.897029
[4628]	valid_0's auc: 0.89703
[4629]	valid_0's auc: 0.897033
[4630]	valid_0's auc: 0.897038
[4631]	valid_0's auc: 0.897047
[4632]	valid_0's auc: 0.897051
[4633]	valid_0's auc: 0.897046
[4634]	valid_0's auc: 0.897048
[4635]	valid_0's auc: 0.897052
[4636]	valid_0's auc: 0.897057
[4637]	valid_0's auc: 0.897062
[4638]	valid_0's auc: 0.897064
[4639]	valid_0's auc: 0.897065
[4640]	valid_0's auc: 0.89707
[4641]	valid_0's auc: 0.897077
[4642]	valid_0's auc: 0.897086
[4643]	valid_0's auc: 0.897097
[4644]	valid_0's auc: 0.897099
[4645]	valid_0's auc: 0.8971
[4646]	valid_0's auc: 0.897112
[4647]	valid_0's auc: 0.897109
[4648]	valid_0's auc: 0.897108
[4649]	valid_0's auc: 0.897102
[4650]	valid_0's auc: 0.897108
[4651]	valid_0's auc: 0.897117
[4652]	valid_0's auc: 0.897123
[4653]	valid_0's auc: 0.897122
[4654]	valid_0's auc: 0.897125
[4655]	valid_0's auc: 0.897123
[4656]	valid_0's auc: 0.897121
[4657]	valid_0's auc: 0.897121
[4658]	valid_0's auc: 0.897126
[4659]	valid_0's auc: 0.897128
[4660]	valid_0's auc: 0.897137
[4661]	valid_0's auc: 0.897135
[4662]	valid_0's auc: 0.897138
[4663]	valid_0's auc: 0.897138
[4664]	valid_0's auc: 0.897145
[4665]	valid_0's auc: 0.897147
[4666]	valid_0's auc: 0.897156
[4667]	valid_0's auc: 0.897158
[4668]	valid_0's auc: 0.897163
[4669]	valid_0's auc: 0.897173
[4670]	valid_0's auc: 0.897179
[4671]	valid_0's auc: 0.897181
[4672]	valid_0's auc: 0.897184
[4673]	valid_0's auc: 0.897184
[4674]	valid_0's auc: 0.897185
[4675]	valid_0's auc: 0.897186
[4676]	valid_0's auc: 0.897185
[4677]	valid_0's auc: 0.8972
[4678]	valid_0's auc: 0.897203
[4679]	valid_0's auc: 0.897212
[4680]	valid_0's auc: 0.89721
[4681]	valid_0's auc: 0.89721
[4682]	valid_0's auc: 0.897218
[4683]	valid_0's auc: 0.897217
[4684]	valid_0's auc: 0.897226
[4685]	valid_0's auc: 0.897229
[4686]	valid_0's auc: 0.897227
[4687]	valid_0's auc: 0.897233
[4688]	valid_0's auc: 0.897229
[4689]	valid_0's auc: 0.897231
[4690]	valid_0's auc: 0.897231
[4691]	valid_0's auc: 0.897228
[4692]	valid_0's auc: 0.89723
[4693]	valid_0's auc: 0.897229
[4694]	valid_0's auc: 0.897238
[4695]	valid_0's auc: 0.897233
[4696]	valid_0's auc: 0.897236
[4697]	valid_0's auc: 0.897248
[4698]	valid_0's auc: 0.897251
[4699]	valid_0's auc: 0.897252
[4700]	valid_0's auc: 0.897256
[4701]	valid_0's auc: 0.897251
[4702]	valid_0's auc: 0.897256
[4703]	valid_0's auc: 0.897253
[4704]	valid_0's auc: 0.897248
[4705]	valid_0's auc: 0.897252
[4706]	valid_0's auc: 0.897261
[4707]	valid_0's auc: 0.897262
[4708]	valid_0's auc: 0.897266
[4709]	valid_0's auc: 0.897268
[4710]	valid_0's auc: 0.897263
[4711]	valid_0's auc: 0.897275
[4712]	valid_0's auc: 0.897273
[4713]	valid_0's auc: 0.897278
[4714]	valid_0's auc: 0.897279
[4715]	valid_0's auc: 0.897271
[4716]	valid_0's auc: 0.897271
[4717]	valid_0's auc: 0.897272
[4718]	valid_0's auc: 0.897272
[4719]	valid_0's auc: 0.897277
[4720]	valid_0's auc: 0.897284
[4721]	valid_0's auc: 0.897289
[4722]	valid_0's auc: 0.897294
[4723]	valid_0's auc: 0.897296
[4724]	valid_0's auc: 0.897296
[4725]	valid_0's auc: 0.897292
[4726]	valid_0's auc: 0.897301
[4727]	valid_0's auc: 0.897294
[4728]	valid_0's auc: 0.897306
[4729]	valid_0's auc: 0.897306
[4730]	valid_0's auc: 0.897307
[4731]	valid_0's auc: 0.89731
[4732]	valid_0's auc: 0.897308
[4733]	valid_0's auc: 0.897311
[4734]	valid_0's auc: 0.897314
[4735]	valid_0's auc: 0.897324
[4736]	valid_0's auc: 0.897337
[4737]	valid_0's auc: 0.897343
[4738]	valid_0's auc: 0.897344
[4739]	valid_0's auc: 0.897345
[4740]	valid_0's auc: 0.897352
[4741]	valid_0's auc: 0.897364
[4742]	valid_0's auc: 0.897367
[4743]	valid_0's auc: 0.897375
[4744]	valid_0's auc: 0.897376
[4745]	valid_0's auc: 0.897376
[4746]	valid_0's auc: 0.897373
[4747]	valid_0's auc: 0.897374
[4748]	valid_0's auc: 0.897372
[4749]	valid_0's auc: 0.897374
[4750]	valid_0's auc: 0.897365
[4751]	valid_0's auc: 0.897362
[4752]	valid_0's auc: 0.897363
[4753]	valid_0's auc: 0.897363
[4754]	valid_0's auc: 0.897364
[4755]	valid_0's auc: 0.897369
[4756]	valid_0's auc: 0.89737
[4757]	valid_0's auc: 0.89737
[4758]	valid_0's auc: 0.897372
[4759]	valid_0's auc: 0.897373
[4760]	valid_0's auc: 0.897373
[4761]	valid_0's auc: 0.897369
[4762]	valid_0's auc: 0.897369
[4763]	valid_0's auc: 0.89737
[4764]	valid_0's auc: 0.89737
[4765]	valid_0's auc: 0.897373
[4766]	valid_0's auc: 0.897375
[4767]	valid_0's auc: 0.897375
[4768]	valid_0's auc: 0.897373
[4769]	valid_0's auc: 0.897367
[4770]	valid_0's auc: 0.897377
[4771]	valid_0's auc: 0.897378
[4772]	valid_0's auc: 0.897385
[4773]	valid_0's auc: 0.897384
[4774]	valid_0's auc: 0.897385
[4775]	valid_0's auc: 0.897385
[4776]	valid_0's auc: 0.897381
[4777]	valid_0's auc: 0.897378
[4778]	valid_0's auc: 0.897377
[4779]	valid_0's auc: 0.897378
[4780]	valid_0's auc: 0.897384
[4781]	valid_0's auc: 0.897391
[4782]	valid_0's auc: 0.897392
[4783]	valid_0's auc: 0.897395
[4784]	valid_0's auc: 0.897398
[4785]	valid_0's auc: 0.8974
[4786]	valid_0's auc: 0.897393
[4787]	valid_0's auc: 0.897392
[4788]	valid_0's auc: 0.897387
[4789]	valid_0's auc: 0.897379
[4790]	valid_0's auc: 0.89738
[4791]	valid_0's auc: 0.897377
[4792]	valid_0's auc: 0.897379
[4793]	valid_0's auc: 0.897378
[4794]	valid_0's auc: 0.897379
[4795]	valid_0's auc: 0.89738
[4796]	valid_0's auc: 0.897382
[4797]	valid_0's auc: 0.89739
[4798]	valid_0's auc: 0.897395
[4799]	valid_0's auc: 0.897401
[4800]	valid_0's auc: 0.8974
[4801]	valid_0's auc: 0.897403
[4802]	valid_0's auc: 0.897406
[4803]	valid_0's auc: 0.897406
[4804]	valid_0's auc: 0.897409
[4805]	valid_0's auc: 0.897408
[4806]	valid_0's auc: 0.897405
[4807]	valid_0's auc: 0.897405
[4808]	valid_0's auc: 0.897408
[4809]	valid_0's auc: 0.897405
[4810]	valid_0's auc: 0.897402
[4811]	valid_0's auc: 0.897399
[4812]	valid_0's auc: 0.897399
[4813]	valid_0's auc: 0.897401
[4814]	valid_0's auc: 0.897401
[4815]	valid_0's auc: 0.897407
[4816]	valid_0's auc: 0.897406
[4817]	valid_0's auc: 0.89741
[4818]	valid_0's auc: 0.897404
[4819]	valid_0's auc: 0.897403
[4820]	valid_0's auc: 0.897412
[4821]	valid_0's auc: 0.897421
[4822]	valid_0's auc: 0.897416
[4823]	valid_0's auc: 0.897421
[4824]	valid_0's auc: 0.897411
[4825]	valid_0's auc: 0.897408
[4826]	valid_0's auc: 0.897409
[4827]	valid_0's auc: 0.897408
[4828]	valid_0's auc: 0.897412
[4829]	valid_0's auc: 0.897413
[4830]	valid_0's auc: 0.897407
[4831]	valid_0's auc: 0.897407
[4832]	valid_0's auc: 0.89741
[4833]	valid_0's auc: 0.897409
[4834]	valid_0's auc: 0.897403
[4835]	valid_0's auc: 0.897401
[4836]	valid_0's auc: 0.89741
[4837]	valid_0's auc: 0.897407
[4838]	valid_0's auc: 0.897407
[4839]	valid_0's auc: 0.897414
[4840]	valid_0's auc: 0.897415
[4841]	valid_0's auc: 0.897417
[4842]	valid_0's auc: 0.897419
[4843]	valid_0's auc: 0.897423
[4844]	valid_0's auc: 0.897421
[4845]	valid_0's auc: 0.897426
[4846]	valid_0's auc: 0.897428
[4847]	valid_0's auc: 0.897427
[4848]	valid_0's auc: 0.897428
[4849]	valid_0's auc: 0.897429
[4850]	valid_0's auc: 0.897436
[4851]	valid_0's auc: 0.897435
[4852]	valid_0's auc: 0.897441
[4853]	valid_0's auc: 0.897444
[4854]	valid_0's auc: 0.897441
[4855]	valid_0's auc: 0.89744
[4856]	valid_0's auc: 0.89744
[4857]	valid_0's auc: 0.897439
[4858]	valid_0's auc: 0.897438
[4859]	valid_0's auc: 0.897439
[4860]	valid_0's auc: 0.897448
[4861]	valid_0's auc: 0.897454
[4862]	valid_0's auc: 0.897459
[4863]	valid_0's auc: 0.897462
[4864]	valid_0's auc: 0.897468
[4865]	valid_0's auc: 0.897476
[4866]	valid_0's auc: 0.897475
[4867]	valid_0's auc: 0.897476
[4868]	valid_0's auc: 0.897479
[4869]	valid_0's auc: 0.897486
[4870]	valid_0's auc: 0.897486
[4871]	valid_0's auc: 0.897495
[4872]	valid_0's auc: 0.897494
[4873]	valid_0's auc: 0.897494
[4874]	valid_0's auc: 0.897497
[4875]	valid_0's auc: 0.897506
[4876]	valid_0's auc: 0.897506
[4877]	valid_0's auc: 0.897507
[4878]	valid_0's auc: 0.89751
[4879]	valid_0's auc: 0.897509
[4880]	valid_0's auc: 0.897502
[4881]	valid_0's auc: 0.897504
[4882]	valid_0's auc: 0.8975
[4883]	valid_0's auc: 0.897495
[4884]	valid_0's auc: 0.897504
[4885]	valid_0's auc: 0.89751
[4886]	valid_0's auc: 0.897516
[4887]	valid_0's auc: 0.89752
[4888]	valid_0's auc: 0.897526
[4889]	valid_0's auc: 0.897525
[4890]	valid_0's auc: 0.897519
[4891]	valid_0's auc: 0.897513
[4892]	valid_0's auc: 0.897514
[4893]	valid_0's auc: 0.897511
[4894]	valid_0's auc: 0.897515
[4895]	valid_0's auc: 0.897515
[4896]	valid_0's auc: 0.897524
[4897]	valid_0's auc: 0.89753
[4898]	valid_0's auc: 0.897532
[4899]	valid_0's auc: 0.89754
[4900]	valid_0's auc: 0.897546
[4901]	valid_0's auc: 0.897553
[4902]	valid_0's auc: 0.897557
[4903]	valid_0's auc: 0.897558
[4904]	valid_0's auc: 0.897565
[4905]	valid_0's auc: 0.897563
[4906]	valid_0's auc: 0.897561
[4907]	valid_0's auc: 0.897572
[4908]	valid_0's auc: 0.897575
[4909]	valid_0's auc: 0.897578
[4910]	valid_0's auc: 0.897572
[4911]	valid_0's auc: 0.897569
[4912]	valid_0's auc: 0.897575
[4913]	valid_0's auc: 0.897578
[4914]	valid_0's auc: 0.897582
[4915]	valid_0's auc: 0.897584
[4916]	valid_0's auc: 0.897593
[4917]	valid_0's auc: 0.897603
[4918]	valid_0's auc: 0.8976
[4919]	valid_0's auc: 0.897594
[4920]	valid_0's auc: 0.897592
[4921]	valid_0's auc: 0.897592
[4922]	valid_0's auc: 0.897597
[4923]	valid_0's auc: 0.897606
[4924]	valid_0's auc: 0.897612
[4925]	valid_0's auc: 0.897615
[4926]	valid_0's auc: 0.897622
[4927]	valid_0's auc: 0.897627
[4928]	valid_0's auc: 0.897634
[4929]	valid_0's auc: 0.897642
[4930]	valid_0's auc: 0.897647
[4931]	valid_0's auc: 0.897646
[4932]	valid_0's auc: 0.897641
[4933]	valid_0's auc: 0.897637
[4934]	valid_0's auc: 0.897643
[4935]	valid_0's auc: 0.897643
[4936]	valid_0's auc: 0.897636
[4937]	valid_0's auc: 0.897636
[4938]	valid_0's auc: 0.897636
[4939]	valid_0's auc: 0.89763
[4940]	valid_0's auc: 0.897635
[4941]	valid_0's auc: 0.897637
[4942]	valid_0's auc: 0.897635
[4943]	valid_0's auc: 0.897634
[4944]	valid_0's auc: 0.897635
[4945]	valid_0's auc: 0.897635
[4946]	valid_0's auc: 0.897639
[4947]	valid_0's auc: 0.897634
[4948]	valid_0's auc: 0.897631
[4949]	valid_0's auc: 0.897631
[4950]	valid_0's auc: 0.897636
[4951]	valid_0's auc: 0.897645
[4952]	valid_0's auc: 0.897653
[4953]	valid_0's auc: 0.897652
[4954]	valid_0's auc: 0.897648
[4955]	valid_0's auc: 0.897654
[4956]	valid_0's auc: 0.897652
[4957]	valid_0's auc: 0.897654
[4958]	valid_0's auc: 0.897645
[4959]	valid_0's auc: 0.897634
[4960]	valid_0's auc: 0.897639
[4961]	valid_0's auc: 0.897634
[4962]	valid_0's auc: 0.897636
[4963]	valid_0's auc: 0.897635
[4964]	valid_0's auc: 0.897635
[4965]	valid_0's auc: 0.897636
[4966]	valid_0's auc: 0.897635
[4967]	valid_0's auc: 0.897643
[4968]	valid_0's auc: 0.897652
[4969]	valid_0's auc: 0.897661
[4970]	valid_0's auc: 0.897662
[4971]	valid_0's auc: 0.897655
[4972]	valid_0's auc: 0.897656
[4973]	valid_0's auc: 0.89765
[4974]	valid_0's auc: 0.897651
[4975]	valid_0's auc: 0.897654
[4976]	valid_0's auc: 0.897663
[4977]	valid_0's auc: 0.897668
[4978]	valid_0's auc: 0.897677
[4979]	valid_0's auc: 0.897678
[4980]	valid_0's auc: 0.897685
[4981]	valid_0's auc: 0.897682
[4982]	valid_0's auc: 0.89768
[4983]	valid_0's auc: 0.89768
[4984]	valid_0's auc: 0.897681
[4985]	valid_0's auc: 0.897674
[4986]	valid_0's auc: 0.897674
[4987]	valid_0's auc: 0.897678
[4988]	valid_0's auc: 0.897679
[4989]	valid_0's auc: 0.897678
[4990]	valid_0's auc: 0.897686
[4991]	valid_0's auc: 0.897696
[4992]	valid_0's auc: 0.897698
[4993]	valid_0's auc: 0.897696
[4994]	valid_0's auc: 0.897696
[4995]	valid_0's auc: 0.897701
[4996]	valid_0's auc: 0.897708
[4997]	valid_0's auc: 0.897711
[4998]	valid_0's auc: 0.897715
[4999]	valid_0's auc: 0.897715
[5000]	valid_0's auc: 0.897718
[5001]	valid_0's auc: 0.897725
[5002]	valid_0's auc: 0.897725
[5003]	valid_0's auc: 0.897727
[5004]	valid_0's auc: 0.897734
[5005]	valid_0's auc: 0.89773
[5006]	valid_0's auc: 0.89773
[5007]	valid_0's auc: 0.897729
[5008]	valid_0's auc: 0.897732
[5009]	valid_0's auc: 0.897736
[5010]	valid_0's auc: 0.897735
[5011]	valid_0's auc: 0.897741
[5012]	valid_0's auc: 0.897753
[5013]	valid_0's auc: 0.897748
[5014]	valid_0's auc: 0.89774
[5015]	valid_0's auc: 0.897739
[5016]	valid_0's auc: 0.897739
[5017]	valid_0's auc: 0.897741
[5018]	valid_0's auc: 0.897742
[5019]	valid_0's auc: 0.897743
[5020]	valid_0's auc: 0.897752
[5021]	valid_0's auc: 0.897756
[5022]	valid_0's auc: 0.897752
[5023]	valid_0's auc: 0.897758
[5024]	valid_0's auc: 0.897758
[5025]	valid_0's auc: 0.89775
[5026]	valid_0's auc: 0.89775
[5027]	valid_0's auc: 0.897752
[5028]	valid_0's auc: 0.897757
[5029]	valid_0's auc: 0.897754
[5030]	valid_0's auc: 0.897752
[5031]	valid_0's auc: 0.897747
[5032]	valid_0's auc: 0.897754
[5033]	valid_0's auc: 0.897753
[5034]	valid_0's auc: 0.897749
[5035]	valid_0's auc: 0.897748
[5036]	valid_0's auc: 0.897751
[5037]	valid_0's auc: 0.897752
[5038]	valid_0's auc: 0.897756
[5039]	valid_0's auc: 0.897763
[5040]	valid_0's auc: 0.897766
[5041]	valid_0's auc: 0.897763
[5042]	valid_0's auc: 0.897766
[5043]	valid_0's auc: 0.897772
[5044]	valid_0's auc: 0.897773
[5045]	valid_0's auc: 0.897774
[5046]	valid_0's auc: 0.897774
[5047]	valid_0's auc: 0.897768
[5048]	valid_0's auc: 0.897764
[5049]	valid_0's auc: 0.897768
[5050]	valid_0's auc: 0.897766
[5051]	valid_0's auc: 0.897764
[5052]	valid_0's auc: 0.897771
[5053]	valid_0's auc: 0.897773
[5054]	valid_0's auc: 0.897779
[5055]	valid_0's auc: 0.897779
[5056]	valid_0's auc: 0.897782
[5057]	valid_0's auc: 0.897784
[5058]	valid_0's auc: 0.897786
[5059]	valid_0's auc: 0.897788
[5060]	valid_0's auc: 0.897794
[5061]	valid_0's auc: 0.897794
[5062]	valid_0's auc: 0.897794
[5063]	valid_0's auc: 0.897794
[5064]	valid_0's auc: 0.897793
[5065]	valid_0's auc: 0.897798
[5066]	valid_0's auc: 0.897801
[5067]	valid_0's auc: 0.897803
[5068]	valid_0's auc: 0.897799
[5069]	valid_0's auc: 0.897798
[5070]	valid_0's auc: 0.897797
[5071]	valid_0's auc: 0.897794
[5072]	valid_0's auc: 0.897798
[5073]	valid_0's auc: 0.897793
[5074]	valid_0's auc: 0.897794
[5075]	valid_0's auc: 0.897793
[5076]	valid_0's auc: 0.897795
[5077]	valid_0's auc: 0.89779
[5078]	valid_0's auc: 0.897795
[5079]	valid_0's auc: 0.897795
[5080]	valid_0's auc: 0.897796
[5081]	valid_0's auc: 0.8978
[5082]	valid_0's auc: 0.897796
[5083]	valid_0's auc: 0.89779
[5084]	valid_0's auc: 0.897793
[5085]	valid_0's auc: 0.8978
[5086]	valid_0's auc: 0.897803
[5087]	valid_0's auc: 0.897807
[5088]	valid_0's auc: 0.897808
[5089]	valid_0's auc: 0.897807
[5090]	valid_0's auc: 0.897812
[5091]	valid_0's auc: 0.897812
[5092]	valid_0's auc: 0.897808
[5093]	valid_0's auc: 0.897807
[5094]	valid_0's auc: 0.897805
[5095]	valid_0's auc: 0.897801
[5096]	valid_0's auc: 0.897804
[5097]	valid_0's auc: 0.897802
[5098]	valid_0's auc: 0.897802
[5099]	valid_0's auc: 0.897794
[5100]	valid_0's auc: 0.897797
[5101]	valid_0's auc: 0.897796
[5102]	valid_0's auc: 0.897794
[5103]	valid_0's auc: 0.897795
[5104]	valid_0's auc: 0.897799
[5105]	valid_0's auc: 0.897803
[5106]	valid_0's auc: 0.897803
[5107]	valid_0's auc: 0.897805
[5108]	valid_0's auc: 0.897813
[5109]	valid_0's auc: 0.897806
[5110]	valid_0's auc: 0.897803
[5111]	valid_0's auc: 0.897806
[5112]	valid_0's auc: 0.897805
[5113]	valid_0's auc: 0.897809
[5114]	valid_0's auc: 0.89781
[5115]	valid_0's auc: 0.897804
[5116]	valid_0's auc: 0.897807
[5117]	valid_0's auc: 0.897806
[5118]	valid_0's auc: 0.897809
[5119]	valid_0's auc: 0.897803
[5120]	valid_0's auc: 0.89781
[5121]	valid_0's auc: 0.89781
[5122]	valid_0's auc: 0.897817
[5123]	valid_0's auc: 0.897815
[5124]	valid_0's auc: 0.897821
[5125]	valid_0's auc: 0.897816
[5126]	valid_0's auc: 0.897815
[5127]	valid_0's auc: 0.897821
[5128]	valid_0's auc: 0.897817
[5129]	valid_0's auc: 0.897817
[5130]	valid_0's auc: 0.897821
[5131]	valid_0's auc: 0.897818
[5132]	valid_0's auc: 0.89782
[5133]	valid_0's auc: 0.897825
[5134]	valid_0's auc: 0.897821
[5135]	valid_0's auc: 0.897824
[5136]	valid_0's auc: 0.89783
[5137]	valid_0's auc: 0.897829
[5138]	valid_0's auc: 0.897826
[5139]	valid_0's auc: 0.897821
[5140]	valid_0's auc: 0.897828
[5141]	valid_0's auc: 0.897836
[5142]	valid_0's auc: 0.897842
[5143]	valid_0's auc: 0.897841
[5144]	valid_0's auc: 0.897839
[5145]	valid_0's auc: 0.897838
[5146]	valid_0's auc: 0.897833
[5147]	valid_0's auc: 0.897831
[5148]	valid_0's auc: 0.897828
[5149]	valid_0's auc: 0.897833
[5150]	valid_0's auc: 0.897834
[5151]	valid_0's auc: 0.897834
[5152]	valid_0's auc: 0.897835
[5153]	valid_0's auc: 0.897834
[5154]	valid_0's auc: 0.897833
[5155]	valid_0's auc: 0.897832
[5156]	valid_0's auc: 0.897841
[5157]	valid_0's auc: 0.897841
[5158]	valid_0's auc: 0.897836
[5159]	valid_0's auc: 0.89784
[5160]	valid_0's auc: 0.897841
[5161]	valid_0's auc: 0.897847
[5162]	valid_0's auc: 0.897848
[5163]	valid_0's auc: 0.897848
[5164]	valid_0's auc: 0.897849
[5165]	valid_0's auc: 0.897855
[5166]	valid_0's auc: 0.897858
[5167]	valid_0's auc: 0.89786
[5168]	valid_0's auc: 0.897857
[5169]	valid_0's auc: 0.897855
[5170]	valid_0's auc: 0.897856
[5171]	valid_0's auc: 0.897852
[5172]	valid_0's auc: 0.897857
[5173]	valid_0's auc: 0.897854
[5174]	valid_0's auc: 0.897849
[5175]	valid_0's auc: 0.897845
[5176]	valid_0's auc: 0.897842
[5177]	valid_0's auc: 0.897845
[5178]	valid_0's auc: 0.897846
[5179]	valid_0's auc: 0.897844
[5180]	valid_0's auc: 0.897849
[5181]	valid_0's auc: 0.897851
[5182]	valid_0's auc: 0.897846
[5183]	valid_0's auc: 0.897852
[5184]	valid_0's auc: 0.897849
[5185]	valid_0's auc: 0.897844
[5186]	valid_0's auc: 0.897846
[5187]	valid_0's auc: 0.897851
[5188]	valid_0's auc: 0.897845
[5189]	valid_0's auc: 0.897844
[5190]	valid_0's auc: 0.897843
[5191]	valid_0's auc: 0.897843
[5192]	valid_0's auc: 0.897843
[5193]	valid_0's auc: 0.897848
[5194]	valid_0's auc: 0.897853
[5195]	valid_0's auc: 0.89785
[5196]	valid_0's auc: 0.897854
[5197]	valid_0's auc: 0.897858
[5198]	valid_0's auc: 0.89787
[5199]	valid_0's auc: 0.897869
[5200]	valid_0's auc: 0.897864
[5201]	valid_0's auc: 0.897862
[5202]	valid_0's auc: 0.897872
[5203]	valid_0's auc: 0.897868
[5204]	valid_0's auc: 0.897864
[5205]	valid_0's auc: 0.897866
[5206]	valid_0's auc: 0.897861
[5207]	valid_0's auc: 0.897854
[5208]	valid_0's auc: 0.897856
[5209]	valid_0's auc: 0.89787
[5210]	valid_0's auc: 0.89787
[5211]	valid_0's auc: 0.897874
[5212]	valid_0's auc: 0.897883
[5213]	valid_0's auc: 0.89789
[5214]	valid_0's auc: 0.897894
[5215]	valid_0's auc: 0.897896
[5216]	valid_0's auc: 0.897893
[5217]	valid_0's auc: 0.897896
[5218]	valid_0's auc: 0.8979
[5219]	valid_0's auc: 0.897908
[5220]	valid_0's auc: 0.897904
[5221]	valid_0's auc: 0.897906
[5222]	valid_0's auc: 0.897911
[5223]	valid_0's auc: 0.897909
[5224]	valid_0's auc: 0.897913
[5225]	valid_0's auc: 0.897912
[5226]	valid_0's auc: 0.897908
[5227]	valid_0's auc: 0.897909
[5228]	valid_0's auc: 0.897909
[5229]	valid_0's auc: 0.897907
[5230]	valid_0's auc: 0.897903
[5231]	valid_0's auc: 0.897903
[5232]	valid_0's auc: 0.897914
[5233]	valid_0's auc: 0.897916
[5234]	valid_0's auc: 0.897915
[5235]	valid_0's auc: 0.897915
[5236]	valid_0's auc: 0.897918
[5237]	valid_0's auc: 0.897916
[5238]	valid_0's auc: 0.897918
[5239]	valid_0's auc: 0.897918
[5240]	valid_0's auc: 0.897924
[5241]	valid_0's auc: 0.89792
[5242]	valid_0's auc: 0.897919
[5243]	valid_0's auc: 0.897914
[5244]	valid_0's auc: 0.897912
[5245]	valid_0's auc: 0.897913
[5246]	valid_0's auc: 0.897906
[5247]	valid_0's auc: 0.897912
[5248]	valid_0's auc: 0.897909
[5249]	valid_0's auc: 0.897906
[5250]	valid_0's auc: 0.897911
[5251]	valid_0's auc: 0.897907
[5252]	valid_0's auc: 0.897906
[5253]	valid_0's auc: 0.897904
[5254]	valid_0's auc: 0.8979
[5255]	valid_0's auc: 0.897896
[5256]	valid_0's auc: 0.897891
[5257]	valid_0's auc: 0.897889
[5258]	valid_0's auc: 0.897891
[5259]	valid_0's auc: 0.897889
[5260]	valid_0's auc: 0.897896
[5261]	valid_0's auc: 0.897896
[5262]	valid_0's auc: 0.897898
[5263]	valid_0's auc: 0.897896
[5264]	valid_0's auc: 0.897892
[5265]	valid_0's auc: 0.897893
[5266]	valid_0's auc: 0.897891
[5267]	valid_0's auc: 0.89789
[5268]	valid_0's auc: 0.897892
[5269]	valid_0's auc: 0.897892
[5270]	valid_0's auc: 0.897891
[5271]	valid_0's auc: 0.897888
[5272]	valid_0's auc: 0.897896
[5273]	valid_0's auc: 0.897902
[5274]	valid_0's auc: 0.897909
[5275]	valid_0's auc: 0.89791
[5276]	valid_0's auc: 0.897915
[5277]	valid_0's auc: 0.897917
[5278]	valid_0's auc: 0.897917
[5279]	valid_0's auc: 0.897914
[5280]	valid_0's auc: 0.897916
[5281]	valid_0's auc: 0.897915
[5282]	valid_0's auc: 0.89791
[5283]	valid_0's auc: 0.897919
[5284]	valid_0's auc: 0.897914
[5285]	valid_0's auc: 0.897922
[5286]	valid_0's auc: 0.897927
[5287]	valid_0's auc: 0.897928
[5288]	valid_0's auc: 0.897929
[5289]	valid_0's auc: 0.897929
[5290]	valid_0's auc: 0.89793
[5291]	valid_0's auc: 0.89793
[5292]	valid_0's auc: 0.897938
[5293]	valid_0's auc: 0.897932
[5294]	valid_0's auc: 0.897935
[5295]	valid_0's auc: 0.897929
[5296]	valid_0's auc: 0.897934
[5297]	valid_0's auc: 0.897937
[5298]	valid_0's auc: 0.897932
[5299]	valid_0's auc: 0.897933
[5300]	valid_0's auc: 0.897939
[5301]	valid_0's auc: 0.897937
[5302]	valid_0's auc: 0.897936
[5303]	valid_0's auc: 0.897937
[5304]	valid_0's auc: 0.897945
[5305]	valid_0's auc: 0.89794
[5306]	valid_0's auc: 0.897941
[5307]	valid_0's auc: 0.897936
[5308]	valid_0's auc: 0.897937
[5309]	valid_0's auc: 0.897942
[5310]	valid_0's auc: 0.897942
[5311]	valid_0's auc: 0.897947
[5312]	valid_0's auc: 0.897946
[5313]	valid_0's auc: 0.89795
[5314]	valid_0's auc: 0.897947
[5315]	valid_0's auc: 0.897945
[5316]	valid_0's auc: 0.897949
[5317]	valid_0's auc: 0.897955
[5318]	valid_0's auc: 0.89795
[5319]	valid_0's auc: 0.897951
[5320]	valid_0's auc: 0.897949
[5321]	valid_0's auc: 0.897944
[5322]	valid_0's auc: 0.897946
[5323]	valid_0's auc: 0.897947
[5324]	valid_0's auc: 0.897947
[5325]	valid_0's auc: 0.897943
[5326]	valid_0's auc: 0.89794
[5327]	valid_0's auc: 0.897941
[5328]	valid_0's auc: 0.897941
[5329]	valid_0's auc: 0.897945
[5330]	valid_0's auc: 0.897947
[5331]	valid_0's auc: 0.897952
[5332]	valid_0's auc: 0.897949
[5333]	valid_0's auc: 0.89795
[5334]	valid_0's auc: 0.897951
[5335]	valid_0's auc: 0.897952
[5336]	valid_0's auc: 0.897953
[5337]	valid_0's auc: 0.897953
[5338]	valid_0's auc: 0.897957
[5339]	valid_0's auc: 0.897953
[5340]	valid_0's auc: 0.897955
[5341]	valid_0's auc: 0.897955
[5342]	valid_0's auc: 0.897958
[5343]	valid_0's auc: 0.897954
[5344]	valid_0's auc: 0.897952
[5345]	valid_0's auc: 0.897954
[5346]	valid_0's auc: 0.897951
[5347]	valid_0's auc: 0.897955
[5348]	valid_0's auc: 0.897953
[5349]	valid_0's auc: 0.897951
[5350]	valid_0's auc: 0.897946
[5351]	valid_0's auc: 0.897948
[5352]	valid_0's auc: 0.89795
[5353]	valid_0's auc: 0.897953
[5354]	valid_0's auc: 0.89796
[5355]	valid_0's auc: 0.897963
[5356]	valid_0's auc: 0.897979
[5357]	valid_0's auc: 0.897981
[5358]	valid_0's auc: 0.897985
[5359]	valid_0's auc: 0.897982
[5360]	valid_0's auc: 0.897978
[5361]	valid_0's auc: 0.897985
[5362]	valid_0's auc: 0.897988
[5363]	valid_0's auc: 0.89799
[5364]	valid_0's auc: 0.897994
[5365]	valid_0's auc: 0.897997
[5366]	valid_0's auc: 0.897994
[5367]	valid_0's auc: 0.897996
[5368]	valid_0's auc: 0.897999
[5369]	valid_0's auc: 0.897999
[5370]	valid_0's auc: 0.897995
[5371]	valid_0's auc: 0.897993
[5372]	valid_0's auc: 0.897989
[5373]	valid_0's auc: 0.897992
[5374]	valid_0's auc: 0.897992
[5375]	valid_0's auc: 0.897992
[5376]	valid_0's auc: 0.897995
[5377]	valid_0's auc: 0.897996
[5378]	valid_0's auc: 0.897997
[5379]	valid_0's auc: 0.89799
[5380]	valid_0's auc: 0.897988
[5381]	valid_0's auc: 0.897987
[5382]	valid_0's auc: 0.897983
[5383]	valid_0's auc: 0.897986
[5384]	valid_0's auc: 0.897986
[5385]	valid_0's auc: 0.897989
[5386]	valid_0's auc: 0.897986
[5387]	valid_0's auc: 0.897982
[5388]	valid_0's auc: 0.89798
[5389]	valid_0's auc: 0.897981
[5390]	valid_0's auc: 0.897982
[5391]	valid_0's auc: 0.897986
[5392]	valid_0's auc: 0.897993
[5393]	valid_0's auc: 0.897999
[5394]	valid_0's auc: 0.898001
[5395]	valid_0's auc: 0.897999
[5396]	valid_0's auc: 0.897996
[5397]	valid_0's auc: 0.898005
[5398]	valid_0's auc: 0.898008
[5399]	valid_0's auc: 0.898017
[5400]	valid_0's auc: 0.898017
[5401]	valid_0's auc: 0.898017
[5402]	valid_0's auc: 0.898021
[5403]	valid_0's auc: 0.898013
[5404]	valid_0's auc: 0.89801
[5405]	valid_0's auc: 0.898014
[5406]	valid_0's auc: 0.898018
[5407]	valid_0's auc: 0.898015
[5408]	valid_0's auc: 0.898013
[5409]	valid_0's auc: 0.898011
[5410]	valid_0's auc: 0.898015
[5411]	valid_0's auc: 0.898017
[5412]	valid_0's auc: 0.898017
[5413]	valid_0's auc: 0.898016
[5414]	valid_0's auc: 0.89801
[5415]	valid_0's auc: 0.898015
[5416]	valid_0's auc: 0.898009
[5417]	valid_0's auc: 0.89801
[5418]	valid_0's auc: 0.898012
[5419]	valid_0's auc: 0.89801
[5420]	valid_0's auc: 0.898009
[5421]	valid_0's auc: 0.89801
[5422]	valid_0's auc: 0.89801
[5423]	valid_0's auc: 0.89801
[5424]	valid_0's auc: 0.898005
[5425]	valid_0's auc: 0.898007
[5426]	valid_0's auc: 0.898007
[5427]	valid_0's auc: 0.898016
[5428]	valid_0's auc: 0.898008
[5429]	valid_0's auc: 0.898015
[5430]	valid_0's auc: 0.898022
[5431]	valid_0's auc: 0.898018
[5432]	valid_0's auc: 0.898023
[5433]	valid_0's auc: 0.898025
[5434]	valid_0's auc: 0.898021
[5435]	valid_0's auc: 0.898022
[5436]	valid_0's auc: 0.89802
[5437]	valid_0's auc: 0.898022
[5438]	valid_0's auc: 0.898021
[5439]	valid_0's auc: 0.89802
[5440]	valid_0's auc: 0.898017
[5441]	valid_0's auc: 0.898016
[5442]	valid_0's auc: 0.898019
[5443]	valid_0's auc: 0.898028
[5444]	valid_0's auc: 0.898029
[5445]	valid_0's auc: 0.898029
[5446]	valid_0's auc: 0.89803
[5447]	valid_0's auc: 0.898033
[5448]	valid_0's auc: 0.89803
[5449]	valid_0's auc: 0.898031
[5450]	valid_0's auc: 0.898036
[5451]	valid_0's auc: 0.898031
[5452]	valid_0's auc: 0.898028
[5453]	valid_0's auc: 0.898035
[5454]	valid_0's auc: 0.898039
[5455]	valid_0's auc: 0.898037
[5456]	valid_0's auc: 0.898041
[5457]	valid_0's auc: 0.898042
[5458]	valid_0's auc: 0.898041
[5459]	valid_0's auc: 0.89804
[5460]	valid_0's auc: 0.898034
[5461]	valid_0's auc: 0.898036
[5462]	valid_0's auc: 0.898034
[5463]	valid_0's auc: 0.898036
[5464]	valid_0's auc: 0.898037
[5465]	valid_0's auc: 0.898043
[5466]	valid_0's auc: 0.898049
[5467]	valid_0's auc: 0.898046
[5468]	valid_0's auc: 0.898048
[5469]	valid_0's auc: 0.898051
[5470]	valid_0's auc: 0.89805
[5471]	valid_0's auc: 0.898052
[5472]	valid_0's auc: 0.898048
[5473]	valid_0's auc: 0.898051
[5474]	valid_0's auc: 0.898048
[5475]	valid_0's auc: 0.898056
[5476]	valid_0's auc: 0.898054
[5477]	valid_0's auc: 0.898055
[5478]	valid_0's auc: 0.898062
[5479]	valid_0's auc: 0.898068
[5480]	valid_0's auc: 0.898067
[5481]	valid_0's auc: 0.898069
[5482]	valid_0's auc: 0.898066
[5483]	valid_0's auc: 0.898069
[5484]	valid_0's auc: 0.898072
[5485]	valid_0's auc: 0.898068
[5486]	valid_0's auc: 0.89807
[5487]	valid_0's auc: 0.898071
[5488]	valid_0's auc: 0.898077
[5489]	valid_0's auc: 0.898081
[5490]	valid_0's auc: 0.898079
[5491]	valid_0's auc: 0.898079
[5492]	valid_0's auc: 0.898079
[5493]	valid_0's auc: 0.898084
[5494]	valid_0's auc: 0.898085
[5495]	valid_0's auc: 0.898085
[5496]	valid_0's auc: 0.898083
[5497]	valid_0's auc: 0.898084
[5498]	valid_0's auc: 0.898085
[5499]	valid_0's auc: 0.898081
[5500]	valid_0's auc: 0.898087
[5501]	valid_0's auc: 0.898091
[5502]	valid_0's auc: 0.898088
[5503]	valid_0's auc: 0.898087
[5504]	valid_0's auc: 0.898086
[5505]	valid_0's auc: 0.898087
[5506]	valid_0's auc: 0.898087
[5507]	valid_0's auc: 0.898092
[5508]	valid_0's auc: 0.898091
[5509]	valid_0's auc: 0.89809
[5510]	valid_0's auc: 0.898097
[5511]	valid_0's auc: 0.898094
[5512]	valid_0's auc: 0.898092
[5513]	valid_0's auc: 0.898091
[5514]	valid_0's auc: 0.898092
[5515]	valid_0's auc: 0.898098
[5516]	valid_0's auc: 0.898097
[5517]	valid_0's auc: 0.898097
[5518]	valid_0's auc: 0.898096
[5519]	valid_0's auc: 0.898093
[5520]	valid_0's auc: 0.898092
[5521]	valid_0's auc: 0.898097
[5522]	valid_0's auc: 0.898105
[5523]	valid_0's auc: 0.898102
[5524]	valid_0's auc: 0.898101
[5525]	valid_0's auc: 0.8981
[5526]	valid_0's auc: 0.898098
[5527]	valid_0's auc: 0.898093
[5528]	valid_0's auc: 0.898094
[5529]	valid_0's auc: 0.898097
[5530]	valid_0's auc: 0.898095
[5531]	valid_0's auc: 0.898096
[5532]	valid_0's auc: 0.898096
[5533]	valid_0's auc: 0.898092
[5534]	valid_0's auc: 0.898095
[5535]	valid_0's auc: 0.898099
[5536]	valid_0's auc: 0.898098
[5537]	valid_0's auc: 0.898097
[5538]	valid_0's auc: 0.898094
[5539]	valid_0's auc: 0.898093
[5540]	valid_0's auc: 0.898095
[5541]	valid_0's auc: 0.898091
[5542]	valid_0's auc: 0.89809
[5543]	valid_0's auc: 0.898086
[5544]	valid_0's auc: 0.898075
[5545]	valid_0's auc: 0.898074
[5546]	valid_0's auc: 0.898073
[5547]	valid_0's auc: 0.898077
[5548]	valid_0's auc: 0.898076
[5549]	valid_0's auc: 0.898078
[5550]	valid_0's auc: 0.898068
[5551]	valid_0's auc: 0.898074
[5552]	valid_0's auc: 0.89808
[5553]	valid_0's auc: 0.898071
[5554]	valid_0's auc: 0.89807
[5555]	valid_0's auc: 0.898071
[5556]	valid_0's auc: 0.89807
[5557]	valid_0's auc: 0.898072
[5558]	valid_0's auc: 0.898069
[5559]	valid_0's auc: 0.898071
[5560]	valid_0's auc: 0.898068
[5561]	valid_0's auc: 0.89807
[5562]	valid_0's auc: 0.898078
[5563]	valid_0's auc: 0.898073
[5564]	valid_0's auc: 0.898074
[5565]	valid_0's auc: 0.898073
[5566]	valid_0's auc: 0.898071
[5567]	valid_0's auc: 0.898075
[5568]	valid_0's auc: 0.898072
[5569]	valid_0's auc: 0.89807
[5570]	valid_0's auc: 0.898072
[5571]	valid_0's auc: 0.898078
[5572]	valid_0's auc: 0.898081
Early stopping, best iteration is:
[5522]	valid_0's auc: 0.898105
Done Training.
#+end_example

Predict

Edit: Score on Kaggle = 0.89791

#+BEGIN_SRC python :session :results output
test = pd.read_csv('test.csv')

submission = pd.DataFrame()
submission['ID_code'] = test.pop('ID_code')

submission['target'] = gbm.predict(test, num_iteration=gbm.best_iteration)

submission.to_csv('submission.csv', index=False)

print(submission.head())
#+END_SRC

#+RESULTS:
: ID_code    target
: 0  test_0  0.109355
: 1  test_1  0.191387
: 2  test_2  0.123151
: 3  test_3  0.168235
: 4  test_4  0.041148

** Light GBM original + binned one-hot features ensemble

We'll use validation sets to train both models and have a separate hold-out test set to train the ensemble.

#+BEGIN_SRC python :session :results output
import pandas as pd
from sklearn.model_selection import train_test_split
import lightgbm as lgb
import numpy as np
from sklearn.metrics import roc_auc_score
#+END_SRC

#+RESULTS:
: Python 3.8.1 (default, Jan 21 2020, 20:27:39) 
: [GCC 9.2.0] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: >>> python.el: native completion setup loaded

#+BEGIN_SRC python :session :results output
train = pd.read_csv('train.csv')

# The ID_code column contains no information, so we remove it
train.drop('ID_code', axis=1, inplace=True)

train, test = train_test_split(train, test_size=0.2)
train, validation = train_test_split(train, test_size=0.2)

train_labels = train.pop('target')
validation_labels = validation.pop('target')
test_labels = test.pop('target')
#+END_SRC

#+RESULTS:

Train the model

#+BEGIN_SRC python :session :results output
# create dataset for lightgbm
lgb_train = lgb.Dataset(train, train_labels)
lgb_eval = lgb.Dataset(validation,
                       validation_labels,
                       reference=lgb_train)
random_state = 42

params = {
    # default num_trees=100
    'num_trees': 10000,
    'objective': 'binary',
    'metric': 'auc',
    'num_leaves': 4,
    'learning_rate': 0.02,
    'boost_from_average': 'false',
    # Percentage of features to be used for each tree
    'feature_fraction': 0.10,
    'min_data_in_leaf': 80,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.4,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 7,
    'bagging_seed' : random_state,
    'seed': random_state
}

print('Starting training...')

# train
original_model = lgb.train(params,
                lgb_train,
                valid_sets=lgb_eval,
                early_stopping_rounds=50)

print('Done Training.')
#+END_SRC

Now the bin and one-hot encoding model. A class to perform the transformations:

#+BEGIN_SRC python :session :results output
class BinAndOneHot:
    """
    Tools to calculate bins for training data and then one-hot encode any any dataframe according to the bins determined by the training data.
    Needs Numpy and Pands.

    Attributes:
        num_bins (int): Number of bins to use for each feature (currently same number must be used for all features)
        bins (numpy ndarray): List of bins determined by training data
    """
    def __init__(self):
        self.num_bins = None
        self.bins = None

    def fit(self, train_dataframe, num_bins):
        """
        Calculates equal width bins for each feature of training data, to be used for consistently binning training, validation, and test data. First and last bins are extended to include +-infinity.

        Args:
            train_dataframe (pandas dataframe): Training data
            num_bins (int): Number of bins for each feature

        Attributes:
            self.num_bins (int): Number of bins is stored as this class Attribute
            self.bins (numpy ndarray): List of bins is assigned to this class Attribute

        Returns:
            None
        """
        # store 'num_bins' as a class attribute so that we use the same number of bins for other functions in this class
        self.num_bins = num_bins

        # populate bins_list with binned features by looping over 'train_dataframe columns'
        bins_list = []
        for column in train_dataframe.columns:

            # we don't need the binned dataframe, just the bins
            _, bins = pd.cut(train_dataframe[column], bins=self.num_bins, retbins=True)

            # extend first and last bins to include +-infinity
            bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))

            # store the result of binning this column in bins_list
            bins_list.append(bins)

        # Assign bins_list to class Attribute to be used in other functions in this class
        self.bins = bins_list

        print("Done calculating bins. List of bins stored as class attribute self.bins.")

        return None

    def transform(self, dataframe):
        """
        Bins each column of a dataframe into bins determined by the training data. Then creates new features one-hot encoding these bins.

        Args:
            dataframe (pandas dataframe): Dataframe to be transormed

        Returns:
            one_hot_bins_dataframe (pandas dataframe): Dataframe of features that one-hot encode our data according to training data bins
        """
        # bin dataframe according to self.bins (determined by training data)
        i = 0
        for column in dataframe.columns:
            dataframe[column] = pd.cut(dataframe[column], bins=self.bins[i])
            i += 1

        # one-hot encoding our binned data
        one_hot_bins_dataframe = pd.get_dummies(dataframe)

        return one_hot_bins_dataframe
#+END_SRC

#+RESULTS:

Warning: the above class does mutate the dataframes it acts on. So we should make copies of train and validation so that we don't need to re-run the previous model. 

Bin and one-hot encode training and validation data:

#+BEGIN_SRC python :session :results output
train_copy = train.copy()
validation_copy = validation.copy()

bin_and_one_hot = BinAndOneHot()

# calculate binning (increasing num_bins greatly increases memory usage)
num_bins = 20
bin_and_one_hot.fit(train_dataframe=train_copy, num_bins=num_bins)

# bin and one-hot encode training and validation data, then convert to LightGBM format and save as binary to be loaded later. Delete dataframes afterwards. Saving to binaries and deleting dataframes greatly helps with memory management.
train_copy = bin_and_one_hot.transform(train_copy)
validation_copy = bin_and_one_hot.transform(validation_copy)

# LightGBM doesn't like JSON characters in feature names, so replace these characters with blank
def fix_feature_names(dataframe):
    dataframe.columns = ["".join (c if c.isalnum() else "_" for c in str(x)) for x in dataframe.columns]
    return dataframe

train_copy = fix_feature_names(train_copy)
validation_copy = fix_feature_names(validation_copy)

# convert to LightGBM format and save as binary (helps with memory management)
train_copy = lgb.Dataset(train_copy, train_labels)
validation_copy = lgb.Dataset(validation_copy,
                              validation_labels,
                              reference=train_copy)

# save to binaries
train_copy.save_binary('train.bin')
del train_copy

validation_copy.save_binary('validation.bin')
del validation_copy
#+END_SRC

#+RESULTS:
: Done calculating bins. List of bins stored as class attribute self.bins.
: [LightGBM] [Info] Saving data to binary file train.bin
: [LightGBM] [Info] Saving data to binary file validation.bin

Train the model

#+BEGIN_SRC python :session :results output
random_state = 42

# load binaries
train_binaries = lgb.Dataset('train.bin')
validation_binaries = lgb.Dataset('validation.bin')

params = {
    # default num_trees=100
    'num_trees': 20000,
    'objective': 'binary',
    'metric': 'auc',
    'num_leaves': 4,
    'learning_rate': 0.02,
    'boost_from_average': 'false',
    # Percentage of features to be used for each tree
    'feature_fraction': 0.10,
    'min_data_in_leaf': 80,
    # Percentage of data to be sampled for each tree
    'bagging_fraction': 0.4,
    # Perform bagging at every k-th tree (bagging_freq must be non-zero for bagging_fraction to be used)
    'bagging_freq': 5,
    # Documentation recommends using number of available cores, not number of available threads
    'num_threads': 7,
    'bagging_seed' : random_state,
    'seed': random_state
}

print('Starting training...')

# train
one_hot_model = lgb.train(params,
                          train_binaries,
                          valid_sets=validation_binaries,
                          early_stopping_rounds=50)

print('Done Training.')
#+END_SRC

Predictions:

Results: ensembling with the one-hot features' predictions just makes the model worse. Probably to be expected given what I have read about one-hot features and tree-learners. 

#+BEGIN_SRC python :session :results output
# predictions with original features
predictions_original = original_model.predict(test, num_iteration=original_model.best_iteration)

# transform the test data so that the one_hot model can make predictions
test_copy = test.copy()
test_copy = bin_and_one_hot.transform(test_copy)

# LightGBM doesn't like JSON characters in feature names, so replace these characters with blank
test_copy = fix_feature_names(test_copy)

# predictions with one-hot model
predictions_one_hot = one_hot_model.predict(test_copy, num_iteration=one_hot_model.best_iteration)
#+END_SRC

#+RESULTS:

Ensembling predictions

#+BEGIN_SRC python :session :results output
predictions = (predictions_original + predictions_one_hot)/2

print(roc_auc_score(test_labels, predictions))
#+END_SRC

#+RESULTS:
: 0.8928183325425134
** NN binned one-hot features ensemble 

#+BEGIN_SRC python :session :results output
import pandas as pd
from sklearn.model_selection import train_test_split
import lightgbm as lgb
import numpy as np
from sklearn.metrics import roc_auc_score
import tensorflow as tf
from tensorflow import keras
from matplotlib import pyplot as plt 
#+END_SRC

#+RESULTS:
: Python 3.8.1 (default, Jan 21 2020, 20:27:39) 
: [GCC 9.2.0] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: 2020-02-17 22:28:17.379269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
: python.el: native completion setup loaded

Import the data. We evaluate each model on validation and train the ensemble on test split.

#+BEGIN_SRC python :session :results output
train = pd.read_csv('train.csv')

# The ID_code column contains no information, so we remove it
train.drop('ID_code', axis=1, inplace=True)

train, test = train_test_split(train, test_size=0.2)
train, validation = train_test_split(train, test_size=0.2)

train_labels = train.pop('target')
validation_labels = validation.pop('target')
test_labels = test.pop('target')
#+END_SRC

#+RESULTS:

Create our new features by binning and one-hot encoding

Warning: this class does mutate the dataframes it acts on. So we should make copies of train and validation so that we don't need to re-run the previous model. 

Bin and one-hot encode training and validation data:

#+BEGIN_SRC python :session :results output
class BinAndOneHot:
    """
    Tools to calculate bins for training data and then one-hot encode any any dataframe according to the bins determined by the training data.
    Needs Numpy and Pands.

    Attributes:
        num_bins (int): Number of bins to use for each feature (currently same number must be used for all features)
        bins (numpy ndarray): List of bins determined by training data
    """
    def __init__(self):
        self.num_bins = None
        self.bins = None

    def fit(self, train_dataframe, num_bins):
        """
        Calculates equal width bins for each feature of training data, to be used for consistently binning training, validation, and test data. First and last bins are extended to include +-infinity.

        Args:
            train_dataframe (pandas dataframe): Training data
            num_bins (int): Number of bins for each feature

        Attributes:
            self.num_bins (int): Number of bins is stored as this class Attribute
            self.bins (numpy ndarray): List of bins is assigned to this class Attribute

        Returns:
            None
        """
        # store 'num_bins' as a class attribute so that we use the same number of bins for other functions in this class
        self.num_bins = num_bins

        # populate bins_list with binned features by looping over 'train_dataframe columns'
        bins_list = []
        for column in train_dataframe.columns:

            # we don't need the binned dataframe, just the bins
            _, bins = pd.cut(train_dataframe[column], bins=self.num_bins, retbins=True)

            # extend first and last bins to include +-infinity
            bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))

            # store the result of binning this column in bins_list
            bins_list.append(bins)

        # Assign bins_list to class Attribute to be used in other functions in this class
        self.bins = bins_list

        print("Done calculating bins. List of bins stored as class attribute self.bins.")

        return None

    def transform(self, dataframe):
        """
        Bins each column of a dataframe into bins determined by the training data. Then creates new features one-hot encoding these bins.

        Args:
            dataframe (pandas dataframe): Dataframe to be transormed

        Returns:
            one_hot_bins_dataframe (pandas dataframe): Dataframe of features that one-hot encode our data according to training data bins
        """
        # bin dataframe according to self.bins (determined by training data)
        i = 0
        for column in dataframe.columns:
            dataframe[column] = pd.cut(dataframe[column], bins=self.bins[i])
            i += 1

        # one-hot encoding our binned data
        one_hot_bins_dataframe = pd.get_dummies(dataframe)

        return one_hot_bins_dataframe

train_copy = train.copy()
validation_copy = validation.copy()

bin_and_one_hot = BinAndOneHot()

# calculate binning (increasing num_bins greatly increases memory usage)
num_bins = 10
bin_and_one_hot.fit(train_dataframe=train_copy, num_bins=num_bins)

# bin and one-hot encode training and validation data
train_copy = bin_and_one_hot.transform(train_copy)
validation_copy = bin_and_one_hot.transform(validation_copy)

# Include the original features
train_copy = pd.concat([train, train_copy], axis=1)
validation_copy = pd.concat([validation, validation_copy], axis=1)

# Tensorflow model needs numpy arrays as inputs:
train_copy = np.array(train_copy, dtype=np.float32)
validation_copy = np.array(validation_copy, dtype=np.float32)

train_labels_tf = np.array(train_labels)
validation_labels_tf = np.array(validation_labels)
#+END_SRC

#+RESULTS:
: Done calculating bins. List of bins stored as class attribute self.bins.

Set up and train the model

#+BEGIN_SRC python :session :results output
# clear keras session so that we can rerun without errors
tf.keras.backend.clear_session()

METRICS = [
    keras.metrics.TruePositives(name='tp'),
    keras.metrics.FalsePositives(name='fp'),
    keras.metrics.TrueNegatives(name='tn'),
    keras.metrics.FalseNegatives(name='fn'),
    keras.metrics.BinaryAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='auc'),
]

N_TRAIN = train_copy.shape[0]
BATCH_SIZE = 2048
EPOCHS = 50
STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE

# we've been struggling with overfitting, so take some ideas from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit

# learning rate decay
# initial learning rate 
initial_rate = 0.001

# 'decay_factor' = x means learning rate decays to 1/2 of the 'initial_rate' after x epochs, 1/3 after 2x epochs, etc.
decay_factor = 10 

lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
    initial_rate,
    decay_steps=STEPS_PER_EPOCH*decay_factor,
    decay_rate=1,
    staircase=False)

def get_optimizer():
    return tf.keras.optimizers.Adam(lr_schedule)

# model architecture
def make_model(train_features, metrics=METRICS):
    model = keras.Sequential([
        keras.layers.Dense(256,
                           kernel_regularizer=keras.regularizers.l2(0.0001),
                           activation='elu',
                           input_shape=(train_features.shape[-1],)),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(256, kernel_regularizer=keras.regularizers.l2(0.0001),
                     activation='elu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(256, kernel_regularizer=keras.regularizers.l2(0.0001),
                     activation='elu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(256, kernel_regularizer=keras.regularizers.l2(0.0001),
                     activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(1, activation='sigmoid')
        ])

    model.compile(
        optimizer=get_optimizer(),
        loss=keras.losses.BinaryCrossentropy(),
        metrics=metrics)
    return model

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc',
    verbose=1,
    patience=20,
    mode='max',
    restore_best_weights=True)

# initialize the model
model = make_model(train_copy)

# Let's try with class weights
num_pos = np.count_nonzero(train_labels_tf)
num_neg = N_TRAIN - num_pos
class_weight = {0: N_TRAIN/(2.0*num_neg) , 1: N_TRAIN/(2.0*num_pos)}

# train
# Features and labels input as numpy arrays
model_history = model.fit(
    train_copy,
    train_labels_tf,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    # callbacks=[early_stopping],
    validation_data=(validation_copy, validation_labels_tf),
    class_weight=class_weight)
#+END_SRC

Define function to plot metrics:

#+BEGIN_SRC python :session :results output
def plot_metrics(history):
    metrics =  ['loss', 'auc', 'precision', 'recall']
    plt.rcParams['figure.figsize'] = (12, 10)
    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
    plt.figure(figsize=(6,4))

    for n, metric in enumerate(metrics):
        name = metric.replace("_"," ").capitalize()
        plt.subplot(2,2,n+1)
        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')
        plt.plot(history.epoch, history.history['val_'+metric], color=colors[0], linestyle="--", label='Val')
        plt.xlabel('Epoch')
        plt.ylabel(name)
        if metric == 'loss':
            plt.ylim([0, plt.ylim()[1]])
        elif metric == 'auc':
            plt.ylim([0.8,1])
        else:
            plt.ylim([0,1])

    plt.legend()
    plt.savefig('metrics.png')
    plt.close()
#+END_SRC

#+RESULTS:

Plot metrics:

#+BEGIN_SRC python :session :results file
# Display metrics plot
plot_metrics(model_history)
'metrics.png'
#+END_SRC

#+RESULTS:
[[file:metrics.png]]

We can run the baseline lightgbm from the previous subheading. Predictions:

#+BEGIN_SRC python :session :results output
# predictions with original features
predictions_original = original_model.predict(test, num_iteration=original_model.best_iteration)

# transform the test data so that the one_hot model can make predictions
test_copy = test.copy()
test_copy = bin_and_one_hot.transform(test_copy)

# include original features
test_copy = pd.concat([test, test_copy], axis=1)

# Tensorflow wants numpy arrays
test_copy = np.array(test_copy)

# predictions with one-hot model (note the array shape of the tensorflow predictions)
predictions_nn = model.predict(test_copy)[:,0]
#+END_SRC

#+RESULTS:
: 2020-02-17 22:46:21.707563: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 704000000 exceeds 10% of system memory.

Ensembling predictions

Results: baseline nn gets auc ~0.84-0.85. Adding one-hot features increases this to 0.87-0.88. But baseline lightgbm gets auc 0.89-0.90, adding one-hot to lightgbm does nothing, if anything decreases auc, and ensembling nn with one-hot + lightgbm baseline decreases auc. 




#+BEGIN_SRC python :session :results output
predictions = (2*predictions_original + predictions_nn)/3

print(roc_auc_score(test_labels, predictions))
#+END_SRC

#+RESULTS:
: 0.896092543156217
** NN baseline + light gbm ensemble 

#+BEGIN_SRC python :session :results output
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.metrics import roc_auc_score
import tensorflow as tf
from tensorflow import keras
from matplotlib import pyplot as plt 
#+END_SRC

#+RESULTS:
: Python 3.8.1 (default, Jan 21 2020, 20:27:39) 
: [GCC 9.2.0] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: 2020-02-18 01:34:36.108719: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
: python.el: native completion setup loaded

Import the data. We evaluate each model on validation and train the ensemble on test split.

#+BEGIN_SRC python :session :results output
train = pd.read_csv('train.csv')

# The ID_code column contains no information, so we remove it
train.drop('ID_code', axis=1, inplace=True)

train, test = train_test_split(train, test_size=0.2)
train, validation = train_test_split(train, test_size=0.2)

train_labels = train.pop('target')
validation_labels = validation.pop('target')
test_labels = test.pop('target')

# make copies, so we can use original dataframes in other models and ensemble
train_copy = train.copy()
validation_copy = validation.copy()

# Tensorflow model needs numpy arrays as inputs:
train_copy = np.array(train_copy)
validation_copy = np.array(validation_copy)

train_labels_tf = np.array(train_labels)
validation_labels_tf = np.array(validation_labels)

print(train_copy[0:10], train_labels_tf[0:10])
#+END_SRC

#+RESULTS:
: [[ 10.6968   7.5859  10.4816 ...   8.3026  15.635   -2.3098]
:  [  9.9505  -7.3431  16.0268 ...   8.6114  13.9202  -7.4491]
:  [  5.569   -8.2506  16.6531 ...   9.7912  19.8233   4.4413]
:  ...
:  [  5.3528  -3.3864  11.1542 ...  10.5047  13.707   -4.5791]
:  [  7.953    1.7199  13.8945 ...   7.9214  16.2634   3.093 ]
:  [  7.1039   0.1329  12.1485 ...  10.0316  18.8246 -19.7406]] [0 0 1 0 0 0 0 0 0 0]

Set up and train the model

#+BEGIN_SRC python :session :results output
# clear keras session so that we can rerun without errors
tf.keras.backend.clear_session()

METRICS = [
    keras.metrics.TruePositives(name='tp'),
    keras.metrics.FalsePositives(name='fp'),
    keras.metrics.TrueNegatives(name='tn'),
    keras.metrics.FalseNegatives(name='fn'),
    keras.metrics.BinaryAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='auc'),
]

N_TRAIN = train_copy.shape[0]
BATCH_SIZE = 2048
EPOCHS = 25 
STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE

# we've been struggling with overfitting, so take some ideas from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit

# learning rate decay
# initial learning rate 
initial_rate = 0.001

# 'decay_factor' = x means learning rate decays to 1/2 of the 'initial_rate' after x epochs, 1/3 after 2x epochs, etc.
decay_factor = 10 

lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
    initial_rate,
    decay_steps=STEPS_PER_EPOCH*decay_factor,
    decay_rate=1,
    staircase=False)

def get_optimizer():
    return tf.keras.optimizers.Adam(lr_schedule)

# model architecture
def make_model(train_features, metrics=METRICS):
    model = keras.Sequential([
        keras.layers.Dense(512,
                           kernel_regularizer=keras.regularizers.l2(0.0001),
                           activation='elu',
                           input_shape=(train_features.shape[-1],)),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(512, kernel_regularizer=keras.regularizers.l2(0.0001),
                     activation='elu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(512, kernel_regularizer=keras.regularizers.l2(0.0001),
                     activation='elu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(512, kernel_regularizer=keras.regularizers.l2(0.0001),
                     activation='elu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(1, activation='sigmoid')
        ])

    model.compile(
        optimizer=get_optimizer(),
        loss=keras.losses.BinaryCrossentropy(),
        metrics=metrics)
    return model

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc',
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)

# initialize the model
model = make_model(train_copy)

# Let's try with class weights
num_pos = np.count_nonzero(train_labels_tf)
num_neg = N_TRAIN - num_pos
class_weight = {0: N_TRAIN/(2.0*num_neg) , 1: N_TRAIN/(2.0*num_pos)}

# train
# Features and labels input as numpy arrays
model_history = model.fit(
    train_copy,
    train_labels_tf,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=[early_stopping],
    validation_data=(validation_copy, validation_labels_tf),
    class_weight=class_weight)
#+END_SRC

Define function to plot metrics:

#+BEGIN_SRC python :session :results output
def plot_metrics(history):
    metrics =  ['loss', 'auc', 'precision', 'recall']
    plt.rcParams['figure.figsize'] = (12, 10)
    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
    plt.figure(figsize=(6,4))

    for n, metric in enumerate(metrics):
        name = metric.replace("_"," ").capitalize()
        plt.subplot(2,2,n+1)
        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')
        plt.plot(history.epoch, history.history['val_'+metric], color=colors[0], linestyle="--", label='Val')
        plt.xlabel('Epoch')
        plt.ylabel(name)
        if metric == 'loss':
            plt.ylim([0, plt.ylim()[1]])
        elif metric == 'auc':
            plt.ylim([0.8,1])
        else:
            plt.ylim([0,1])

    plt.legend()
    plt.savefig('metrics.png')
    plt.close()
#+END_SRC

#+RESULTS:

Plot metrics:

#+BEGIN_SRC python :session :results file
# Display metrics plot
plot_metrics(model_history)
'metrics.png'
#+END_SRC

#+RESULTS:
[[file:metrics.png]]

We can run the baseline lightgbm from the previous subheading. Predictions:

#+BEGIN_SRC python :session :results output
# predictions with original features
predictions_original = original_model.predict(test, num_iteration=original_model.best_iteration)

# transform the test data so that the one_hot model can make predictions
test_copy = test.copy()

# Tensorflow wants numpy arrays
test_copy = np.array(test_copy)

# predictions with one-hot model (note the array shape of the tensorflow predictions)
predictions_nn = model.predict(test_copy)[:,0]
#+END_SRC

#+RESULTS:

Ensembling predictions

Results: the neural network baseline model just reduces the auc of the lightgbm model if we average predictions or try weighted average. 

Interestingly, multiplying the predictions slightly increased auc (but only when careful not to overfit on nn ie. stopped after 25 epochs. Did not see this improvement when training for longer epochs). I need to plot some metrics for lightgbm.

taking the minimum of predictions also increases auc. minimum outscores multiply by .005


#+BEGIN_SRC python :session :results output
predictions = np.multiply(predictions_original, predictions_nn)

print(roc_auc_score(test_labels, predictions))
#+END_SRC

#+RESULTS:
: 0.8921516686422715
